#!/usr/bin/env python3
"""
ãƒãƒƒãƒã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«
Batch System Optimizer

å¤§é‡ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†ã—ã€é‡è¤‡ã‚„ãƒ¬ã‚¬ã‚·ãƒ¼ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã™ã‚‹
"""

import os
import shutil
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple
import ast
import re
from datetime import datetime, timedelta

class BatchSystemOptimizer:
    """ãƒãƒƒãƒã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, batch_dir: str = "functions"):
        self.batch_dir = Path(batch_dir)
        self.archive_dir = Path("legacy")
        self.duplicate_files: Dict[str, List[Path]] = {}
        self.outdated_files: List[Path] = []
        self.large_files: List[Tuple[Path, int]] = []
        self.active_files: Set[Path] = set()

    def analyze_file_system(self) -> Dict[str, any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆ†æ"""
        if not self.batch_dir.exists():
            return {"error": "Batch directory not found"}

        python_files = list(self.batch_dir.rglob("*.py"))
        total_files = len(python_files)
        total_size = sum(f.stat().st_size for f in python_files)

        print(f"ğŸ“Š Analysis Results:")
        print(f"   Total Python files: {total_files}")
        print(f"   Total size: {total_size / 1024 / 1024:.2f} MB")

        # ãƒ•ã‚¡ã‚¤ãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯
        self._find_duplicate_files(python_files)

        # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
        self._find_outdated_files(python_files)

        # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
        self._find_large_files(python_files)

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        self._identify_active_files(python_files)

        return {
            "total_files": total_files,
            "total_size_mb": total_size / 1024 / 1024,
            "duplicates": len(self.duplicate_files),
            "outdated": len(self.outdated_files),
            "large_files": len(self.large_files),
            "active_files": len(self.active_files)
        }

    def _find_duplicate_files(self, files: List[Path]):
        """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        file_hashes: Dict[str, List[Path]] = {}

        for file_path in files:
            try:
                with open(file_path, 'rb') as f:
                    content = f.read()
                    file_hash = hashlib.md5(content).hexdigest()

                if file_hash not in file_hashes:
                    file_hashes[file_hash] = []
                file_hashes[file_hash].append(file_path)
            except:
                continue

        # é‡è¤‡ãŒã‚ã‚‹ã‚‚ã®ã‚’æŠ½å‡º
        self.duplicate_files = {
            hash_val: paths for hash_val, paths in file_hashes.items()
            if len(paths) > 1
        }

        if self.duplicate_files:
            print(f"\nğŸ” Found {len(self.duplicate_files)} groups of duplicate files:")
            for i, (hash_val, paths) in enumerate(self.duplicate_files.items()):
                if i < 5:  # æœ€åˆã®5ã¤ã ã‘è¡¨ç¤º
                    print(f"   Group {i+1}: {len(paths)} files")
                    for path in paths:
                        print(f"     - {path}")

    def _find_outdated_files(self, files: List[Path]):
        """å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆ90æ—¥ä»¥ä¸Šæ›´æ–°ã•ã‚Œã¦ã„ãªã„ï¼‰"""
        cutoff_date = datetime.now() - timedelta(days=90)

        for file_path in files:
            try:
                stat = file_path.stat()
                mod_time = datetime.fromtimestamp(stat.st_mtime)

                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¤ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
                outdated_patterns = [
                    'old_', 'backup_', 'temp_', 'test_', 'debug_',
                    '_old', '_backup', '_temp', '_test', '_debug',
                    '2024', '2023', '_v1', '_v2', '_v3'
                ]

                is_outdated = (
                    mod_time < cutoff_date or
                    any(pattern in file_path.name.lower() for pattern in outdated_patterns)
                )

                if is_outdated:
                    self.outdated_files.append(file_path)
            except:
                continue

        print(f"\nğŸ“… Found {len(self.outdated_files)} potentially outdated files")

    def _find_large_files(self, files: List[Path]):
        """å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆ1MBä»¥ä¸Šï¼‰"""
        for file_path in files:
            try:
                size = file_path.stat().st_size
                if size > 1024 * 1024:  # 1MBä»¥ä¸Š
                    self.large_files.append((file_path, size))
            except:
                continue

        self.large_files.sort(key=lambda x: x[1], reverse=True)

        if self.large_files:
            print(f"\nğŸ“¦ Found {len(self.large_files)} large files (>1MB):")
            for i, (path, size) in enumerate(self.large_files[:5]):
                print(f"   {i+1}. {path} ({size / 1024 / 1024:.2f} MB)")

    def _identify_active_files(self, files: List[Path]):
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š"""
        active_patterns = [
            'main', 'production', 'enhanced', 'improved', 'stable',
            'final', 'comprehensive', 'advanced', 'optimized'
        ]

        recent_date = datetime.now() - timedelta(days=30)

        for file_path in files:
            try:
                stat = file_path.stat()
                mod_time = datetime.fromtimestamp(stat.st_mtime)

                # æœ€è¿‘æ›´æ–°ã•ã‚ŒãŸã‹ã€é‡è¦ãªãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€
                is_active = (
                    mod_time > recent_date or
                    any(pattern in file_path.name.lower() for pattern in active_patterns) or
                    self._has_recent_imports(file_path)
                )

                if is_active:
                    self.active_files.add(file_path)
            except:
                continue

        print(f"\nâœ… Identified {len(self.active_files)} active files")

    def _has_recent_imports(self, file_path: Path) -> bool:
        """æœ€æ–°ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            modern_patterns = [
                'from dataclasses import',
                'from typing import',
                'async def',
                'await ',
                'from pathlib import',
                'from datetime import',
                'logging.getLogger',
                'with open('
            ]

            return any(pattern in content for pattern in modern_patterns)
        except:
            return False

    def create_optimization_plan(self) -> Dict[str, any]:
        """æœ€é©åŒ–ãƒ—ãƒ©ãƒ³ã‚’ä½œæˆ"""
        plan = {
            "actions": [],
            "potential_savings": 0,
            "files_to_archive": 0,
            "files_to_remove": 0
        }

        # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
        for hash_val, paths in self.duplicate_files.items():
            if len(paths) > 1:
                # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’1ã¤æ®‹ã—ã€ä»–ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
                newest = max(paths, key=lambda p: p.stat().st_mtime)
                duplicates = [p for p in paths if p != newest]

                plan["actions"].append({
                    "action": "archive_duplicates",
                    "keep": str(newest),
                    "archive": [str(p) for p in duplicates],
                    "savings_mb": sum(p.stat().st_size for p in duplicates) / 1024 / 1024
                })

                plan["files_to_archive"] += len(duplicates)
                plan["potential_savings"] += sum(p.stat().st_size for p in duplicates)

        # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
        outdated_not_active = [f for f in self.outdated_files if f not in self.active_files]
        if outdated_not_active:
            plan["actions"].append({
                "action": "archive_outdated",
                "files": [str(f) for f in outdated_not_active[:20]],  # æœ€åˆã®20å€‹
                "total_count": len(outdated_not_active),
                "savings_mb": sum(f.stat().st_size for f in outdated_not_active) / 1024 / 1024
            })

            plan["files_to_archive"] += len(outdated_not_active)
            plan["potential_savings"] += sum(f.stat().st_size for f in outdated_not_active)

        plan["potential_savings_mb"] = plan["potential_savings"] / 1024 / 1024

        return plan

    def execute_optimization(self, plan: Dict[str, any], dry_run: bool = True):
        """æœ€é©åŒ–ã‚’å®Ÿè¡Œ"""
        if dry_run:
            print(f"\nğŸ” DRY RUN - No actual changes will be made")
        else:
            print(f"\nğŸš€ Executing optimization plan...")

        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        if not dry_run:
            self.archive_dir.mkdir(exist_ok=True)

        executed_actions = 0

        for action in plan["actions"]:
            if action["action"] == "archive_duplicates":
                print(f"\nğŸ“ Archiving duplicate files:")
                print(f"   Keeping: {action['keep']}")
                for duplicate in action["archive"]:
                    print(f"   Archiving: {duplicate}")
                    if not dry_run:
                        self._archive_file(Path(duplicate))
                executed_actions += 1

            elif action["action"] == "archive_outdated":
                print(f"\nğŸ“… Archiving {action['total_count']} outdated files")
                for file_path in action["files"][:10]:  # æœ€åˆã®10å€‹ã ã‘è¡¨ç¤º
                    print(f"   Archiving: {file_path}")
                    if not dry_run:
                        self._archive_file(Path(file_path))
                executed_actions += 1

        print(f"\nâœ… Optimization complete!")
        print(f"   Actions executed: {executed_actions}")
        print(f"   Potential space savings: {plan['potential_savings_mb']:.2f} MB")
        print(f"   Files to archive: {plan['files_to_archive']}")

    def _archive_file(self, file_path: Path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–"""
        try:
            # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’ä¿æŒ
            relative_path = file_path.relative_to(self.batch_dir)
            archive_path = self.archive_dir / relative_path

            # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
            archive_path.parent.mkdir(parents=True, exist_ok=True)

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç§»å‹•
            shutil.move(str(file_path), str(archive_path))
            print(f"   âœ… Archived: {file_path} -> {archive_path}")

        except Exception as e:
            print(f"   âŒ Failed to archive {file_path}: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ§¹ Batch System Optimizer")
    print("=" * 50)

    optimizer = BatchSystemOptimizer()

    # ã‚·ã‚¹ãƒ†ãƒ åˆ†æ
    results = optimizer.analyze_file_system()

    if "error" in results:
        print(f"âŒ Error: {results['error']}")
        return

    # æœ€é©åŒ–ãƒ—ãƒ©ãƒ³ä½œæˆ
    plan = optimizer.create_optimization_plan()

    print(f"\nğŸ“‹ Optimization Plan:")
    print(f"   Potential savings: {plan['potential_savings_mb']:.2f} MB")
    print(f"   Files to archive: {plan['files_to_archive']}")
    print(f"   Actions planned: {len(plan['actions'])}")

    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã§å®Ÿè¡Œ
    optimizer.execute_optimization(plan, dry_run=True)

    print(f"\nğŸ’¡ To execute the optimization plan:")
    print(f"   python batch_system_optimizer.py --execute")

if __name__ == "__main__":
    main()