#!/usr/bin/env python3
"""
Miraikakaku ML Prediction System
è‡ªå‹•æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æ ªä¾¡äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 

ä¸»è¦æ©Ÿèƒ½:
1. ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•åé›†ãƒ»å‰å‡¦ç†
2. è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
3. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡ãƒ»æœ€é©åŒ–
4. äºˆæ¸¬çµæœã®è‡ªå‹•ç”Ÿæˆãƒ»ä¿å­˜
5. ãƒ¢ãƒ‡ãƒ«ã®å®šæœŸãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
"""

import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import joblib
import json
import logging
from datetime import datetime, timedelta
import os
import asyncio
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MLPredictionSystem:
    """æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.config = {
            # ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®š
            'price_history_days': 730,  # 2å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿
            'prediction_days': 30,      # 30æ—¥å…ˆã¾ã§äºˆæ¸¬
            'training_ratio': 0.8,      # 80%ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨
            'validation_ratio': 0.2,    # 20%ã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ä½¿ç”¨
            
            # ç‰¹å¾´é‡è¨­å®š
            'technical_indicators': [
                'sma_5', 'sma_10', 'sma_20', 'sma_50',
                'ema_12', 'ema_26',
                'rsi', 'macd', 'bb_upper', 'bb_lower',
                'volume_sma', 'price_change', 'volatility'
            ],
            
            # ãƒ¢ãƒ‡ãƒ«è¨­å®š
            'models': {
                'random_forest': {
                    'class': RandomForestRegressor,
                    'params': {
                        'n_estimators': [50, 100, 200],
                        'max_depth': [10, 20, None],
                        'min_samples_split': [2, 5, 10]
                    }
                },
                'gradient_boosting': {
                    'class': GradientBoostingRegressor,
                    'params': {
                        'n_estimators': [100, 200],
                        'learning_rate': [0.05, 0.1, 0.2],
                        'max_depth': [3, 5, 7]
                    }
                },
                'ridge_regression': {
                    'class': Ridge,
                    'params': {
                        'alpha': [0.1, 1.0, 10.0, 100.0]
                    }
                }
            },
            
            # ä¿å­˜ãƒ‘ã‚¹
            'models_dir': 'ml_models',
            'predictions_dir': 'ml_predictions',
            'reports_dir': 'ml_reports',
            
            # æ›´æ–°é »åº¦
            'retrain_threshold_days': 7,  # é€±1å›ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            'min_accuracy_threshold': 0.75  # æœ€ä½ç²¾åº¦è¦æ±‚
        }
        
        self.models = {}
        self.scalers = {}
        self.feature_columns = []
        self.trained_symbols = set()
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for dir_path in [self.config['models_dir'], self.config['predictions_dir'], self.config['reports_dir']]:
            os.makedirs(dir_path, exist_ok=True)
            
    async def run_ml_training_batch(self, symbols: List[str]) -> Dict:
        """æ©Ÿæ¢°å­¦ç¿’ãƒãƒƒãƒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"""
        logger.info(f"ğŸ¤– ML ãƒãƒƒãƒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹: {len(symbols)}éŠ˜æŸ„")
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'symbols_processed': 0,
            'models_trained': 0,
            'predictions_generated': 0,
            'errors': []
        }
        
        for symbol in symbols:
            try:
                # ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»å‰å‡¦ç†
                data = await self.fetch_and_preprocess_data(symbol)
                if data is None or len(data) < 100:  # æœ€ä½100æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                    continue
                
                # ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
                model_results = await self.train_models(symbol, data)
                
                # äºˆæ¸¬ç”Ÿæˆ
                if model_results['best_model'] is not None:
                    predictions = await self.generate_predictions(symbol, data, model_results['best_model'])
                    await self.save_predictions(symbol, predictions)
                    results['predictions_generated'] += 1
                
                results['symbols_processed'] += 1
                results['models_trained'] += len(model_results['trained_models'])
                
                # é€²æ—ãƒ­ã‚°
                if results['symbols_processed'] % 10 == 0:
                    logger.info(f"é€²æ—: {results['symbols_processed']}/{len(symbols)} éŠ˜æŸ„å‡¦ç†å®Œäº†")
                    
            except Exception as e:
                logger.error(f"MLå‡¦ç†ã‚¨ãƒ©ãƒ¼ [{symbol}]: {e}")
                results['errors'].append(f"{symbol}: {str(e)}")
                
        # çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        await self.generate_ml_report(results)
        
        logger.info(f"âœ… ML ãƒãƒƒãƒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {results['symbols_processed']}éŠ˜æŸ„å‡¦ç†")
        return results
        
    async def fetch_and_preprocess_data(self, symbol: str) -> Optional[pd.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»å‰å‡¦ç†"""
        try:
            # yfinanceã‹ã‚‰ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾—
            ticker = yf.Ticker(symbol)
            end_date = datetime.now()
            start_date = end_date - timedelta(days=self.config['price_history_days'])
            
            hist = ticker.history(start=start_date, end=end_date, interval="1d")
            
            if hist.empty:
                logger.warning(f"ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {symbol}")
                return None
                
            # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            hist = hist.dropna()
            
            # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¿½åŠ 
            data = self.calculate_technical_indicators(hist)
            
            # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™
            data = self.prepare_features_target(data)
            
            return data.dropna()
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¨ãƒ©ãƒ¼ [{symbol}]: {e}")
            return None
            
    def calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—"""
        data = df.copy()
        
        # ç§»å‹•å¹³å‡ç·š
        data['sma_5'] = data['Close'].rolling(window=5).mean()
        data['sma_10'] = data['Close'].rolling(window=10).mean()
        data['sma_20'] = data['Close'].rolling(window=20).mean()
        data['sma_50'] = data['Close'].rolling(window=50).mean()
        
        # æŒ‡æ•°ç§»å‹•å¹³å‡ç·š
        data['ema_12'] = data['Close'].ewm(span=12).mean()
        data['ema_26'] = data['Close'].ewm(span=26).mean()
        
        # RSI
        delta = data['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        data['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD
        data['macd'] = data['ema_12'] - data['ema_26']
        
        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        bb_period = 20
        bb_std = 2
        bb_ma = data['Close'].rolling(window=bb_period).mean()
        bb_std_val = data['Close'].rolling(window=bb_period).std()
        data['bb_upper'] = bb_ma + (bb_std_val * bb_std)
        data['bb_lower'] = bb_ma - (bb_std_val * bb_std)
        
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ æŒ‡æ¨™
        data['volume_sma'] = data['Volume'].rolling(window=20).mean()
        
        # ä¾¡æ ¼å¤‰å‹•
        data['price_change'] = data['Close'].pct_change()
        data['volatility'] = data['price_change'].rolling(window=20).std()
        
        return data
        
    def prepare_features_target(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾´é‡ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™"""
        # ç‰¹å¾´é‡é¸æŠ
        feature_columns = [col for col in self.config['technical_indicators'] if col in data.columns]
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: 5æ—¥å¾Œã®çµ‚å€¤
        data['target'] = data['Close'].shift(-5)
        
        # ç‰¹å¾´é‡ã®ã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
        features_data = data[feature_columns + ['target']].copy()
        
        self.feature_columns = feature_columns
        
        return features_data
        
    async def train_models(self, symbol: str, data: pd.DataFrame) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"""
        logger.info(f"ğŸ”§ ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹: {symbol}")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X = data[self.feature_columns].values
        y = data['target'].values
        
        # NaNé™¤å»
        mask = ~np.isnan(y)
        X, y = X[mask], y[mask]
        
        if len(X) < 50:  # æœ€ä½ãƒ‡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {symbol} ({len(X)}ä»¶)")
            return {'best_model': None, 'trained_models': []}
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=1-self.config['training_ratio'], random_state=42
        )
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        self.scalers[symbol] = scaler
        
        # ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»è©•ä¾¡
        model_results = []
        trained_models = []
        
        for model_name, model_config in self.config['models'].items():
            try:
                # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
                model = model_config['class']()
                grid_search = GridSearchCV(
                    model, model_config['params'], 
                    cv=3, scoring='neg_mean_absolute_error',
                    n_jobs=-1
                )
                
                grid_search.fit(X_train_scaled, y_train)
                best_model = grid_search.best_estimator_
                
                # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
                y_pred = best_model.predict(X_test_scaled)
                
                mae = mean_absolute_error(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                
                accuracy = max(0, 1 - (mae / np.mean(y_test)))  # ç›¸å¯¾èª¤å·®åŸºæº–ã®ç²¾åº¦
                
                model_result = {
                    'model_name': model_name,
                    'model': best_model,
                    'mae': mae,
                    'mse': mse,
                    'r2': r2,
                    'accuracy': accuracy,
                    'best_params': grid_search.best_params_
                }
                
                model_results.append(model_result)
                trained_models.append(model_name)
                
                logger.info(f"  {model_name}: MAE={mae:.4f}, R2={r2:.4f}, ç²¾åº¦={accuracy:.3%}")
                
            except Exception as e:
                logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ [{model_name}]: {e}")
                
        # æœ€é©ãƒ¢ãƒ‡ãƒ«é¸æŠ
        if model_results:
            best_model_result = max(model_results, key=lambda x: x['accuracy'])
            
            if best_model_result['accuracy'] >= self.config['min_accuracy_threshold']:
                # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                model_file = os.path.join(self.config['models_dir'], f"{symbol}_{best_model_result['model_name']}.joblib")
                joblib.dump(best_model_result['model'], model_file)
                
                scaler_file = os.path.join(self.config['models_dir'], f"{symbol}_scaler.joblib")
                joblib.dump(scaler, scaler_file)
                
                logger.info(f"âœ… æœ€é©ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {best_model_result['model_name']} (ç²¾åº¦: {best_model_result['accuracy']:.3%})")
                
                return {
                    'best_model': best_model_result,
                    'trained_models': trained_models,
                    'all_results': model_results
                }
            else:
                logger.warning(f"ç²¾åº¦ä¸è¶³: {symbol} (æœ€å¤§ç²¾åº¦: {best_model_result['accuracy']:.3%})")
                
        return {'best_model': None, 'trained_models': trained_models}
        
    async def generate_predictions(self, symbol: str, data: pd.DataFrame, best_model_result: Dict) -> Dict:
        """äºˆæ¸¬ç”Ÿæˆ"""
        try:
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
            latest_data = data.tail(1)
            X_latest = latest_data[self.feature_columns].values
            
            if symbol in self.scalers:
                X_latest_scaled = self.scalers[symbol].transform(X_latest)
                
                # äºˆæ¸¬å®Ÿè¡Œ
                prediction = best_model_result['model'].predict(X_latest_scaled)[0]
                current_price = latest_data['target'].iloc[0] if not pd.isna(latest_data['target'].iloc[0]) else data['Close'].iloc[-1]
                
                # äºˆæ¸¬æœŸé–“ã®ä¾¡æ ¼æ¨ç§»ç”Ÿæˆ (ç°¡æ˜“ç‰ˆ)
                predictions = []
                base_price = current_price
                
                for i in range(self.config['prediction_days']):
                    # ç°¡æ˜“ãªä¾¡æ ¼æ¨ç§»ãƒ¢ãƒ‡ãƒ« (å®Ÿéš›ã«ã¯ã‚ˆã‚Šè¤‡é›‘ãªäºˆæ¸¬ãŒå¿…è¦)
                    daily_change = np.random.normal(0, 0.02)  # æ—¥æ¬¡å¤‰å‹•
                    trend_factor = (prediction - base_price) / base_price * 0.1  # ãƒˆãƒ¬ãƒ³ãƒ‰è¦å› 
                    
                    predicted_price = base_price * (1 + trend_factor + daily_change)
                    
                    predictions.append({
                        'date': (datetime.now() + timedelta(days=i+1)).strftime('%Y-%m-%d'),
                        'predicted_price': round(predicted_price, 2),
                        'confidence': max(0.5, best_model_result['accuracy'] * 0.9),  # ä¿¡é ¼åº¦
                        'model_used': best_model_result['model_name']
                    })
                    
                    base_price = predicted_price
                    
                return {
                    'symbol': symbol,
                    'predictions': predictions,
                    'model_accuracy': best_model_result['accuracy'],
                    'model_name': best_model_result['model_name'],
                    'generated_at': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"äºˆæ¸¬ç”Ÿæˆã‚¨ãƒ©ãƒ¼ [{symbol}]: {e}")
            
        return {'symbol': symbol, 'predictions': [], 'error': 'prediction_failed'}
        
    async def save_predictions(self, symbol: str, predictions: Dict):
        """äºˆæ¸¬çµæœä¿å­˜"""
        try:
            prediction_file = os.path.join(
                self.config['predictions_dir'], 
                f"{symbol}_predictions_{datetime.now().strftime('%Y%m%d')}.json"
            )
            
            with open(prediction_file, 'w', encoding='utf-8') as f:
                json.dump(predictions, f, indent=2, ensure_ascii=False)
                
            logger.debug(f"äºˆæ¸¬ä¿å­˜å®Œäº†: {prediction_file}")
            
        except Exception as e:
            logger.error(f"äºˆæ¸¬ä¿å­˜ã‚¨ãƒ©ãƒ¼ [{symbol}]: {e}")
            
    async def train_lstm_model(self, symbol: str, data: pd.DataFrame) -> Optional[Dict]:
        """LSTMæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° (é«˜åº¦ç‰ˆ)"""
        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            sequence_length = 60  # 60æ—¥åˆ†ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
            price_data = data['Close'].values.reshape(-1, 1)
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(price_data)
            
            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            X, y = [], []
            for i in range(sequence_length, len(scaled_data)):
                X.append(scaled_data[i-sequence_length:i, 0])
                y.append(scaled_data[i, 0])
                
            X, y = np.array(X), np.array(y)
            X = X.reshape((X.shape[0], X.shape[1], 1))
            
            if len(X) < 100:  # æœ€ä½ãƒ‡ãƒ¼ã‚¿æ•°
                return None
                
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            split_idx = int(len(X) * self.config['training_ratio'])
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            # LSTMãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
            model = Sequential([
                LSTM(50, return_sequences=True, input_shape=(sequence_length, 1)),
                Dropout(0.2),
                LSTM(50, return_sequences=False),
                Dropout(0.2),
                Dense(25),
                Dense(1)
            ])
            
            model.compile(optimizer='adam', loss='mean_squared_error')
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            model.fit(X_train, y_train, batch_size=32, epochs=50, verbose=0)
            
            # è©•ä¾¡
            y_pred = model.predict(X_test)
            mae = mean_absolute_error(y_test, y_pred)
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            model_path = os.path.join(self.config['models_dir'], f"{symbol}_lstm.h5")
            model.save(model_path)
            
            scaler_path = os.path.join(self.config['models_dir'], f"{symbol}_lstm_scaler.joblib")
            joblib.dump(scaler, scaler_path)
            
            return {
                'model_type': 'lstm',
                'mae': mae,
                'accuracy': max(0, 1 - mae),
                'model_path': model_path,
                'scaler_path': scaler_path
            }
            
        except Exception as e:
            logger.error(f"LSTMè¨“ç·´ã‚¨ãƒ©ãƒ¼ [{symbol}]: {e}")
            return None
            
    async def generate_ml_report(self, results: Dict):
        """ML ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        try:
            report = {
                'training_summary': results,
                'model_performance': {
                    'total_symbols': results['symbols_processed'],
                    'successful_predictions': results['predictions_generated'],
                    'success_rate': results['predictions_generated'] / max(1, results['symbols_processed']),
                    'error_rate': len(results['errors']) / max(1, results['symbols_processed'])
                },
                'timestamp': datetime.now().isoformat(),
                'next_training': (datetime.now() + timedelta(days=self.config['retrain_threshold_days'])).isoformat()
            }
            
            report_file = os.path.join(
                self.config['reports_dir'],
                f"ml_training_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
                
            logger.info(f"ğŸ“‹ MLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_file}")
            logger.info(f"   æˆåŠŸç‡: {report['model_performance']['success_rate']:.1%}")
            logger.info(f"   ã‚¨ãƒ©ãƒ¼ç‡: {report['model_performance']['error_rate']:.1%}")
            
        except Exception as e:
            logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            
    async def cleanup_old_models(self, days: int = 30):
        """å¤ã„ãƒ¢ãƒ‡ãƒ«ãƒ»äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            cutoff_time = datetime.now() - timedelta(days=days)
            
            for directory in [self.config['models_dir'], self.config['predictions_dir']]:
                for filename in os.listdir(directory):
                    file_path = os.path.join(directory, filename)
                    if os.path.getmtime(file_path) < cutoff_time.timestamp():
                        os.remove(file_path)
                        logger.debug(f"å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {filename}")
                        
        except Exception as e:
            logger.error(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

# ãƒãƒƒãƒå‡¦ç†çµ±åˆç”¨ã‚¯ãƒ©ã‚¹
class MLBatchIntegration:
    """MLã‚·ã‚¹ãƒ†ãƒ ã®ãƒãƒƒãƒå‡¦ç†çµ±åˆ"""
    
    def __init__(self):
        self.ml_system = MLPredictionSystem()
        
    async def run_daily_ml_tasks(self) -> Dict:
        """æ—¥æ¬¡MLå‡¦ç†"""
        logger.info("ğŸ¤– æ—¥æ¬¡MLå‡¦ç†é–‹å§‹")
        
        # ä¸»è¦éŠ˜æŸ„ã®äºˆæ¸¬æ›´æ–°
        major_symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA', '7203.T', '9984.T', '6758.T']
        results = await self.ml_system.run_ml_training_batch(major_symbols)
        
        return results
        
    async def run_weekly_ml_tasks(self) -> Dict:
        """é€±æ¬¡MLå‡¦ç†"""
        logger.info("ğŸ¤– é€±æ¬¡MLå‡¦ç†é–‹å§‹")
        
        # å…¨éŠ˜æŸ„ã®ãƒ¢ãƒ‡ãƒ«æ›´æ–°
        # (å®Ÿè£…æ™‚ã¯ã€APIã‹ã‚‰éŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’å–å¾—)
        all_symbols = await self._get_all_symbols()
        
        results = await self.ml_system.run_ml_training_batch(all_symbols[:100])  # æœ€åˆã®100éŠ˜æŸ„
        
        # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await self.ml_system.cleanup_old_models()
        
        return results
        
    async def _get_all_symbols(self) -> List[str]:
        """å…¨éŠ˜æŸ„ãƒªã‚¹ãƒˆå–å¾—"""
        try:
            import requests
            response = requests.get('http://localhost:8000/api/finance/stocks/search?query=', timeout=10)
            if response.status_code == 200:
                # å®Ÿè£…æ™‚ã¯APIã‹ã‚‰éŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’å–å¾—
                pass
        except:
            pass
            
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«éŠ˜æŸ„
        return ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA', 'META', 'AMZN', '7203.T', '9984.T']

async def main():
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    ml_batch = MLBatchIntegration()
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = await ml_batch.run_daily_ml_tasks()
    print(f"MLå‡¦ç†çµæœ: {results}")

if __name__ == "__main__":
    asyncio.run(main())