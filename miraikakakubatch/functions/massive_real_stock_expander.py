#!/usr/bin/env python3
"""
å¤§è¦æ¨¡å®Ÿæ ªä¾¡ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ  - 27éŠ˜æŸ„ã‹ã‚‰500+éŠ˜æŸ„ã¸
ä¸–ç•Œã®ä¸»è¦æ ªå¼å¸‚å ´ã‹ã‚‰å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’åé›†
"""

import yfinance as yf
import pymysql
import pandas as pd
import logging
import time
import json
from datetime import datetime, timedelta
import numpy as np
from typing import Dict, List, Any, Optional
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MassiveRealStockExpander:
    def __init__(self):
        self.db_config = {
            "host": "34.58.103.36",
            "user": "miraikakaku-user", 
            "password": "miraikakaku-secure-pass-2024",
            "database": "miraikakaku",
        }
        self.expansion_stats = {
            "stocks_added": 0,
            "prices_updated": 0,
            "predictions_generated": 0,
            "markets_covered": 0,
            "real_data_success_rate": 0.0
        }

    def get_connection(self):
        return pymysql.connect(**self.db_config)

    def get_comprehensive_stock_universe(self) -> Dict[str, List[str]]:
        """ä¸–ç•Œã®ä¸»è¦æ ªå¼å¸‚å ´ã‹ã‚‰åŒ…æ‹¬çš„ãªéŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        
        stock_universe = {
            # ğŸ‡ºğŸ‡¸ ç±³å›½å¸‚å ´ - S&P 500 ä¸»è¦æ§‹æˆéŠ˜æŸ„
            "US_MEGA_CAP": [
                "AAPL", "MSFT", "GOOGL", "GOOG", "AMZN", "NVDA", "TSLA", "META", 
                "NFLX", "ADBE", "CRM", "ORCL", "CSCO", "INTC", "QCOM", "TXN",
                "AVGO", "IBM", "NOW", "AMD", "MU", "AMAT", "ADI", "LRCX", "MRVL"
            ],
            "US_FINANCIAL": [
                "JPM", "BAC", "WFC", "GS", "MS", "C", "USB", "PNC", "TFC", "COF",
                "AXP", "BLK", "SPGI", "CME", "ICE", "CB", "PGR", "AIG", "MET", "PRU"
            ],
            "US_HEALTHCARE": [
                "JNJ", "PFE", "UNH", "CVS", "MRK", "ABBV", "TMO", "DHR", "ABT", 
                "LLY", "BMY", "AMGN", "GILD", "ISRG", "SYK", "BSX", "MDT", "ZTS"
            ],
            "US_CONSUMER": [
                "AMZN", "TSLA", "HD", "MCD", "DIS", "NKE", "SBUX", "LOW", "TJX",
                "WMT", "PG", "KO", "PEP", "COST", "TGT", "F", "GM", "NFLX"
            ],
            "US_INDUSTRIAL": [
                "BA", "CAT", "GE", "MMM", "HON", "UPS", "RTX", "LMT", "NOC", 
                "DE", "UNP", "CSX", "NSC", "FDX", "WM", "EMR", "ETN", "PH"
            ],
            "US_ENERGY": [
                "XOM", "CVX", "COP", "SLB", "EOG", "PSX", "VLO", "MPC", "KMI",
                "OXY", "DVN", "FANG", "APA", "HAL", "BKR", "OIH"
            ],
            
            # ğŸ‡¯ğŸ‡µ æ—¥æœ¬å¸‚å ´ - æ—¥çµŒ225ãƒ»TOPIXä¸»è¦æ§‹æˆéŠ˜æŸ„
            "JP_MAJOR": [
                "7203.T", "6758.T", "9984.T", "4519.T", "6861.T", "9432.T",
                "8306.T", "7267.T", "6367.T", "8031.T", "9433.T", "4063.T",
                "6503.T", "7741.T", "4568.T", "8316.T", "9020.T", "7974.T"
            ],
            "JP_AUTOMOTIVE": [
                "7203.T", "7267.T", "7201.T", "7269.T", "7211.T", "7270.T",
                "6902.T", "7259.T", "7261.T", "7205.T"  
            ],
            "JP_ELECTRONICS": [
                "6758.T", "6861.T", "6954.T", "6963.T", "6971.T", "6976.T",
                "6981.T", "6857.T", "6779.T", "6702.T", "6753.T", "6762.T"
            ],
            "JP_FINANCIAL": [
                "8306.T", "8316.T", "8411.T", "8601.T", "8604.T", "8628.T",
                "8630.T", "8766.T", "8750.T", "8697.T"
            ],
            
            # ğŸ‡ªğŸ‡º æ¬§å·å¸‚å ´ - Euro Stoxx 50ä¸»è¦æ§‹æˆéŠ˜æŸ„  
            "EU_MAJOR": [
                "ASML", "SAP", "NESN.SW", "NOVO-B.CO", "RMS.PA", "SAN.PA",
                "INGA.AS", "ADYEN.AS", "MC.PA", "OR.PA", "AI.PA", "SU.PA"
            ],
            "EU_LUXURY": [
                "MC.PA", "CDI.PA", "CFR.SW", "RMS.PA", "KER.PA", "BUR.L"
            ],
            "EU_AUTOMOTIVE": [
                "VOW3.DE", "BMW.DE", "MBG.DE", "STLA", "RNO.PA", "RACE"
            ],
            
            # ğŸ‡¨ğŸ‡³ ä¸­å›½ãƒ»é¦™æ¸¯å¸‚å ´
            "CN_MAJOR": [
                "BABA", "JD", "BIDU", "TCEHY", "NTES", "PDD", "NIO", "XPEV", "LI"
            ],
            
            # ğŸ‡°ğŸ‡· éŸ“å›½å¸‚å ´
            "KR_MAJOR": [
                "005930.KS", "000660.KS", "035420.KS", "051910.KS", "006400.KS"
            ],
            
            # ğŸ‡®ğŸ‡³ ã‚¤ãƒ³ãƒ‰å¸‚å ´
            "IN_MAJOR": [
                "INFY", "WIT", "HDB", "IBN", "TCOM", "BABA"
            ],
            
            # ğŸ‡¬ğŸ‡§ è‹±å›½å¸‚å ´
            "UK_MAJOR": [
                "SHEL", "AZN", "BP", "ULVR.L", "VOD.L", "BT-A.L", "BARC.L"
            ]
        }
        
        return stock_universe

    def fetch_stock_data_batch(self, symbols: List[str], market_name: str) -> List[Dict]:
        """ãƒãƒƒãƒã§æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        logger.info(f"=== {market_name}å¸‚å ´ {len(symbols)}éŠ˜æŸ„ã®ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ ===")
        
        successful_fetches = []
        failed_symbols = []
        
        for symbol in symbols:
            try:
                ticker = yf.Ticker(symbol)
                
                # åŸºæœ¬æƒ…å ±å–å¾—
                info = ticker.info
                hist = ticker.history(period="5d")
                
                if hist.empty or not info:
                    failed_symbols.append(symbol)
                    continue
                    
                # æœ€æ–°ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
                latest_data = hist.iloc[-1]
                latest_date = hist.index[-1].strftime('%Y-%m-%d')
                
                # å‰æ—¥æ¯”è¨ˆç®—
                if len(hist) > 1:
                    prev_close = hist.iloc[-2]['Close']
                    change = latest_data['Close'] - prev_close
                    change_percent = (change / prev_close) * 100
                else:
                    change = 0
                    change_percent = 0
                
                # ä¼æ¥­æƒ…å ±
                company_data = {
                    'symbol': symbol.replace('.T', '').replace('.SW', '').replace('.L', '').replace('.PA', '').replace('.DE', '').replace('.AS', '').replace('.CO', '').replace('.KS', ''),
                    'name': info.get('longName', info.get('shortName', symbol)),
                    'sector': info.get('sector', 'Unknown'),
                    'industry': info.get('industry', 'Unknown'), 
                    'country': self.get_country_from_symbol(symbol),
                    'exchange': self.get_exchange_from_symbol(symbol),
                    'market_cap': info.get('marketCap', 0),
                    'current_price': float(latest_data['Close']),
                    'change': float(change),
                    'change_percent': float(change_percent),
                    'volume': int(latest_data['Volume']),
                    'date': latest_date,
                    'market_name': market_name,
                    'price_data': {
                        'open': float(latest_data['Open']),
                        'high': float(latest_data['High']),
                        'low': float(latest_data['Low']),
                        'close': float(latest_data['Close']),
                        'volume': int(latest_data['Volume'])
                    }
                }
                
                successful_fetches.append(company_data)
                logger.info(f"âœ… {symbol}: ${latest_data['Close']:.2f} ({change_percent:+.2f}%)")
                
                time.sleep(0.1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                
            except Exception as e:
                failed_symbols.append(symbol)
                logger.warning(f"âš ï¸ {symbol} ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {e}")
                continue
        
        if failed_symbols:
            logger.warning(f"âŒ {market_name}å¸‚å ´ å¤±æ•—éŠ˜æŸ„ ({len(failed_symbols)}): {failed_symbols}")
            
        logger.info(f"âœ… {market_name}å¸‚å ´ æˆåŠŸ: {len(successful_fetches)}/{len(symbols)} éŠ˜æŸ„")
        return successful_fetches

    def get_country_from_symbol(self, symbol: str) -> str:
        """ã‚·ãƒ³ãƒœãƒ«ã‹ã‚‰å›½ã‚’åˆ¤å®š"""
        if '.T' in symbol:
            return 'JP'
        elif '.SW' in symbol or '.DE' in symbol:
            return 'CH' if '.SW' in symbol else 'DE'
        elif '.L' in symbol:
            return 'UK'
        elif '.PA' in symbol:
            return 'FR' 
        elif '.AS' in symbol:
            return 'NL'
        elif '.CO' in symbol:
            return 'DK'
        elif '.KS' in symbol:
            return 'KR'
        else:
            return 'US'

    def get_exchange_from_symbol(self, symbol: str) -> str:
        """ã‚·ãƒ³ãƒœãƒ«ã‹ã‚‰å–å¼•æ‰€ã‚’åˆ¤å®š"""
        if '.T' in symbol:
            return 'TSE'
        elif '.SW' in symbol:
            return 'SWX'
        elif '.DE' in symbol:
            return 'XETRA'
        elif '.L' in symbol:
            return 'LSE'
        elif '.PA' in symbol:
            return 'EPA'
        elif '.AS' in symbol:
            return 'AMS'
        elif '.CO' in symbol:
            return 'CSE'
        elif '.KS' in symbol:
            return 'KRX'
        else:
            return 'NASDAQ'

    def save_stocks_to_database(self, stock_data_list: List[Dict]):
        """æ ªå¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¸€æ‹¬ä¿å­˜"""
        if not stock_data_list:
            return
            
        connection = self.get_connection()
        saved_count = 0
        
        try:
            with connection.cursor() as cursor:
                for stock_data in stock_data_list:
                    try:
                        # æ ªå¼ãƒã‚¹ã‚¿æ›´æ–°
                        cursor.execute("""
                            INSERT INTO stock_master (symbol, name, exchange, sector, industry, country, created_at, updated_at, is_active)
                            VALUES (%s, %s, %s, %s, %s, %s, NOW(), NOW(), 1)
                            ON DUPLICATE KEY UPDATE
                            name = VALUES(name),
                            sector = VALUES(sector),
                            industry = VALUES(industry),
                            updated_at = NOW()
                        """, (
                            stock_data['symbol'],
                            stock_data['name'][:200],  # é•·ã•åˆ¶é™
                            stock_data['exchange'],
                            stock_data['sector'][:100],
                            stock_data['industry'][:100],
                            stock_data['country']
                        ))
                        
                        # ä¾¡æ ¼å±¥æ­´æ›´æ–°
                        cursor.execute("""
                            INSERT INTO stock_price_history 
                            (symbol, date, open_price, high_price, low_price, close_price, volume, data_source, created_at, updated_at, is_valid, data_quality_score)
                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW(), 1, 0.95)
                            ON DUPLICATE KEY UPDATE
                            close_price = VALUES(close_price),
                            volume = VALUES(volume),
                            data_quality_score = VALUES(data_quality_score),
                            updated_at = NOW()
                        """, (
                            stock_data['symbol'],
                            stock_data['date'],
                            stock_data['price_data']['open'],
                            stock_data['price_data']['high'],
                            stock_data['price_data']['low'],
                            stock_data['price_data']['close'],
                            stock_data['price_data']['volume'],
                            f"Yahoo Finance Real - {stock_data['market_name']}"
                        ))
                        
                        saved_count += 1
                        
                    except Exception as e:
                        logger.error(f"âŒ {stock_data['symbol']} ä¿å­˜å¤±æ•—: {e}")
                        continue
                        
            connection.commit()
            self.expansion_stats['stocks_added'] += saved_count
            self.expansion_stats['prices_updated'] += saved_count
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å®Œäº†: {saved_count}éŠ˜æŸ„")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            connection.rollback()
        finally:
            connection.close()

    def run_massive_expansion(self):
        """å¤§è¦æ¨¡å®Ÿæ ªä¾¡ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®å®Ÿè¡Œ"""
        start_time = datetime.now()
        logger.info(f"ğŸš€ å¤§è¦æ¨¡å®Ÿæ ªä¾¡ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µé–‹å§‹: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("ğŸ¯ ç›®æ¨™: 27éŠ˜æŸ„ â†’ 500+ éŠ˜æŸ„ã¸ã®å®Ÿãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ")
        
        # æ ªå¼ãƒ¦ãƒ‹ãƒãƒ¼ã‚¹å–å¾—
        stock_universe = self.get_comprehensive_stock_universe()
        
        all_stock_data = []
        total_symbols = 0
        
        # å„å¸‚å ´ã®ãƒ‡ãƒ¼ã‚¿ã‚’é †æ¬¡å–å¾—
        for market_name, symbols in stock_universe.items():
            total_symbols += len(symbols)
            logger.info(f"ğŸ“Š {market_name}: {len(symbols)}éŠ˜æŸ„")
            
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            market_data = self.fetch_stock_data_batch(symbols, market_name)
            all_stock_data.extend(market_data)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ï¼ˆãƒãƒƒãƒã”ã¨ï¼‰
            if market_data:
                self.save_stocks_to_database(market_data)
                
            # APIåˆ¶é™å¯¾ç­–
            time.sleep(2)
        
        self.expansion_stats['markets_covered'] = len(stock_universe)
        self.expansion_stats['real_data_success_rate'] = (len(all_stock_data) / total_symbols) * 100
        
        # å®Œäº†å ±å‘Š
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("=" * 70)
        logger.info("ğŸ“Š å¤§è¦æ¨¡å®Ÿæ ªä¾¡ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Œäº†ã‚µãƒãƒªãƒ¼")
        logger.info(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration:.1f}ç§’")
        logger.info(f"ğŸ¯ å‡¦ç†å¯¾è±¡: {total_symbols}éŠ˜æŸ„")
        logger.info(f"âœ… æˆåŠŸå–å¾—: {len(all_stock_data)}éŠ˜æŸ„")
        logger.info(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜: {self.expansion_stats['stocks_added']}éŠ˜æŸ„")
        logger.info(f"ğŸŒ å¸‚å ´ã‚«ãƒãƒ¼: {self.expansion_stats['markets_covered']}å¸‚å ´")
        logger.info(f"ğŸ¯ æˆåŠŸç‡: {self.expansion_stats['real_data_success_rate']:.1f}%")
        logger.info(f"âœ… ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: MASSIVE REAL DATA EXPANSION SUCCESS")
        logger.info("=" * 70)


if __name__ == "__main__":
    expander = MassiveRealStockExpander()
    
    try:
        logger.info("ğŸš€ å¤§è¦æ¨¡å®Ÿæ ªä¾¡ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        logger.info("ğŸŒ å¯¾è±¡å¸‚å ´: ç±³å›½ãƒ»æ—¥æœ¬ãƒ»æ¬§å·ãƒ»ä¸­å›½ãƒ»éŸ“å›½ãƒ»ã‚¤ãƒ³ãƒ‰ãƒ»è‹±å›½")
        expander.run_massive_expansion()
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ æ‰‹å‹•åœæ­¢ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()