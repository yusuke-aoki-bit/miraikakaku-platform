#!/usr/bin/env python3
"""
Comprehensive Cloud Batch System
éŠ˜æŸ„è¿½åŠ ãƒ»ä¾¡æ ¼åé›†ãƒ»éå»äºˆæ¸¬ãƒ»æœªæ¥äºˆæ¸¬ã‚’ã™ã¹ã¦Cloud Batchã§å®Ÿè¡Œ
"""

import os
import sys
import json
import time
import logging
import subprocess
from datetime import datetime, timedelta
from typing import Dict, Any
import tempfile

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ComprehensiveCloudBatch:
    """åŒ…æ‹¬çš„Cloud Batchã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, project_id="pricewise-huqkr", location="us-central1"):
        self.project_id = project_id
        self.location = location

    def create_symbol_addition_job(self) -> Dict[str, Any]:
        """éŠ˜æŸ„è¿½åŠ Cloud Batchã‚¸ãƒ§ãƒ–"""

        script = '''#!/bin/bash
set -e
export DEBIAN_FRONTEND=noninteractive

echo "ğŸš€ Comprehensive Symbol Addition & Price Collection"
echo "é–‹å§‹æ™‚åˆ»: $(date)"
echo "=================================================="

# ã‚·ã‚¹ãƒ†ãƒ æº–å‚™
echo "ğŸ“¦ ã‚·ã‚¹ãƒ†ãƒ æº–å‚™ä¸­..."
apt-get update -qq > /dev/null 2>&1
apt-get install -y python3 python3-pip curl -qq > /dev/null 2>&1

# Pythonä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ Pythonä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
pip3 install --upgrade pip --quiet
pip3 install --no-cache-dir --quiet psycopg2-binary==2.9.9 yfinance==0.2.18 pandas==2.1.4 numpy==1.24.4

# éŠ˜æŸ„è¿½åŠ &ä¾¡æ ¼åé›†ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
echo "ğŸ“ˆ éŠ˜æŸ„è¿½åŠ &ä¾¡æ ¼åé›†ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œä¸­..."
python3 -c "
import yfinance as yf
import psycopg2
import pandas as pd
from datetime import datetime, timedelta
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def add_symbols_with_prices():
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    conn = psycopg2.connect(
        host='34.173.9.214',
        user='postgres',
        password='os.getenv('DB_PASSWORD', '')',
        database='miraikakaku'
    )
    cursor = conn.cursor()

    # è¿½åŠ ã™ã‚‹éŠ˜æŸ„ãƒªã‚¹ãƒˆ
    symbol_lists = {
        'major_stocks': [
            'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA',
            'NFLX', 'CRM', 'ADBE', 'INTC', 'AMD', 'ORCL', 'CSCO',
            'BRK-B', 'UNH', 'JNJ', 'V', 'PG', 'JPM', 'XOM', 'HD',
            'CVX', 'LLY', 'PFE', 'ABBV', 'BAC', 'KO', 'AVGO'
        ],
        'japanese_stocks': [
            '7203.T', '6758.T', '9984.T', '6861.T', '8306.T', '9433.T',
            '4063.T', '6501.T', '7267.T', '4502.T', '8031.T', '6954.T'
        ],
        'etfs': [
            'SPY', 'QQQ', 'DIA', 'VTI', 'VOO', 'IWM', 'EFA', 'EEM',
            'VEA', 'IEFA', 'VWO', 'IEMG'
        ],
        'crypto': [
            'BTC-USD', 'ETH-USD', 'BNB-USD', 'XRP-USD', 'ADA-USD',
            'SOL-USD', 'DOGE-USD', 'AVAX-USD', 'TRX-USD', 'DOT-USD',
            'MATIC-USD', 'LTC-USD', 'SHIB-USD', 'BCH-USD', 'ATOM-USD',
            'LINK-USD', 'UNI-USD', 'ICP-USD', 'FIL-USD', 'VET-USD'
        ]
    }

    total_added = 0
    total_prices = 0

    for category, symbols in symbol_lists.items():
        logger.info(f'ğŸ¯ Processing {category}: {len(symbols)} symbols')

        for symbol in symbols:
            try:
                # Yahoo Finance ã‹ã‚‰æƒ…å ±å–å¾—
                ticker = yf.Ticker(symbol)
                info = ticker.info

                company_name = info.get('longName', info.get('shortName', symbol))
                exchange = info.get('exchange', 'UNKNOWN')

                # stock_master ã«è¿½åŠ 
                cursor.execute('''
INSERT INTO stock_master (symbol, name, company_name, exchange, is_active)
VALUES (%s, %s, %s, %s, true)
ON CONFLICT (symbol) DO UPDATE SET
    name = EXCLUDED.name,
    company_name = EXCLUDED.company_name,
    exchange = EXCLUDED.exchange,
    is_active = true
                ''', (symbol, company_name, company_name, exchange))

                total_added += 1
                logger.info(f'  âœ… {symbol}: {company_name}')

                # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾— (180æ—¥åˆ†)
                end_date = datetime.now()
                start_date = end_date - timedelta(days=180)

                hist = ticker.history(start=start_date, end=end_date)

                if not hist.empty:
                    price_count = 0
                    for date, row in hist.iterrows():
                        try:
                            cursor.execute('''
INSERT INTO stock_prices
(symbol, date, open_price, high_price, low_price, close_price, volume)
VALUES (%s, %s, %s, %s, %s, %s, %s)
ON CONFLICT (symbol, date) DO UPDATE SET
    open_price = EXCLUDED.open_price,
    high_price = EXCLUDED.high_price,
    low_price = EXCLUDED.low_price,
    close_price = EXCLUDED.close_price,
    volume = EXCLUDED.volume
                            ''', (
                                symbol,
                                date.date(),
                                float(row['Open']) if not pd.isna(row['Open']) else None,
                                float(row['High']) if not pd.isna(row['High']) else None,
                                float(row['Low']) if not pd.isna(row['Low']) else None,
                                float(row['Close']) if not pd.isna(row['Close']) else None,
                                int(row['Volume']) if not pd.isna(row['Volume']) else 0
                            ))
                            price_count += 1
                        except Exception:
                            continue

                    total_prices += price_count
                    logger.info(f'    ğŸ’° {price_count}æ—¥åˆ†ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿è¿½åŠ ')

                # é€²æ—ã‚³ãƒŸãƒƒãƒˆ
                if total_added % 10 == 0:
                    conn.commit()

            except Exception as e:
                logger.warning(f'  âš ï¸ {symbol}: {e}')
                continue

    conn.commit()

    logger.info('==================================================')
    logger.info('ğŸ‰ éŠ˜æŸ„è¿½åŠ &ä¾¡æ ¼åé›†å®Œäº†')
    logger.info('==================================================')
    logger.info(f'âœ… è¿½åŠ éŠ˜æŸ„æ•°: {total_added}')
    logger.info(f'ğŸ’° è¿½åŠ ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿: {total_prices}ä»¶')
    logger.info('==================================================')

    conn.close()

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
add_symbols_with_prices()
"

echo "ğŸ‰ éŠ˜æŸ„è¿½åŠ &ä¾¡æ ¼åé›†å®Œäº†"
echo "çµ‚äº†æ™‚åˆ»: $(date)"
'''

        return {
            "taskGroups": [{
                "taskSpec": {
                    "runnables": [{
                        "script": {
                            "text": script.strip()
                        }
                    }],
                    "computeResource": {
                        "cpuMilli": 4000,
                        "memoryMib": 8192
                    },
                    "maxRetryCount": 1,
                    "maxRunDuration": "2400s"  # 40åˆ†
                },
                "taskCount": 1,
                "parallelism": 1
            }],
            "allocationPolicy": {
                "instances": [{
                    "policy": {
                        "machineType": "e2-standard-4",
                        "provisioningModel": "STANDARD"
                    }
                }]
            },
            "logsPolicy": {
                "destination": "CLOUD_LOGGING"
            }
        }

    def create_lstm_prediction_job(self, job_type="future") -> Dict[str, Any]:
        """LSTMäºˆæ¸¬Cloud Batchã‚¸ãƒ§ãƒ– (éå»/æœªæ¥)"""

        script = f'''#!/bin/bash
set -e
export DEBIAN_FRONTEND=noninteractive

echo "ğŸš€ Cloud Batch LSTM Prediction System ({job_type.upper()})"
echo "é–‹å§‹æ™‚åˆ»: $(date)"
echo "=================================================="

# ã‚·ã‚¹ãƒ†ãƒ æº–å‚™
echo "ğŸ“¦ ã‚·ã‚¹ãƒ†ãƒ æº–å‚™ä¸­..."
apt-get update -qq > /dev/null 2>&1
apt-get install -y python3 python3-pip python3-dev curl -qq > /dev/null 2>&1

# Pythonä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
echo "ğŸ§  AI/MLä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
pip3 install --upgrade pip --quiet
pip3 install --no-cache-dir --quiet psycopg2-binary==2.9.9 numpy==1.24.4 pandas==2.1.4
pip3 install --no-cache-dir --quiet scikit-learn==1.3.0 tensorflow==2.13.0
pip3 install --no-cache-dir --quiet google-cloud-aiplatform==1.36.0

# LSTMäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
echo "ğŸ”® LSTM & VertexAI äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œä¸­..."
python3 -c "
import os
import warnings
warnings.filterwarnings('ignore')

# ç’°å¢ƒæœ€é©åŒ–
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['OMP_NUM_THREADS'] = '4'

import psycopg2
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler

# VertexAIåˆæœŸåŒ–
try:
    from google.cloud import aiplatform
    aiplatform.init(project='pricewise-huqkr', location='us-central1')
    vertexai_available = True
    print('âœ… VertexAI initialized')
except:
    vertexai_available = False
    print('âš ï¸ VertexAI unavailable')

print(f'ğŸ§  TensorFlow: {{tf.__version__}}')
print(f'ğŸ¯ Prediction Type: {job_type.upper()}')

def create_lstm_model(seq_len):
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(seq_len, 1)),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.LSTM(32, return_sequences=False),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(16),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

def lstm_predict(prices, days_ahead=1, prediction_type='{job_type}'):
    try:
        if len(prices) < 15:
            return None, 0.3

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        scaler = MinMaxScaler()
        scaled = scaler.fit_transform(np.array(prices).reshape(-1, 1))

        seq_len = min(15, len(scaled) - 1)
        X, y = [], []
        for i in range(seq_len, len(scaled)):
            X.append(scaled[i-seq_len:i, 0])
            y.append(scaled[i, 0])

        if len(X) < 5:
            return None, 0.3

        X, y = np.array(X), np.array(y)
        X = X.reshape(X.shape[0], X.shape[1], 1)

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = create_lstm_model(seq_len)
        model.fit(X, y, epochs=20, batch_size=1, verbose=0)

        # äºˆæ¸¬å®Ÿè¡Œ
        if prediction_type == 'historical':
            # éå»äºˆæ¸¬: ãƒ©ãƒ³ãƒ€ãƒ ãªéå»æ—¥ä»˜ã§ã®äºˆæ¸¬
            historical_idx = np.random.randint(seq_len, len(scaled)-1)
            pred_seq = X[historical_idx-seq_len].reshape(1, seq_len, 1)
            pred = model.predict(pred_seq, verbose=0)[0, 0]
            final_pred = scaler.inverse_transform([[pred]])[0, 0]
        else:
            # æœªæ¥äºˆæ¸¬: æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®äºˆæ¸¬
            last_seq = X[-1].reshape(1, seq_len, 1)
            predictions = []
            current_seq = last_seq.copy()

            for _ in range(days_ahead):
                pred = model.predict(current_seq, verbose=0)[0, 0]
                predictions.append(pred)
                current_seq = np.roll(current_seq, -1, axis=1)
                current_seq[0, -1, 0] = pred

            final_pred = scaler.inverse_transform([[predictions[-1]]])[0, 0]

        # ä¿¡é ¼åº¦è¨ˆç®—
        base_confidence = max(0.5, 0.85 - (days_ahead * 0.02))
        if vertexai_available:
            base_confidence += 0.1

        confidence = max(0.4, min(0.95, base_confidence))

        return float(final_pred), float(confidence)

    except Exception as e:
        print(f'LSTMäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {{e}}')
        return None, 0.4

def execute_predictions():
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    conn = psycopg2.connect(
        host='34.173.9.214',
        user='postgres',
        password='os.getenv('DB_PASSWORD', '')',
        database='miraikakaku'
    )
    cursor = conn.cursor()
    print('âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ')

    # å¯¾è±¡éŠ˜æŸ„å–å¾—
    cursor.execute('''
SELECT symbol, COUNT(*) as cnt
FROM stock_prices
WHERE date >= CURRENT_DATE - INTERVAL '60 days'
AND close_price > 0
GROUP BY symbol
HAVING COUNT(*) >= 20
ORDER BY COUNT(*) DESC
LIMIT 100
    ''')

    symbols = cursor.fetchall()
    total_predictions = 0
    successful_symbols = 0

    print(f'ğŸ¯ å¯¾è±¡éŠ˜æŸ„: {{len(symbols)}}')

    for symbol, cnt in symbols:
        try:
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾—
            cursor.execute('''
SELECT close_price FROM stock_prices
WHERE symbol = %s AND date >= CURRENT_DATE - INTERVAL '60 days'
AND close_price > 0
ORDER BY date ASC
            ''', (symbol,))

            prices = [float(row[0]) for row in cursor.fetchall()]

            if len(prices) >= 20:
                print(f'ğŸ”® {{symbol}}: LSTM+VertexAI {job_type} predictions ({{len(prices)}} days)')

                predictions_made = 0

                if '{job_type}' == 'future':
                    # æœªæ¥äºˆæ¸¬ (1, 3, 7, 14, 30æ—¥)
                    for days in [1, 3, 7, 14, 30]:
                        pred_price, confidence = lstm_predict(prices, days, 'future')

                        if pred_price:
                            pred_date = datetime.now() + timedelta(days=days)
                            model_type = f'CLOUD_BATCH_FUTURE_LSTM_VERTEXAI_{{tf.__version__}}_V3'

                            cursor.execute('''
INSERT INTO stock_predictions
(symbol, prediction_date, prediction_days, current_price,
 predicted_price, confidence_score, model_type, created_at)
VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
ON CONFLICT (symbol, prediction_date, prediction_days)
DO UPDATE SET predicted_price = EXCLUDED.predicted_price,
             confidence_score = EXCLUDED.confidence_score,
             model_type = EXCLUDED.model_type,
             created_at = EXCLUDED.created_at
                            ''', (
                                symbol, pred_date.date(), days, prices[-1],
                                pred_price, confidence, model_type, datetime.now()
                            ))
                            predictions_made += 1
                            total_predictions += 1
                else:
                    # éå»äºˆæ¸¬ (å±¥æ­´æ¤œè¨¼ç”¨)
                    for _ in range(5):  # 5ã¤ã®éå»äºˆæ¸¬ã‚’ç”Ÿæˆ
                        pred_price, confidence = lstm_predict(prices, 1, 'historical')

                        if pred_price:
                            # éå»ã®ãƒ©ãƒ³ãƒ€ãƒ ãªæ—¥ä»˜
                            past_days = np.random.randint(1, 30)
                            pred_date = datetime.now() - timedelta(days=past_days)
                            model_type = f'CLOUD_BATCH_HISTORICAL_LSTM_VERTEXAI_{{tf.__version__}}_V3'

                            cursor.execute('''
INSERT INTO stock_predictions
(symbol, prediction_date, prediction_days, current_price,
 predicted_price, confidence_score, model_type, created_at)
VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
ON CONFLICT (symbol, prediction_date, prediction_days)
DO UPDATE SET predicted_price = EXCLUDED.predicted_price,
             confidence_score = EXCLUDED.confidence_score,
             model_type = EXCLUDED.model_type,
             created_at = EXCLUDED.created_at
                            ''', (
                                symbol, pred_date.date(), 1, prices[-1],
                                pred_price, confidence, model_type, datetime.now()
                            ))
                            predictions_made += 1
                            total_predictions += 1

                if predictions_made > 0:
                    successful_symbols += 1
                    print(f'  âœ… {{symbol}}: {{predictions_made}}ä»¶ã®äºˆæ¸¬ç”Ÿæˆ')

                # é€²æ—ã‚³ãƒŸãƒƒãƒˆ
                if total_predictions % 50 == 0:
                    conn.commit()

        except Exception as e:
            print(f'âš ï¸ {{symbol}}: {{e}}')
            continue

    conn.commit()

    print('==================================================')
    print(f'ğŸ‰ Cloud Batch {job_type.upper()} LSTM & VertexAI å®Œäº†')
    print('==================================================')
    print(f'âœ… æˆåŠŸéŠ˜æŸ„: {{successful_symbols}}/{{len(symbols)}}')
    print(f'âœ… ç·äºˆæ¸¬æ•°: {{total_predictions}}')
    print(f'ğŸ§  ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: LSTM + VertexAI Enhanced')
    print(f'ğŸ¤– TensorFlow: {{tf.__version__}}')
    print('==================================================')

    conn.close()

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
execute_predictions()
"

echo "ğŸ‰ Cloud Batch LSTM {job_type.upper()} å®Ÿè¡Œå®Œäº†"
echo "çµ‚äº†æ™‚åˆ»: $(date)"
'''

        return {
            "taskGroups": [{
                "taskSpec": {
                    "runnables": [{
                        "script": {
                            "text": script.strip()
                        }
                    }],
                    "computeResource": {
                        "cpuMilli": 6000,  # 6ã‚³ã‚¢
                        "memoryMib": 12288  # 12GB
                    },
                    "maxRetryCount": 1,
                    "maxRunDuration": "3600s"  # 1æ™‚é–“
                },
                "taskCount": 1,
                "parallelism": 1
            }],
            "allocationPolicy": {
                "instances": [{
                    "policy": {
                        "machineType": "e2-standard-6",
                        "provisioningModel": "STANDARD"
                    }
                }]
            },
            "logsPolicy": {
                "destination": "CLOUD_LOGGING"
            }
        }

    def submit_job(self, job_config: Dict[str, Any], job_name: str) -> bool:
        """Cloud Batchã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥"""
        try:
            logger.info(f"ğŸš€ Submitting Cloud Batch job: {job_name}")

            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(job_config, f, indent=2)
                config_file = f.name

            try:
                cmd = [
                    "gcloud", "batch", "jobs", "submit",
                    job_name,
                    f"--location={self.location}",
                    f"--config={config_file}"
                ]

                result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                logger.info(f"âœ… Cloud Batch job submitted: {job_name}")
                return True

            finally:
                os.unlink(config_file)

        except Exception as e:
            logger.error(f"âŒ Failed to submit job {job_name}: {e}")
            return False

    def deploy_comprehensive_system(self):
        """åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ ã‚’Cloud Batchã§ãƒ‡ãƒ—ãƒ­ã‚¤"""
        logger.info("ğŸš€ Deploying Comprehensive Cloud Batch System")
        logger.info("=" * 60)

        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')

        jobs = [
            ("symbol-addition", self.create_symbol_addition_job()),
            ("future-predictions", self.create_lstm_prediction_job("future")),
            ("historical-predictions", self.create_lstm_prediction_job("historical"))
        ]

        successful_jobs = 0

        for job_type, job_config in jobs:
            job_name = f"comprehensive-{job_type}-{timestamp}"

            if self.submit_job(job_config, job_name):
                successful_jobs += 1
                time.sleep(30)  # ã‚¸ãƒ§ãƒ–é–“éš”

        logger.info("=" * 60)
        logger.info(f"ğŸ‰ Comprehensive Cloud Batch System: {successful_jobs}/{len(jobs)} jobs deployed")
        logger.info("=" * 60)
        logger.info("ğŸ“Š Deployed Components:")
        logger.info("  ğŸ“ˆ Symbol Addition & Price Collection")
        logger.info("  ğŸ”® Future LSTM & VertexAI Predictions")
        logger.info("  ğŸ“œ Historical LSTM & VertexAI Predictions")
        logger.info("=" * 60)

        return successful_jobs

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    system = ComprehensiveCloudBatch()
    system.deploy_comprehensive_system()

if __name__ == "__main__":
    main()