#!/usr/bin/env python3
"""
ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ 
æ—¢å­˜ã®ãƒãƒƒãƒå‡¦ç†ã¨ä¸¦è¡Œã—ã¦ETFãƒ»ç‚ºæ›¿ãƒ»ç±³å›½æ ªãƒ‡ãƒ¼ã‚¿ã‚’åé›†
"""

import yfinance as yf
import psycopg2
import psycopg2.extras
import logging
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import threading

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ParallelDataCollection:
    def __init__(self):
        self.db_config = {
            "host": "34.173.9.214",
            "user": "postgres", 
            "password": "miraikakaku-postgres-secure-2024",
            "database": "miraikakaku",
        }
        self.stats = {
            "us_stocks": {"processed": 0, "successful": 0, "failed": 0},
            "etfs": {"processed": 0, "successful": 0, "failed": 0},
            "forex": {"processed": 0, "successful": 0, "failed": 0}
        }
        self.lock = threading.Lock()

    def get_connection(self):
        return psycopg2.connect(**self.db_config)

    def collect_major_us_stocks(self):
        """ä¸»è¦ç±³å›½æ ªåé›†"""
        major_us_stocks = [
            "AAPL", "MSFT", "GOOGL", "GOOG", "AMZN", "TSLA", "META", "NVDA",
            "NFLX", "AMD", "CRM", "ORCL", "ADBE", "INTC", "CSCO", "IBM",
            "PYPL", "UBER", "LYFT", "ZOOM", "SHOP", "SQ", "TWTR", "SNAP",
            "BA", "GE", "F", "GM", "CAT", "DE", "MMM", "HON",
            "JPM", "BAC", "WFC", "C", "GS", "MS", "V", "MA",
            "KO", "PEP", "MCD", "SBUX", "NKE", "DIS", "WMT", "TGT"
        ]
        
        logger.info(f"ğŸ‡ºğŸ‡¸ ä¸»è¦ç±³å›½æ ªåé›†é–‹å§‹: {len(major_us_stocks)}éŠ˜æŸ„")
        
        for symbol in major_us_stocks:
            try:
                success = self.fetch_and_save_stock_data(symbol, "US Major Stock")
                with self.lock:
                    self.stats["us_stocks"]["processed"] += 1
                    if success:
                        self.stats["us_stocks"]["successful"] += 1
                        logger.info(f"âœ… {symbol}: ç±³å›½æ ªãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
                    else:
                        self.stats["us_stocks"]["failed"] += 1
                
                time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                
            except Exception as e:
                with self.lock:
                    self.stats["us_stocks"]["failed"] += 1
                logger.error(f"âŒ {symbol}: {e}")

    def collect_major_etfs(self):
        """ä¸»è¦ETFåé›†"""
        major_etfs = [
            # ç±³å›½ETF
            "SPY", "QQQ", "IWM", "VTI", "VOO", "IVV", "VEA", "VWO",
            "AGG", "BND", "TLT", "GLD", "SLV", "USO", "UNG", "XLF",
            "XLK", "XLE", "XLV", "XLI", "XLB", "XLU", "XLP", "XLRE",
            # æ—¥æœ¬ETF
            "1306.T", "1321.T", "1570.T", "1591.T", "2558.T", "1563.T"
        ]
        
        logger.info(f"ğŸ“Š ä¸»è¦ETFåé›†é–‹å§‹: {len(major_etfs)}éŠ˜æŸ„")
        
        for symbol in major_etfs:
            try:
                success = self.fetch_and_save_stock_data(symbol, "Major ETF")
                with self.lock:
                    self.stats["etfs"]["processed"] += 1
                    if success:
                        self.stats["etfs"]["successful"] += 1
                        logger.info(f"âœ… {symbol}: ETFãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
                    else:
                        self.stats["etfs"]["failed"] += 1
                
                time.sleep(0.5)
                
            except Exception as e:
                with self.lock:
                    self.stats["etfs"]["failed"] += 1
                logger.error(f"âŒ {symbol}: {e}")

    def collect_major_forex(self):
        """ä¸»è¦ç‚ºæ›¿ãƒšã‚¢åé›†"""
        major_forex = [
            "USDJPY=X", "EURJPY=X", "GBPJPY=X", "AUDJPY=X", "CHFJPY=X",
            "CADJPY=X", "NZDJPY=X", "ZARJPY=X", "EURUSD=X", "GBPUSD=X",
            "AUDUSD=X", "USDCHF=X", "USDCAD=X", "NZDUSD=X", "EURGBP=X",
            "EURAUD=X", "GBPAUD=X", "AUDCAD=X", "EURCHF=X", "GBPCHF=X"
        ]
        
        logger.info(f"ğŸ’± ä¸»è¦ç‚ºæ›¿ãƒšã‚¢åé›†é–‹å§‹: {len(major_forex)}ãƒšã‚¢")
        
        for symbol in major_forex:
            try:
                success = self.collect_forex_data(symbol)
                with self.lock:
                    self.stats["forex"]["processed"] += 1
                    if success:
                        self.stats["forex"]["successful"] += 1
                        logger.info(f"âœ… {symbol}: ç‚ºæ›¿ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
                    else:
                        self.stats["forex"]["failed"] += 1
                
                time.sleep(0.3)
                
            except Exception as e:
                with self.lock:
                    self.stats["forex"]["failed"] += 1
                logger.error(f"âŒ {symbol}: {e}")

    def fetch_and_save_stock_data(self, symbol, data_source):
        """æ ªä¾¡ãƒ»ETFãƒ‡ãƒ¼ã‚¿å–å¾—ã¨ä¿å­˜"""
        try:
            ticker = yf.Ticker(symbol)
            hist = ticker.history(period="5d")
            
            if hist.empty:
                return False
            
            latest_data = hist.iloc[-1]
            latest_date = hist.index[-1].strftime('%Y-%m-%d')
            
            connection = self.get_connection()
            try:
                with connection.cursor() as cursor:
                    cursor.execute("""
                        INSERT INTO stock_price_history 
                        (symbol, date, open_price, high_price, low_price, close_price, volume, 
                         data_source, created_at, updated_at, is_valid, data_quality_score)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW(), 1, 0.95)
                        ON DUPLICATE KEY UPDATE
                        close_price = VALUES(close_price),
                        volume = VALUES(volume),
                        updated_at = NOW()
                    """, (
                        symbol, latest_date,
                        float(latest_data['Open']),
                        float(latest_data['High']),
                        float(latest_data['Low']),
                        float(latest_data['Close']),
                        int(latest_data['Volume']),
                        f"Parallel Collection - {data_source}"
                    ))
                    
                connection.commit()
                return True
                
            finally:
                connection.close()
                
        except Exception:
            return False

    def collect_forex_data(self, fx_symbol):
        """ç‚ºæ›¿ãƒ‡ãƒ¼ã‚¿åé›†"""
        try:
            ticker = yf.Ticker(fx_symbol)
            hist = ticker.history(period="5d")
            
            if hist.empty:
                return False
            
            latest_data = hist.iloc[-1]
            latest_date = hist.index[-1].strftime('%Y-%m-%d')
            
            # ç‚ºæ›¿ãƒ‡ãƒ¼ã‚¿ã¯åˆ¥é€”ãƒ­ã‚°å‡ºåŠ›ï¼ˆDBã¸ã®ä¿å­˜ã¯ç°¡ç•¥åŒ–ï¼‰
            logger.info(f"ğŸ’± {fx_symbol}: {latest_data['Close']:.4f} @ {latest_date}")
            return True
            
        except Exception:
            return False

    def run_parallel_collection(self):
        """ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè¡Œ"""
        start_time = datetime.now()
        logger.info(f"ğŸš€ ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 3ã¤ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ä¸¦åˆ—å®Ÿè¡Œ
        with ThreadPoolExecutor(max_workers=3) as executor:
            # ç±³å›½æ ªåé›†
            us_future = executor.submit(self.collect_major_us_stocks)
            
            # ETFåé›†
            etf_future = executor.submit(self.collect_major_etfs)
            
            # ç‚ºæ›¿åé›†
            fx_future = executor.submit(self.collect_major_forex)
            
            # ã™ã¹ã¦ã®å‡¦ç†å®Œäº†ã‚’å¾…æ©Ÿ
            us_future.result()
            etf_future.result() 
            fx_future.result()
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        logger.info("=" * 80)
        logger.info("ğŸ“Š ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
        logger.info(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration:.1f}ç§’")
        
        total_processed = sum(cat["processed"] for cat in self.stats.values())
        total_successful = sum(cat["successful"] for cat in self.stats.values())
        total_failed = sum(cat["failed"] for cat in self.stats.values())
        
        logger.info(f"ğŸ“ˆ ç·åˆçµæœ:")
        logger.info(f"  ğŸ¯ ç·å‡¦ç†: {total_processed}ä»¶")
        logger.info(f"  âœ… ç·æˆåŠŸ: {total_successful}ä»¶")
        logger.info(f"  âŒ ç·å¤±æ•—: {total_failed}ä»¶")
        logger.info(f"  ğŸ“Š æˆåŠŸç‡: {(total_successful/total_processed*100):.1f}%")
        
        logger.info(f"ğŸ‡ºğŸ‡¸ ç±³å›½æ ª:")
        logger.info(f"  å‡¦ç†: {self.stats['us_stocks']['processed']}ä»¶")
        logger.info(f"  æˆåŠŸ: {self.stats['us_stocks']['successful']}ä»¶")
        
        logger.info(f"ğŸ“Š ETF:")
        logger.info(f"  å‡¦ç†: {self.stats['etfs']['processed']}ä»¶")
        logger.info(f"  æˆåŠŸ: {self.stats['etfs']['successful']}ä»¶")
        
        logger.info(f"ğŸ’± ç‚ºæ›¿:")
        logger.info(f"  å‡¦ç†: {self.stats['forex']['processed']}ä»¶")
        logger.info(f"  æˆåŠŸ: {self.stats['forex']['successful']}ä»¶")
        
        logger.info("=" * 80)

if __name__ == "__main__":
    collector = ParallelDataCollection()
    
    try:
        collector.run_parallel_collection()
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ æ‰‹å‹•åœæ­¢")
    except Exception as e:
        logger.error(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()