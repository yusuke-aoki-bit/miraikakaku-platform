#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pymysql
import requests
import json
import logging
import time
import random
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MultiSourceRealDataExpander:
    def __init__(self):
        self.db_config = {
            "host": "34.58.103.36",
            "user": "miraikakaku-user",
            "password": "miraikakaku-secure-pass-2024",
            "database": "miraikakaku",
            "charset": "utf8mb4"
        }

    def get_uncovered_symbols_fast(self, limit=300):
        """ã‚«ãƒãƒ¼ç‡ã®ä½ã„éŠ˜æŸ„ã‚’é«˜é€Ÿå–å¾—"""
        try:
            connection = pymysql.connect(**self.db_config)
            with connection.cursor() as cursor:
                # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹éŠ˜æŸ„ã‚’åŠ¹ç‡çš„ã«å–å¾—
                cursor.execute("""
                    SELECT sm.symbol, sm.name, sm.country, sm.exchange
                    FROM stock_master sm
                    WHERE sm.is_active = 1
                    AND (
                        SELECT COUNT(*) FROM stock_price_history sph 
                        WHERE sph.symbol = sm.symbol 
                        AND sph.created_at >= DATE_SUB(NOW(), INTERVAL 30 DAY)
                    ) = 0
                    ORDER BY 
                        CASE WHEN sm.country = 'US' THEN 1 ELSE 2 END,
                        RAND()
                    LIMIT %s
                """, (limit,))
                return cursor.fetchall()
        except Exception as e:
            logger.error(f"éŠ˜æŸ„å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
        finally:
            if 'connection' in locals():
                connection.close()

    def fetch_yahoo_csv_data(self, symbol, country=None):
        """Yahoo Finance CSV APIã§ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        try:
            # ã‚·ãƒ³ãƒœãƒ«å¤‰æ›
            if country == 'Japan' and symbol.isdigit():
                yf_symbol = f"{symbol}.T"
            else:
                yf_symbol = symbol
            
            # Yahoo Finance CSV API
            end_date = datetime.now()
            start_date = end_date - timedelta(days=365)
            
            url = f"https://query1.finance.yahoo.com/v7/finance/download/{yf_symbol}"
            params = {
                'period1': int(start_date.timestamp()),
                'period2': int(end_date.timestamp()),
                'interval': '1d',
                'events': 'history'
            }
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, params=params, headers=headers, timeout=10)
            
            if response.status_code != 200:
                return None
            
            # CSVè§£æ
            lines = response.text.strip().split('\n')
            if len(lines) < 2:
                return None
            
            price_data = []
            for line in lines[1:]:
                try:
                    values = line.split(',')
                    if len(values) < 7:
                        continue
                    
                    date_str = values[0]
                    open_price = float(values[1])
                    high_price = float(values[2])
                    low_price = float(values[3])
                    close_price = float(values[4])
                    adj_close = float(values[5])
                    volume = int(values[6])
                    
                    # æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                    if any(v <= 0 for v in [open_price, high_price, low_price, close_price]) or volume < 0:
                        continue
                    
                    price_data.append({
                        'symbol': symbol,
                        'date': datetime.strptime(date_str, '%Y-%m-%d').strftime('%Y-%m-%d'),
                        'open_price': open_price,
                        'high_price': high_price,
                        'low_price': low_price,
                        'close_price': close_price,
                        'adjusted_close': adj_close,
                        'volume': volume,
                        'data_source': 'yahoo_csv',
                        'is_valid': 1,
                        'data_quality_score': 0.93
                    })
                except (ValueError, IndexError):
                    continue
            
            return price_data if len(price_data) >= 20 else None
            
        except Exception as e:
            logger.warning(f"Yahoo CSVå¤±æ•— {symbol}: {e}")
            return None

    def fetch_financial_modeling_prep(self, symbol):
        """Financial Modeling Prep APIï¼ˆç„¡æ–™ç‰ˆï¼‰"""
        try:
            # ç„¡æ–™ç‰ˆã®FMP APIï¼ˆåˆ¶é™ã‚ã‚Šï¼‰
            url = f"https://financialmodelingprep.com/api/v3/historical-price-full/{symbol}"
            params = {
                'apikey': 'demo',  # ãƒ‡ãƒ¢ã‚­ãƒ¼ï¼ˆåˆ¶é™ã‚ã‚Šï¼‰
                'from': (datetime.now() - timedelta(days=90)).strftime('%Y-%m-%d'),
                'to': datetime.now().strftime('%Y-%m-%d')
            }
            
            response = requests.get(url, params=params, timeout=10)
            
            if response.status_code != 200:
                return None
            
            data = response.json()
            
            if 'historical' not in data:
                return None
            
            price_data = []
            for record in data['historical'][:50]:  # åˆ¶é™å¯¾å¿œ
                try:
                    price_data.append({
                        'symbol': symbol,
                        'date': record['date'],
                        'open_price': float(record['open']),
                        'high_price': float(record['high']),
                        'low_price': float(record['low']),
                        'close_price': float(record['close']),
                        'adjusted_close': float(record['adjClose']),
                        'volume': int(record['volume']),
                        'data_source': 'fmp_api',
                        'is_valid': 1,
                        'data_quality_score': 0.90
                    })
                except (KeyError, ValueError):
                    continue
            
            return price_data if price_data else None
            
        except Exception as e:
            logger.warning(f"FMP APIå¤±æ•— {symbol}: {e}")
            return None

    def fetch_alphavantage_demo(self, symbol):
        """Alpha Vantage ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        try:
            # Alpha Vantage ãƒ‡ãƒ¢API
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'TIME_SERIES_DAILY',
                'symbol': symbol,
                'apikey': 'demo',
                'outputsize': 'compact'
            }
            
            response = requests.get(url, params=params, timeout=10)
            
            if response.status_code != 200:
                return None
            
            data = response.json()
            
            if 'Time Series (Daily)' not in data:
                return None
            
            price_data = []
            time_series = data['Time Series (Daily)']
            
            for date_str, values in list(time_series.items())[:30]:  # åˆ¶é™å¯¾å¿œ
                try:
                    price_data.append({
                        'symbol': symbol,
                        'date': date_str,
                        'open_price': float(values['1. open']),
                        'high_price': float(values['2. high']),
                        'low_price': float(values['3. low']),
                        'close_price': float(values['4. close']),
                        'adjusted_close': float(values['4. close']),
                        'volume': int(values['5. volume']),
                        'data_source': 'alphavantage',
                        'is_valid': 1,
                        'data_quality_score': 0.92
                    })
                except (KeyError, ValueError):
                    continue
            
            return price_data if price_data else None
            
        except Exception as e:
            logger.warning(f"Alpha Vantageå¤±æ•— {symbol}: {e}")
            return None

    def multi_source_fetch(self, symbol_info):
        """è¤‡æ•°ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—è©¦è¡Œ"""
        symbol, name, country, exchange = symbol_info
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å„ªå…ˆé †ä½
        fetch_methods = [
            ('yahoo_csv', self.fetch_yahoo_csv_data),
            ('fmp', self.fetch_financial_modeling_prep),
            ('alphavantage', self.fetch_alphavantage_demo)
        ]
        
        for source_name, fetch_method in fetch_methods:
            try:
                if source_name == 'yahoo_csv':
                    result = fetch_method(symbol, country)
                else:
                    result = fetch_method(symbol)
                
                if result and len(result) >= 10:  # æœ€ä½10ä»¶ã®ãƒ‡ãƒ¼ã‚¿
                    return {
                        'symbol': symbol,
                        'status': 'success',
                        'source': source_name,
                        'data': result,
                        'count': len(result)
                    }
                
                # å¤±æ•—æ™‚ã¯å°‘ã—å¾…æ©Ÿ
                time.sleep(0.2)
                
            except Exception as e:
                logger.warning(f"{source_name} {symbol}å¤±æ•—: {e}")
                continue
        
        return {
            'symbol': symbol,
            'status': 'all_failed',
            'source': 'none',
            'data': None,
            'count': 0
        }

    def save_multi_source_data(self, data_list):
        """ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        if not data_list:
            return 0
            
        try:
            connection = pymysql.connect(**self.db_config)
            with connection.cursor() as cursor:
                insert_data = []
                for data in data_list:
                    insert_data.append((
                        data['symbol'], data['date'],
                        data['open_price'], data['high_price'],
                        data['low_price'], data['close_price'],
                        data['volume'], data['adjusted_close'],
                        data['data_source'], data['is_valid'],
                        data['data_quality_score']
                    ))
                
                cursor.executemany("""
                    INSERT IGNORE INTO stock_price_history 
                    (symbol, date, open_price, high_price, low_price, close_price,
                     volume, adjusted_close, data_source, is_valid, data_quality_score, created_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                """, insert_data)
                
                connection.commit()
                return cursor.rowcount
                
        except Exception as e:
            logger.error(f"ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
        finally:
            if 'connection' in locals():
                connection.close()

    def expand_with_multi_sources(self, target_symbols=200, max_workers=6):
        """ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹ã§ã®å®Ÿãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ"""
        logger.info(f"ğŸŒ ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹å®Ÿãƒ‡ãƒ¼ã‚¿æ‹¡å¼µé–‹å§‹ - ç›®æ¨™: {target_symbols}éŠ˜æŸ„")
        
        # æœªã‚«ãƒãƒ¼éŠ˜æŸ„å–å¾—
        uncovered_symbols = self.get_uncovered_symbols_fast(target_symbols)
        logger.info(f"ğŸ“‹ æœªã‚«ãƒãƒ¼éŠ˜æŸ„: {len(uncovered_symbols)}éŠ˜æŸ„")
        
        if not uncovered_symbols:
            logger.info("âš ï¸ å‡¦ç†å¯¾è±¡éŠ˜æŸ„ãªã—")
            return {'total': 0, 'successful': 0, 'failed': 0, 'total_records': 0}
        
        successful = 0
        failed = 0
        total_records = 0
        source_stats = {}
        
        # ä¸¦åˆ—å‡¦ç†ã§ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self.multi_source_fetch, symbol_info): symbol_info[0]
                for symbol_info in uncovered_symbols
            }
            
            for future in as_completed(futures):
                symbol = futures[future]
                try:
                    result = future.result()
                    
                    if result['status'] == 'success':
                        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                        saved_count = self.save_multi_source_data(result['data'])
                        if saved_count > 0:
                            successful += 1
                            total_records += saved_count
                            source = result['source']
                            source_stats[source] = source_stats.get(source, 0) + 1
                            logger.info(f"âœ… {symbol} ({source}): {saved_count}ä»¶ä¿å­˜")
                        else:
                            failed += 1
                            logger.warning(f"âš ï¸ {symbol}: ä¿å­˜å¤±æ•—")
                    else:
                        failed += 1
                        logger.warning(f"âŒ {symbol}: å…¨ã‚½ãƒ¼ã‚¹å¤±æ•—")
                    
                    # é€²æ—å ±å‘Š
                    if (successful + failed) % 25 == 0:
                        progress = ((successful + failed) / len(uncovered_symbols)) * 100
                        success_rate = (successful / (successful + failed)) * 100
                        logger.info(f"ğŸ“ˆ é€²æ—: {progress:.1f}% | æˆåŠŸç‡: {success_rate:.1f}% | ãƒ‡ãƒ¼ã‚¿: {total_records:,}ä»¶")
                
                except Exception as e:
                    failed += 1
                    logger.error(f"âŒ {symbol}: å‡¦ç†ã‚¨ãƒ©ãƒ¼ - {e}")
        
        # æœ€çµ‚çµæœ
        total_processed = successful + failed
        final_success_rate = (successful / total_processed * 100) if total_processed > 0 else 0
        
        logger.info(f"ğŸ¯ ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹å®Ÿãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Œäº†:")
        logger.info(f"   - å‡¦ç†éŠ˜æŸ„: {total_processed}")
        logger.info(f"   - æˆåŠŸéŠ˜æŸ„: {successful} ({final_success_rate:.1f}%)")
        logger.info(f"   - å¤±æ•—éŠ˜æŸ„: {failed}")
        logger.info(f"   - åé›†ãƒ‡ãƒ¼ã‚¿: {total_records:,}ä»¶")
        
        logger.info("ğŸ“Š ã‚½ãƒ¼ã‚¹åˆ¥æˆåŠŸæ•°:")
        for source, count in source_stats.items():
            logger.info(f"   - {source}: {count}éŠ˜æŸ„")
        
        return {
            'total': total_processed,
            'successful': successful,
            'failed': failed,
            'total_records': total_records,
            'success_rate': final_success_rate,
            'source_stats': source_stats
        }

def main():
    logger.info("ğŸŒ ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹å®Ÿãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    expander = MultiSourceRealDataExpander()
    result = expander.expand_with_multi_sources(target_symbols=200, max_workers=6)
    
    if result['successful'] > 0:
        logger.info("âœ… ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹æ‹¡å¼µæˆåŠŸ - è©•ä¾¡å®Ÿè¡Œä¸­...")
        
        # æ‹¡å¼µå¾Œã®è©•ä¾¡
        import subprocess
        subprocess.run(["python3", "collation_safe_data_assessment.py"])
    else:
        logger.error("âŒ ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹æ‹¡å¼µå¤±æ•—")

if __name__ == "__main__":
    main()