#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import psycopg2
import psycopg2.extras
import yfinance as yf
import pandas as pd
import logging
import time
import asyncio
import aiohttp
import requests
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RapidBatchRealDataCollector:
    def __init__(self):
        self.db_config = {
            "host": "34.173.9.214",
            "user": "postgres",
            "password": "miraikakaku-postgres-secure-2024",
            "database": "miraikakaku",
            "port": 5432
        }
        self.lock = threading.Lock()
        self.batch_results = []

    def get_high_priority_missing_symbols(self, limit=500):
        """é«˜å„ªå…ˆåº¦ã§æœªå‡¦ç†éŠ˜æŸ„ã‚’å–å¾—"""
        try:
            connection = psycopg2.connect(**self.db_config)
            with connection.cursor() as cursor:
                # å„ªå…ˆåº¦ä»˜ãéŠ˜æŸ„å–å¾—ï¼ˆUS, æ—¥æœ¬, ä¸»è¦å–å¼•æ‰€å„ªå…ˆï¼‰
                cursor.execute("""
                    SELECT sm.symbol, sm.name, sm.country, sm.exchange, sm.sector
                    FROM stock_master sm
                    WHERE sm.is_active = 1 
                    AND sm.symbol NOT IN (
                        SELECT DISTINCT symbol FROM stock_price_history 
                        WHERE data_source = 'yfinance' AND created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY)
                    )
                    ORDER BY 
                        CASE 
                            WHEN sm.country IN ('US', 'United States') THEN 1
                            WHEN sm.country = 'Japan' THEN 2
                            WHEN sm.exchange IN ('NYSE', 'NASDAQ', 'TSE', 'LSE') THEN 3
                            ELSE 4
                        END,
                        CASE 
                            WHEN sm.sector IN ('Technology', 'Healthcare', 'Finance') THEN 1
                            WHEN sm.sector IN ('Energy', 'Consumer', 'Industrial') THEN 2
                            ELSE 3
                        END,
                        CHAR_LENGTH(sm.symbol) ASC,
                        sm.symbol
                    LIMIT %s
                """, (limit,))
                return cursor.fetchall()
        except Exception as e:
            logger.error(f"å„ªå…ˆåº¦éŠ˜æŸ„å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
        finally:
            if 'connection' in locals():
                connection.close()

    def smart_symbol_conversion(self, symbol, country, exchange):
        """ã‚¹ãƒãƒ¼ãƒˆãªã‚·ãƒ³ãƒœãƒ«å¤‰æ›"""
        # æ—¥æœ¬æ ª
        if country == 'Japan' or exchange == 'TSE':
            if symbol.isdigit() and len(symbol) >= 4:
                return f"{symbol}.T"
        
        # é€šè²¨ãƒšã‚¢
        if symbol.endswith('=X') or 'USD' in symbol or 'EUR' in symbol:
            return symbol
        
        # æ¬§å·æ ª
        european_exchanges = ['LSE', 'EPA', 'AMS', 'SWX', 'FRA']
        if exchange in european_exchanges:
            if exchange == 'LSE':
                return f"{symbol}.L"
            elif exchange == 'EPA':
                return f"{symbol}.PA"
            elif exchange == 'AMS':
                return f"{symbol}.AS"
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãã®ã¾ã¾ï¼ˆUSæ ªï¼‰
        return symbol

    def fetch_yfinance_optimized(self, symbol_info):
        """æœ€é©åŒ–ã•ã‚ŒãŸyfinanceãƒ‡ãƒ¼ã‚¿å–å¾—"""
        symbol, name, country, exchange, sector = symbol_info
        
        try:
            # ã‚·ãƒ³ãƒœãƒ«å¤‰æ›
            yf_symbol = self.smart_symbol_conversion(symbol, country, exchange)
            
            # yfinance tickerå–å¾—
            ticker = yf.Ticker(yf_symbol)
            
            # æœŸé–“è¨­å®šï¼ˆéå»6ãƒ¶æœˆã§é«˜é€ŸåŒ–ï¼‰
            end_date = datetime.now()
            start_date = end_date - timedelta(days=180)
            
            # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆè¤‡æ•°è©¦è¡Œï¼‰
            hist_data = None
            for attempt in range(2):  # è©¦è¡Œå›æ•°å‰Šæ¸›
                try:
                    hist_data = ticker.history(
                        start=start_date.strftime('%Y-%m-%d'),
                        end=end_date.strftime('%Y-%m-%d'),
                        interval='1d',
                        auto_adjust=True,
                        prepost=False
                    )
                    if not hist_data.empty:
                        break
                except Exception as e:
                    if attempt == 0:
                        time.sleep(0.5)
                    continue
            
            if hist_data is None or hist_data.empty:
                return {'symbol': symbol, 'status': 'no_data', 'data': None, 'count': 0}
                
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            price_data = []
            for date, row in hist_data.iterrows():
                try:
                    # åŸºæœ¬æ¤œè¨¼
                    values = [row['Open'], row['High'], row['Low'], row['Close'], row['Volume']]
                    if any(pd.isna(values)) or any(v <= 0 for v in values[:4]) or row['Volume'] < 0:
                        continue
                    
                    price_data.append({
                        'symbol': symbol,
                        'date': date.strftime('%Y-%m-%d'),
                        'open_price': float(row['Open']),
                        'high_price': float(row['High']),
                        'low_price': float(row['Low']),
                        'close_price': float(row['Close']),
                        'adjusted_close': float(row['Close']),
                        'volume': int(row['Volume']),
                        'data_source': 'yfinance',
                        'is_valid': 1,
                        'data_quality_score': 0.95
                    })
                except (ValueError, OverflowError, KeyError):
                    continue
            
            if len(price_data) >= 30:  # æœ€ä½30æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿æˆåŠŸ
                return {
                    'symbol': symbol, 
                    'status': 'success', 
                    'data': price_data, 
                    'count': len(price_data),
                    'yf_symbol': yf_symbol
                }
            else:
                return {'symbol': symbol, 'status': 'insufficient_data', 'data': None, 'count': len(price_data)}
                
        except Exception as e:
            return {'symbol': symbol, 'status': 'error', 'data': None, 'count': 0, 'error': str(e)}

    def save_batch_data_optimized(self, batch_data):
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        if not batch_data:
            return 0
            
        try:
            connection = psycopg2.connect(**self.db_config)
            with connection.cursor() as cursor:
                insert_data = []
                for data in batch_data:
                    insert_data.append((
                        data['symbol'], data['date'],
                        data['open_price'], data['high_price'],
                        data['low_price'], data['close_price'],
                        data['volume'], data['adjusted_close'],
                        data['data_source'], data['is_valid'],
                        data['data_quality_score']
                    ))
                
                # å¤§å®¹é‡ä¸€æ‹¬æŒ¿å…¥
                cursor.executemany("""
                    INSERT IGNORE INTO stock_price_history 
                    (symbol, date, open_price, high_price, low_price, close_price,
                     volume, adjusted_close, data_source, is_valid, data_quality_score, created_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                """, insert_data)
                
                connection.commit()
                return cursor.rowcount
                
        except Exception as e:
            logger.error(f"ãƒãƒƒãƒä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
        finally:
            if 'connection' in locals():
                connection.close()

    def process_symbol_batch(self, symbol_batch):
        """ã‚·ãƒ³ãƒœãƒ«ãƒãƒƒãƒã®ä¸¦åˆ—å‡¦ç†"""
        batch_results = []
        
        with ThreadPoolExecutor(max_workers=8) as executor:
            # ä¸¦åˆ—ã§yfinanceãƒ‡ãƒ¼ã‚¿å–å¾—
            futures = {
                executor.submit(self.fetch_yfinance_optimized, symbol_info): symbol_info[0]
                for symbol_info in symbol_batch
            }
            
            for future in as_completed(futures):
                symbol = futures[future]
                try:
                    result = future.result()
                    batch_results.append(result)
                    
                    if result['status'] == 'success':
                        logger.info(f"âœ… {symbol}: {result['count']}ä»¶å–å¾— ({result.get('yf_symbol', symbol)})")
                    else:
                        logger.warning(f"âš ï¸ {symbol}: {result['status']}")
                        
                except Exception as e:
                    logger.error(f"âŒ {symbol}: å‡¦ç†ã‚¨ãƒ©ãƒ¼ - {e}")
                    batch_results.append({'symbol': symbol, 'status': 'error', 'data': None})
        
        return batch_results

    def rapid_collect_real_data(self, target_symbols=1000, batch_size=50):
        """é«˜é€Ÿå®Ÿãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info(f"ğŸš€ é«˜é€Ÿå®Ÿãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ - ç›®æ¨™: {target_symbols}éŠ˜æŸ„")
        
        # é«˜å„ªå…ˆåº¦éŠ˜æŸ„å–å¾—
        missing_symbols = self.get_high_priority_missing_symbols(target_symbols)
        logger.info(f"ğŸ“‹ é«˜å„ªå…ˆåº¦å¯¾è±¡: {len(missing_symbols)}éŠ˜æŸ„")
        
        if not missing_symbols:
            logger.info("âš ï¸ å‡¦ç†å¯¾è±¡éŠ˜æŸ„ãªã—")
            return {'total': 0, 'successful': 0, 'failed': 0, 'total_records': 0}
        
        # ãƒãƒƒãƒå‡¦ç†
        total_successful = 0
        total_failed = 0
        total_records = 0
        
        for batch_start in range(0, len(missing_symbols), batch_size):
            batch_end = min(batch_start + batch_size, len(missing_symbols))
            symbol_batch = missing_symbols[batch_start:batch_end]
            
            logger.info(f"ğŸ“¦ ãƒãƒƒãƒå‡¦ç† {batch_start//batch_size + 1}: {len(symbol_batch)}éŠ˜æŸ„")
            
            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            batch_results = self.process_symbol_batch(symbol_batch)
            
            # æˆåŠŸãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬ä¿å­˜
            successful_data = []
            batch_successful = 0
            batch_failed = 0
            
            for result in batch_results:
                if result['status'] == 'success' and result['data']:
                    successful_data.extend(result['data'])
                    batch_successful += 1
                else:
                    batch_failed += 1
            
            # ãƒãƒƒãƒä¿å­˜
            if successful_data:
                saved_count = self.save_batch_data_optimized(successful_data)
                total_records += saved_count
                logger.info(f"ğŸ’¾ ãƒãƒƒãƒä¿å­˜: {saved_count:,}ä»¶ ({len(successful_data):,}ä»¶ä¸­)")
            
            total_successful += batch_successful
            total_failed += batch_failed
            
            # é€²æ—å ±å‘Š
            progress = ((batch_end) / len(missing_symbols)) * 100
            success_rate = (total_successful / (total_successful + total_failed)) * 100 if (total_successful + total_failed) > 0 else 0
            
            logger.info(f"ğŸ“ˆ é€²æ—: {progress:.1f}% | æˆåŠŸ: {total_successful}, å¤±æ•—: {total_failed} | æˆåŠŸç‡: {success_rate:.1f}%")
            logger.info(f"ğŸ“Š ç´¯ç©ãƒ‡ãƒ¼ã‚¿: {total_records:,}ä»¶")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼ˆãƒãƒƒãƒé–“ï¼‰
            time.sleep(1.0)
        
        # æœ€çµ‚çµæœ
        total_processed = total_successful + total_failed
        final_success_rate = (total_successful / total_processed * 100) if total_processed > 0 else 0
        
        logger.info(f"ğŸ¯ é«˜é€Ÿå®Ÿãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†:")
        logger.info(f"   - å‡¦ç†éŠ˜æŸ„: {total_processed}")
        logger.info(f"   - æˆåŠŸéŠ˜æŸ„: {total_successful} ({final_success_rate:.1f}%)")
        logger.info(f"   - å¤±æ•—éŠ˜æŸ„: {total_failed}")
        logger.info(f"   - åé›†ãƒ‡ãƒ¼ã‚¿: {total_records:,}ä»¶")
        
        return {
            'total': total_processed,
            'successful': total_successful,
            'failed': total_failed,
            'total_records': total_records,
            'success_rate': final_success_rate
        }

def main():
    logger.info("ğŸ”¥ é«˜é€Ÿãƒãƒƒãƒå®Ÿãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    collector = RapidBatchRealDataCollector()
    
    # é«˜é€Ÿãƒãƒƒãƒåé›†å®Ÿè¡Œï¼ˆ500éŠ˜æŸ„ã€ãƒãƒƒãƒã‚µã‚¤ã‚º50ï¼‰
    result = collector.rapid_collect_real_data(target_symbols=500, batch_size=50)
    
    if result['successful'] > 0:
        logger.info("âœ… é«˜é€Ÿãƒãƒƒãƒåé›†æˆåŠŸ - è©•ä¾¡å®Ÿè¡Œä¸­...")
        
        # åé›†å¾Œã®è©•ä¾¡
        import subprocess
        subprocess.run(["python3", "collation_safe_data_assessment.py"])
    else:
        logger.error("âŒ é«˜é€Ÿãƒãƒƒãƒåé›†å¤±æ•—")

if __name__ == "__main__":
    main()