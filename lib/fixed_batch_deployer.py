#!/usr/bin/env python3
"""
Fixed Batch Job Deployer - pip installation issue resolver
‰øÆÊ≠£Áâà„Éê„ÉÉ„ÉÅ„Ç∏„Éß„Éñ„Éá„Éó„É≠„Ç§„É§„Éº - pip „Ç§„É≥„Çπ„Éà„Éº„É´ÂïèÈ°åËß£Ê±∫
"""

import os
import sys
import json
import time
import logging
import subprocess
from datetime import datetime, timedelta
from typing import Dict, Any
import tempfile

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FixedBatchDeployer:
    """‰øÆÊ≠£Áâà„Éê„ÉÉ„ÉÅ„Ç∏„Éß„Éñ„Éá„Éó„É≠„Ç§„É§„Éº"""

    def __init__(self, project_id="pricewise-huqkr", location="us-central1"):
        self.project_id = project_id
        self.location = location

    def create_fixed_data_job(self) -> Dict[str, Any]:
        """‰øÆÊ≠£Áâà„Éá„Éº„ÇøÂèéÈõÜ„Éª‰∫àÊ∏¨ÁîüÊàê„Ç∏„Éß„Éñ„Çí‰ΩúÊàê"""

        script = """#!/bin/bash
set -e

echo "üîß ‰øÆÊ≠£Áâà„Éê„ÉÉ„ÉÅ„Ç∏„Éß„ÉñÂÆüË°åÈñãÂßã"
echo "ÈñãÂßãÊôÇÂàª: $(date)"
echo "=================================="

# 1. „Ç∑„Çπ„ÉÜ„É†Ê∫ñÂÇô„Å®pip„Ç§„É≥„Çπ„Éà„Éº„É´
echo "üîß „Ç∑„Çπ„ÉÜ„É†Ê∫ñÂÇô‰∏≠..."
apt-get update -qq
apt-get install -y python3-pip curl wget -qq

# 2. PythonÁí∞Â¢ÉÁ¢∫Ë™ç
echo "üêç PythonÁí∞Â¢ÉÁ¢∫Ë™ç..."
python3 --version
pip3 --version

# 3. ÂøÖË¶Å„Éë„ÉÉ„Ç±„Éº„Ç∏„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´
echo "üì¶ „Éë„ÉÉ„Ç±„Éº„Ç∏„Ç§„É≥„Çπ„Éà„Éº„É´‰∏≠..."
pip3 install --upgrade pip
pip3 install psycopg2-binary==2.9.9 yfinance==0.2.18 pandas==2.1.4 numpy==1.24.4 requests==2.31.0

# 4. „Éá„Éº„Çø„Éô„Éº„ÇπÊé•Á∂öÁ¢∫Ë™ç
echo "üîå „Éá„Éº„Çø„Éô„Éº„ÇπÊé•Á∂öÁ¢∫Ë™ç..."
python3 -c "
import psycopg2
try:
    conn = psycopg2.connect(
        host='34.173.9.214',
        user='postgres',
        password='os.getenv('DB_PASSWORD', '')',
        database='miraikakaku',
        connect_timeout=10
    )
    print('‚úÖ „Éá„Éº„Çø„Éô„Éº„ÇπÊé•Á∂öÊàêÂäü')
    conn.close()
except Exception as e:
    print(f'‚ùå „Éá„Éº„Çø„Éô„Éº„ÇπÊé•Á∂öÂ§±Êïó: {e}')
    exit(1)
"

# 5. „Ç∑„É≥„Éó„É´‰∫àÊ∏¨„Éá„Éº„ÇøÁîüÊàê
echo "üîÆ „Ç∑„É≥„Éó„É´‰∫àÊ∏¨„Éá„Éº„ÇøÁîüÊàê‰∏≠..."
python3 -c "
import psycopg2
import numpy as np
from datetime import datetime, timedelta
import random

def generate_simple_predictions():
    print('üöÄ „Ç∑„É≥„Éó„É´‰∫àÊ∏¨„Éá„Éº„ÇøÁîüÊàêÈñãÂßã')

    conn = psycopg2.connect(
        host='34.173.9.214',
        user='postgres',
        password='os.getenv('DB_PASSWORD', '')',
        database='miraikakaku'
    )
    cursor = conn.cursor()

    # ÊúÄÊñ∞„ÅÆ‰æ°Ê†º„Éá„Éº„Çø„Åå„ÅÇ„ÇãÈäòÊüÑ„ÇíÂèñÂæó
    cursor.execute('''
        SELECT sp.symbol, sp.close_price, sm.company_name
        FROM stock_prices sp
        JOIN stock_master sm ON sp.symbol = sm.symbol
        WHERE sp.date >= CURRENT_DATE - INTERVAL '7 days'
        AND sp.close_price IS NOT NULL
        AND sm.is_active = true
        ORDER BY RANDOM()
        LIMIT 100
    ''')

    symbols_data = cursor.fetchall()
    predictions_created = 0

    print(f'üéØ ÂØæË±°ÈäòÊüÑÊï∞: {len(symbols_data)}')

    for symbol, current_price, company_name in symbols_data:
        try:
            # ÈÅéÂéª10Êó•„ÅÆ‰æ°Ê†º„Éá„Éº„Çø„ÇíÂèñÂæó
            cursor.execute('''
                SELECT close_price FROM stock_prices
                WHERE symbol = %s
                AND date >= CURRENT_DATE - INTERVAL '10 days'
                AND close_price IS NOT NULL
                ORDER BY date DESC
                LIMIT 10
            ''', (symbol,))

            price_history = [row[0] for row in cursor.fetchall()]

            if len(price_history) >= 3:
                # „Ç∑„É≥„Éó„É´„Å™Áµ±Ë®à„Éô„Éº„Çπ‰∫àÊ∏¨
                avg_price = np.mean(price_history)
                price_std = np.std(price_history)
                trend = (price_history[0] - price_history[-1]) / len(price_history)

                # Ë§áÊï∞ÊúüÈñì„ÅÆ‰∫àÊ∏¨„ÇíÁîüÊàê
                for days_ahead in [1, 3, 7, 14, 30]:
                    prediction_date = datetime.now() + timedelta(days=days_ahead)

                    # „Éà„É¨„É≥„Éâ + „É©„É≥„ÉÄ„É†Â§âÂãï
                    trend_component = trend * days_ahead
                    random_component = random.gauss(0, max(price_std * 0.1, current_price * 0.02))
                    predicted_price = float(current_price + trend_component + random_component)

                    # Â¶•ÂΩìÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
                    predicted_price = max(predicted_price, current_price * 0.8)
                    predicted_price = min(predicted_price, current_price * 1.2)

                    # ‰ø°È†ºÂ∫¶ÔºàÊúüÈñì„ÅåÈï∑„ÅÑ„Åª„Å©‰Ωé‰∏ãÔºâ
                    confidence = max(0.4, 0.8 - (days_ahead * 0.015))

                    cursor.execute('''
                        INSERT INTO stock_predictions
                        (symbol, prediction_date, prediction_days, current_price,
                         predicted_price, confidence_score, model_type, created_at)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                        ON CONFLICT (symbol, prediction_date, prediction_days) DO UPDATE SET
                            predicted_price = EXCLUDED.predicted_price,
                            confidence_score = EXCLUDED.confidence_score,
                            model_type = EXCLUDED.model_type,
                            created_at = EXCLUDED.created_at
                    ''', (
                        symbol,
                        prediction_date.date(),
                        days_ahead,
                        float(current_price),
                        predicted_price,
                        confidence,
                        'FIXED_SIMPLE_V1',
                        datetime.now()
                    ))

                    predictions_created += 1

        except Exception as e:
            print(f'‚ö†Ô∏è {symbol} „Ç®„É©„Éº: {e}')
            continue

        # ÈÄ≤ÊçóË°®Á§∫
        if predictions_created % 100 == 0:
            conn.commit()
            print(f'‚úÖ ÈÄ≤Êçó: {predictions_created}‰ª∂„ÅÆ‰∫àÊ∏¨„ÇíÁîüÊàê')

    conn.commit()
    print(f'‚úÖ ÂêàË®à {predictions_created}‰ª∂„ÅÆ‰∫àÊ∏¨„Éá„Éº„Çø„ÇíÁîüÊàêÂÆå‰∫Ü')

    # ÁµêÊûúÁ¢∫Ë™ç
    cursor.execute('''
        SELECT COUNT(*) FROM stock_predictions
        WHERE created_at >= NOW() - INTERVAL '10 minutes'
    ''')
    recent_count = cursor.fetchone()[0]
    print(f'üìä ‰ªäÂõûÁîüÊàê„Åï„Çå„Åü‰∫àÊ∏¨Êï∞: {recent_count}')

    conn.close()

generate_simple_predictions()
"

echo "üéâ ‰øÆÊ≠£Áâà„Éê„ÉÉ„ÉÅ„Ç∏„Éß„ÉñÂÆüË°åÂÆå‰∫Ü"
echo "ÁµÇ‰∫ÜÊôÇÂàª: $(date)"
"""

        return {
            "taskGroups": [{
                "taskSpec": {
                    "runnables": [{
                        "script": {
                            "text": script.strip()
                        }
                    }],
                    "computeResource": {
                        "cpuMilli": 2000,
                        "memoryMib": 4096
                    },
                    "maxRetryCount": 2,
                    "maxRunDuration": "1800s"
                },
                "taskCount": 1,
                "parallelism": 1
            }],
            "allocationPolicy": {
                "instances": [{
                    "policy": {
                        "machineType": "e2-standard-2",
                        "provisioningModel": "STANDARD"
                    }
                }]
            },
            "logsPolicy": {
                "destination": "CLOUD_LOGGING"
            }
        }

    def submit_fixed_job(self, job_name: str) -> bool:
        """‰øÆÊ≠£Áâà„Ç∏„Éß„Éñ„ÇíÊäïÂÖ•"""
        try:
            logger.info(f"Submitting fixed job: {job_name}")

            job_config = self.create_fixed_data_job()

            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(job_config, f, indent=2)
                config_file = f.name

            try:
                cmd = [
                    "gcloud", "batch", "jobs", "submit",
                    job_name,
                    f"--location={self.location}",
                    f"--config={config_file}"
                ]

                result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                logger.info(f"Fixed job submitted successfully: {job_name}")
                return True

            finally:
                os.unlink(config_file)

        except Exception as e:
            logger.error(f"Failed to submit fixed job {job_name}: {e}")
            return False

    def deploy_fixed_job(self) -> None:
        """‰øÆÊ≠£Áâà„Ç∏„Éß„Éñ„Çí„Éá„Éó„É≠„Ç§"""
        timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
        job_name = f"fixed-prediction-job-{timestamp}"

        if self.submit_fixed_job(job_name):
            logger.info(f"‚úÖ Fixed prediction job deployed: {job_name}")
        else:
            logger.error(f"‚ùå Failed to deploy fixed job: {job_name}")

def main():
    """„É°„Ç§„É≥Èñ¢Êï∞"""
    deployer = FixedBatchDeployer()
    deployer.deploy_fixed_job()

if __name__ == "__main__":
    main()