#!/usr/bin/env python3
"""
MiraiKakaku ã‚·ã‚¹ãƒ†ãƒ ç°¡ç´ åŒ–è¨ˆç”»
5,191å€‹ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æã—ã€é‡è¤‡ãƒ»æœªä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
"""

import os
import sys
import shutil
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SystemCleanup:
    def __init__(self, root_path):
        self.root_path = Path(root_path)
        self.core_systems = [
            'simple_api_server.py',
            'cloud_run_data_collector.py',
            'cloud_run_prediction_generator.py',
            'enhanced_symbol_manager.py'
        ]

        # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®šç¾©
        self.duplicate_patterns = [
            '*collector*.py',
            '*prediction*.py',
            '*batch*.py',
            '*massive*.py',
            '*enhanced*.py',
            '*advanced*.py',
            '*comprehensive*.py',
            '*optimized*.py',
            '*improved*.py',
            '*fixed*.py',
            '*corrected*.py',
            '*updated*.py'
        ]

        # ä¿æŒã™ã¹ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.keep_directories = {
            'miraikakakuapi',
            'miraikakakufront',
            'miraikakakubatch/functions',  # ã‚³ã‚¢æ©Ÿèƒ½ã®ã¿
            'shared',
            'lib',
            'cloud_functions'  # æœ¬æ ¼é‹ç”¨ç‰ˆã®ã¿
        }

        # å‰Šé™¤å€™è£œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.cleanup_directories = {
            'archive',  # å®Œå…¨å‰Šé™¤
            'monitoring',  # çµ±åˆæ¸ˆã¿
            'scripts',  # çµ±åˆæ¸ˆã¿
            'docs',  # çµ±åˆæ¸ˆã¿
            'deployment',  # çµ±åˆæ¸ˆã¿
            'migration'  # å®Œäº†æ¸ˆã¿
        }

    def analyze_system_complexity(self):
        """ã‚·ã‚¹ãƒ†ãƒ è¤‡é›‘æ€§åˆ†æ"""
        stats = {
            'total_files': 0,
            'duplicate_files': 0,
            'unused_files': 0,
            'core_files': 0,
            'archive_files': 0
        }

        duplicate_groups = {}

        for pattern in self.duplicate_patterns:
            files = list(self.root_path.rglob(pattern))
            if len(files) > 1:
                duplicate_groups[pattern] = files
                stats['duplicate_files'] += len(files)

        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        archive_path = self.root_path / 'archive'
        if archive_path.exists():
            stats['archive_files'] = len(list(archive_path.rglob('*.py')))

        # ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        stats['total_files'] = len(list(self.root_path.rglob('*.py')))

        logger.info(f"ğŸ” ã‚·ã‚¹ãƒ†ãƒ è¤‡é›‘æ€§åˆ†æçµæœ:")
        logger.info(f"  ç·Pythonãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']}")
        logger.info(f"  é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«: {stats['duplicate_files']}")
        logger.info(f"  ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«: {stats['archive_files']}")

        return stats, duplicate_groups

    def create_backup(self):
        """é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        backup_dir = self.root_path / f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        backup_dir.mkdir(exist_ok=True)

        # ã‚³ã‚¢ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        for core_file in self.core_systems:
            for file_path in self.root_path.rglob(core_file):
                relative_path = file_path.relative_to(self.root_path)
                backup_path = backup_dir / relative_path
                backup_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(file_path, backup_path)
                logger.info(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {relative_path}")

        return backup_dir

    def cleanup_archive_directory(self):
        """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å®Œå…¨å‰Šé™¤"""
        archive_path = self.root_path / 'archive'
        if archive_path.exists():
            logger.info(f"ğŸ—‘ï¸  ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤: {archive_path}")
            shutil.rmtree(archive_path)
            return True
        return False

    def cleanup_duplicate_files(self, duplicate_groups, dry_run=True):
        """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        deleted_count = 0

        for pattern, files in duplicate_groups.items():
            if len(files) <= 1:
                continue

            # ã‚³ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®šã—ã€ãã‚Œä»¥å¤–ã‚’å‰Šé™¤å€™è£œã¨ã™ã‚‹
            core_file = None
            for file_path in files:
                if any(core in file_path.name for core in self.core_systems):
                    core_file = file_path
                    break

            # ã‚³ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿æŒ
            if not core_file:
                core_file = max(files, key=lambda f: f.stat().st_mtime)

            for file_path in files:
                if file_path != core_file:
                    logger.info(f"ğŸ—‘ï¸  å‰Šé™¤å€™è£œ: {file_path.relative_to(self.root_path)}")
                    if not dry_run:
                        file_path.unlink()
                        deleted_count += 1

        logger.info(f"ğŸ§¹ é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {deleted_count}å€‹ (dry_run={dry_run})")
        return deleted_count

    def optimize_batch_directory(self):
        """ãƒãƒƒãƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æœ€é©åŒ–"""
        batch_path = self.root_path / 'miraikakakubatch' / 'functions'
        if not batch_path.exists():
            return 0

        # ä½¿ç”¨ä¸­ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
        active_files = {
            'enhanced_symbol_manager.py',
            'database/cloud_sql.py',
            'database/models/__init__.py'
        }

        deleted_count = 0
        for py_file in batch_path.rglob('*.py'):
            relative_path = py_file.relative_to(batch_path)
            if str(relative_path) not in active_files:
                logger.info(f"ğŸ—‘ï¸  ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å€™è£œ: {relative_path}")
                # dry_run ã¨ã—ã¦ã€å®Ÿéš›ã®å‰Šé™¤ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
                # py_file.unlink()
                deleted_count += 1

        return deleted_count

    def generate_cleanup_report(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        stats, duplicate_groups = self.analyze_system_complexity()

        report = f"""
# MiraiKakaku ã‚·ã‚¹ãƒ†ãƒ ç°¡ç´ åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
ç”Ÿæˆæ—¥æ™‚: {datetime.now().isoformat()}

## ç¾åœ¨ã®çŠ¶æ³
- ç·Pythonãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['total_files']:,}
- é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«: {stats['duplicate_files']:,}
- ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«: {stats['archive_files']:,}

## å‰Šé™¤å¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«
"""

        for pattern, files in duplicate_groups.items():
            if len(files) > 1:
                report += f"\n### {pattern}\n"
                for file_path in files:
                    report += f"- {file_path.relative_to(self.root_path)}\n"

        report += f"""
## æ¨å®šå‰Šæ¸›åŠ¹æœ
- ãƒ•ã‚¡ã‚¤ãƒ«å‰Šæ¸›æ•°: {stats['duplicate_files'] + stats['archive_files']:,}
- ç°¡ç´ åŒ–ç‡: {((stats['duplicate_files'] + stats['archive_files']) / stats['total_files'] * 100):.1f}%

## å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
```bash
python3 system_cleanup_plan.py --execute
```
"""

        report_path = self.root_path / 'SYSTEM_CLEANUP_REPORT.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        logger.info(f"ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_path}")
        return report_path

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    root_path = "/mnt/c/Users/yuuku/cursor/miraikakaku"
    cleanup = SystemCleanup(root_path)

    # åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    logger.info("ğŸš€ ã‚·ã‚¹ãƒ†ãƒ ç°¡ç´ åŒ–åˆ†æé–‹å§‹")

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    backup_dir = cleanup.create_backup()
    logger.info(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå®Œäº†: {backup_dir}")

    # åˆ†æå®Ÿè¡Œ
    stats, duplicate_groups = cleanup.analyze_system_complexity()

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report_path = cleanup.generate_cleanup_report()

    # å®‰å…¨ãªå‰Šé™¤ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®ã¿ï¼‰
    if cleanup.cleanup_archive_directory():
        logger.info("âœ… ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤å®Œäº†")

    logger.info("âœ… ã‚·ã‚¹ãƒ†ãƒ åˆ†æå®Œäº†")
    logger.info(f"ğŸ“‹ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")

if __name__ == "__main__":
    main()