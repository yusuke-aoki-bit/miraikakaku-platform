#!/usr/bin/env python3
"""
MiraiKakaku ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ç½å®³å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ 
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã€å¾©æ—§æ‰‹é †ã®å®Ÿè£…
"""

import os
import sys
import subprocess
import logging
import json
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
import tarfile
import psycopg2
from typing import Dict, Optional, List

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - BackupRecovery - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BackupDisasterRecoverySystem:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»ç½å®³å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.backup_base_path = Path(os.getenv('BACKUP_PATH', '/mnt/c/Users/yuuku/cursor/miraikakaku/backups'))
        self.backup_base_path.mkdir(parents=True, exist_ok=True)

        self.db_config = {
            'host': os.getenv('DB_HOST', '34.173.9.214'),
            'database': os.getenv('DB_NAME', 'miraikakaku'),
            'user': os.getenv('DB_USER', 'postgres'),
            'password': os.getenv('DB_PASSWORD'),
            'port': int(os.getenv('DB_PORT', '5432'))
        }

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š
        self.backup_config = {
            'retention_days': 30,  # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿æŒæœŸé–“
            'critical_tables': [
                'stock_master',
                'stock_prices',
                'stock_predictions',
                'prediction_accuracy'
            ],
            'critical_files': [
                'miraikakakuapi/simple_api_server.py',
                'cloud_run_data_collector.py',
                'cloud_run_prediction_generator.py',
                'miraikakakufront/app/page.tsx',
                'miraikakakufront/app/lib/api.ts'
            ]
        }

        # ãƒªã‚«ãƒãƒªãƒ†ã‚¹ãƒˆè¨­å®š
        self.recovery_test_config = {
            'test_interval_hours': 168,  # é€±1å›
            'test_data_sample_size': 100,
            'rpo_minutes': 60,  # Recovery Point Objective
            'rto_minutes': 30   # Recovery Time Objective
        }

    def create_database_backup(self, backup_type: str = 'full') -> Optional[Path]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"db_backup_{backup_type}_{timestamp}"
        backup_path = self.backup_base_path / 'database' / backup_name

        backup_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            if backup_type == 'full':
                # å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                logger.info("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹")
                pg_dump_cmd = [
                    'pg_dump',
                    f'--host={self.db_config["host"]}',
                    f'--port={self.db_config["port"]}',
                    f'--username={self.db_config["user"]}',
                    f'--dbname={self.db_config["database"]}',
                    '--format=custom',
                    '--verbose',
                    f'--file={backup_path}.dump'
                ]

                # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’ç’°å¢ƒå¤‰æ•°ã§æ¸¡ã™
                env = os.environ.copy()
                env['PGPASSWORD'] = self.db_config['password']

                result = subprocess.run(
                    pg_dump_cmd,
                    env=env,
                    capture_output=True,
                    text=True,
                    timeout=300
                )

                if result.returncode != 0:
                    logger.error(f"pg_dump failed: {result.stderr}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šSQLãƒ€ãƒ³ãƒ—å½¢å¼ã§è©¦è¡Œ
                    return self.create_sql_backup(backup_path)

                # åœ§ç¸®
                self.compress_backup(f"{backup_path}.dump")

                logger.info(f"âœ… å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå®Œäº†: {backup_path}.dump.gz")
                return Path(f"{backup_path}.dump.gz")

            elif backup_type == 'incremental':
                # å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå¤‰æ›´ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ï¼‰
                return self.create_incremental_backup(backup_path)

            elif backup_type == 'critical':
                # é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿
                return self.create_critical_tables_backup(backup_path)

        except subprocess.TimeoutExpired:
            logger.error("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            return None
        except Exception as e:
            logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return self.create_sql_backup(backup_path)  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    def create_sql_backup(self, backup_path: Path) -> Optional[Path]:
        """SQLãƒ€ãƒ³ãƒ—å½¢å¼ã§ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        logger.info("ğŸ“ SQLãƒ€ãƒ³ãƒ—å½¢å¼ã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ")

        conn = psycopg2.connect(**self.db_config)
        backup_file = f"{backup_path}.sql"

        try:
            with conn.cursor() as cursor:
                with open(backup_file, 'w') as f:
                    # é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                    for table in self.backup_config['critical_tables']:
                        cursor.execute(f"SELECT COUNT(*) FROM {table}")
                        count = cursor.fetchone()[0]
                        logger.info(f"  {table}: {count}ä»¶")

                        # CREATE TABLEæ–‡ã‚’å–å¾—
                        cursor.execute(f"""
                            SELECT 'CREATE TABLE IF NOT EXISTS {table} AS ' ||
                                   'SELECT * FROM {table} WHERE false;'
                        """)
                        f.write(cursor.fetchone()[0] + '\n\n')

                        # ãƒ‡ãƒ¼ã‚¿ã‚’COPYå½¢å¼ã§å‡ºåŠ›
                        copy_sql = f"COPY {table} TO STDOUT WITH CSV HEADER"
                        with conn.cursor() as copy_cursor:
                            f.write(f"\\copy {table} FROM stdin WITH CSV HEADER\n")
                            copy_cursor.copy_expert(copy_sql, f)
                            f.write("\\.\n\n")

            # åœ§ç¸®
            self.compress_backup(backup_file)
            logger.info(f"âœ… SQLãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_file}.gz")
            return Path(f"{backup_file}.gz")

        except Exception as e:
            logger.error(f"SQLãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        finally:
            conn.close()

    def create_incremental_backup(self, backup_path: Path) -> Optional[Path]:
        """å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ"""
        logger.info("ğŸ”„ å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹")

        conn = psycopg2.connect(**self.db_config)
        backup_file = f"{backup_path}_incremental.sql"

        try:
            with conn.cursor() as cursor:
                with open(backup_file, 'w') as f:
                    # éå»24æ™‚é–“ã®å¤‰æ›´ãƒ‡ãƒ¼ã‚¿ã®ã¿
                    for table in self.backup_config['critical_tables']:
                        if table == 'stock_prices':
                            cursor.execute(f"""
                                COPY (
                                    SELECT * FROM {table}
                                    WHERE created_at >= NOW() - INTERVAL '24 hours'
                                       OR updated_at >= NOW() - INTERVAL '24 hours'
                                ) TO STDOUT WITH CSV HEADER
                            """)
                            f.write(f"\\copy {table}_incremental FROM stdin WITH CSV HEADER\n")
                            for line in cursor:
                                f.write(line)
                            f.write("\\.\n\n")

            self.compress_backup(backup_file)
            logger.info(f"âœ… å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_file}.gz")
            return Path(f"{backup_file}.gz")

        except Exception as e:
            logger.error(f"å·®åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        finally:
            conn.close()

    def create_critical_tables_backup(self, backup_path: Path) -> Optional[Path]:
        """é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        logger.info("âš ï¸ é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹")

        conn = psycopg2.connect(**self.db_config)
        backup_data = {}

        try:
            with conn.cursor() as cursor:
                for table in self.backup_config['critical_tables']:
                    cursor.execute(f"SELECT * FROM {table} LIMIT 10000")  # æœ€æ–°10000ä»¶
                    columns = [desc[0] for desc in cursor.description]
                    rows = cursor.fetchall()

                    backup_data[table] = {
                        'columns': columns,
                        'data': rows,
                        'count': len(rows)
                    }

                    logger.info(f"  {table}: {len(rows)}ä»¶")

            # JSONå½¢å¼ã§ä¿å­˜
            backup_file = f"{backup_path}_critical.json"
            with open(backup_file, 'w') as f:
                json.dump(backup_data, f, default=str, indent=2)

            self.compress_backup(backup_file)
            logger.info(f"âœ… é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_file}.gz")
            return Path(f"{backup_file}.gz")

        except Exception as e:
            logger.error(f"é‡è¦ãƒ†ãƒ¼ãƒ–ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        finally:
            conn.close()

    def create_file_backup(self) -> Optional[Path]:
        """é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_name = f"files_backup_{timestamp}.tar.gz"
        backup_path = self.backup_base_path / 'files' / backup_name

        backup_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            logger.info("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹")

            with tarfile.open(backup_path, 'w:gz') as tar:
                for file_path in self.backup_config['critical_files']:
                    full_path = Path('/mnt/c/Users/yuuku/cursor/miraikakaku') / file_path
                    if full_path.exists():
                        tar.add(full_path, arcname=file_path)
                        logger.info(f"  âœ“ {file_path}")
                    else:
                        logger.warning(f"  âœ— {file_path} not found")

            logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_path}")
            return backup_path

        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def compress_backup(self, file_path: str):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®åœ§ç¸®"""
        subprocess.run(['gzip', '-f', file_path], check=False)

    def verify_backup(self, backup_path: Path) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ•´åˆæ€§æ¤œè¨¼"""
        logger.info(f"ğŸ” ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼: {backup_path}")

        if not backup_path.exists():
            logger.error("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return False

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        size_mb = backup_path.stat().st_size / (1024 * 1024)
        logger.info(f"  ã‚µã‚¤ã‚º: {size_mb:.2f} MB")

        if size_mb < 0.1:
            logger.error("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒå°ã•ã™ãã¾ã™")
            return False

        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
        checksum = self.calculate_checksum(backup_path)
        logger.info(f"  ãƒã‚§ãƒƒã‚¯ã‚µãƒ : {checksum}")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata_path = backup_path.with_suffix('.meta')
        with open(metadata_path, 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'size_bytes': backup_path.stat().st_size,
                'checksum': checksum,
                'verified': True
            }, f)

        logger.info("âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¤œè¨¼æˆåŠŸ")
        return True

    def calculate_checksum(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def test_recovery_procedure(self) -> Dict:
        """ç½å®³å¾©æ—§æ‰‹é †ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("ğŸ”§ ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆé–‹å§‹")
        test_results = {
            'timestamp': datetime.now().isoformat(),
            'tests': {},
            'overall_status': 'PASSED'
        }

        # 1. æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ç¢ºèª
        latest_backup = self.find_latest_backup()
        if not latest_backup:
            test_results['tests']['backup_availability'] = 'FAILED'
            test_results['overall_status'] = 'FAILED'
            logger.error("âŒ æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return test_results

        test_results['tests']['backup_availability'] = 'PASSED'
        logger.info(f"âœ… æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {latest_backup}")

        # 2. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¹´é½¢ã®ç¢ºèª
        backup_age = datetime.now() - datetime.fromtimestamp(latest_backup.stat().st_mtime)
        if backup_age > timedelta(hours=24):
            test_results['tests']['backup_freshness'] = 'WARNING'
            logger.warning(f"âš ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒ{backup_age.days}æ—¥å¤ã„")
        else:
            test_results['tests']['backup_freshness'] = 'PASSED'

        # 3. å¾©æ—§æ™‚é–“ç›®æ¨™ï¼ˆRTOï¼‰ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        recovery_start = datetime.now()

        # ãƒ†ã‚¹ãƒˆå¾©æ—§ï¼ˆå®Ÿéš›ã«ã¯å®Ÿè¡Œã—ãªã„ï¼‰
        estimated_recovery_time = self.estimate_recovery_time(latest_backup)

        if estimated_recovery_time <= self.recovery_test_config['rto_minutes']:
            test_results['tests']['rto_compliance'] = 'PASSED'
            logger.info(f"âœ… RTOé”æˆå¯èƒ½: {estimated_recovery_time}åˆ†")
        else:
            test_results['tests']['rto_compliance'] = 'FAILED'
            test_results['overall_status'] = 'FAILED'
            logger.error(f"âŒ RTOæœªé”: {estimated_recovery_time}åˆ† > {self.recovery_test_config['rto_minutes']}åˆ†")

        # 4. å¾©æ—§ãƒã‚¤ãƒ³ãƒˆç›®æ¨™ï¼ˆRPOï¼‰ã®ç¢ºèª
        data_loss_minutes = backup_age.total_seconds() / 60
        if data_loss_minutes <= self.recovery_test_config['rpo_minutes']:
            test_results['tests']['rpo_compliance'] = 'PASSED'
            logger.info(f"âœ… RPOé”æˆ: {data_loss_minutes:.0f}åˆ†")
        else:
            test_results['tests']['rpo_compliance'] = 'WARNING'
            logger.warning(f"âš ï¸ RPOè¶…é: {data_loss_minutes:.0f}åˆ† > {self.recovery_test_config['rpo_minutes']}åˆ†")

        # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        test_report_path = self.backup_base_path / f"recovery_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(test_report_path, 'w') as f:
            json.dump(test_results, f, indent=2)

        logger.info(f"""
ğŸ”§ ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆçµæœ
======================
å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {test_results['overall_status']}
ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯ç”¨æ€§: {test_results['tests']['backup_availability']}
ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é®®åº¦: {test_results['tests']['backup_freshness']}
RTOé”æˆå¯èƒ½æ€§: {test_results['tests']['rto_compliance']}
RPOé”æˆå¯èƒ½æ€§: {test_results['tests']['rpo_compliance']}

ãƒ¬ãƒãƒ¼ãƒˆ: {test_report_path}
        """)

        return test_results

    def find_latest_backup(self) -> Optional[Path]:
        """æœ€æ–°ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        backup_files = list(self.backup_base_path.glob('**/*.gz'))
        if not backup_files:
            return None
        return max(backup_files, key=lambda p: p.stat().st_mtime)

    def estimate_recovery_time(self, backup_path: Path) -> int:
        """å¾©æ—§æ™‚é–“ã®æ¨å®šï¼ˆåˆ†ï¼‰"""
        size_gb = backup_path.stat().st_size / (1024 ** 3)
        # çµŒé¨“å‰‡ï¼š1GBã‚ãŸã‚Š5åˆ†ã§å¾©æ—§
        estimated_minutes = int(size_gb * 5) + 10  # åŸºæœ¬æ™‚é–“10åˆ†
        return estimated_minutes

    def cleanup_old_backups(self):
        """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤"""
        logger.info("ğŸ§¹ å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")

        cutoff_date = datetime.now() - timedelta(days=self.backup_config['retention_days'])
        deleted_count = 0

        for backup_file in self.backup_base_path.glob('**/*'):
            if backup_file.is_file():
                file_time = datetime.fromtimestamp(backup_file.stat().st_mtime)
                if file_time < cutoff_date:
                    logger.info(f"  å‰Šé™¤: {backup_file.name}")
                    backup_file.unlink()
                    deleted_count += 1

        logger.info(f"âœ… {deleted_count}å€‹ã®å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤")

    def create_recovery_script(self) -> Path:
        """ç½å®³å¾©æ—§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ç”Ÿæˆ"""
        script_path = self.backup_base_path / 'recovery_script.sh'

        script_content = f"""#!/bin/bash
# MiraiKakaku ç½å®³å¾©æ—§ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ç”Ÿæˆæ—¥æ™‚: {datetime.now().isoformat()}

echo "ğŸš¨ MiraiKakaku ç½å®³å¾©æ—§é–‹å§‹"
echo "================================"

# 1. æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ç¢ºèª
LATEST_BACKUP=$(ls -t {self.backup_base_path}/database/*.gz | head -1)
echo "æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: $LATEST_BACKUP"

# 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§
echo "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§ä¸­..."
gunzip -c "$LATEST_BACKUP" | psql \\
    --host={self.db_config['host']} \\
    --port={self.db_config['port']} \\
    --username={self.db_config['user']} \\
    --dbname={self.db_config['database']}

# 3. ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•
echo "ğŸ”„ ã‚µãƒ¼ãƒ“ã‚¹å†èµ·å‹•ä¸­..."
cd /mnt/c/Users/yuuku/cursor/miraikakaku

# APIã‚µãƒ¼ãƒãƒ¼
pkill -f "simple_api_server.py"
nohup python3 miraikakakuapi/simple_api_server.py &

# ãƒ‡ãƒ¼ã‚¿åé›†ã‚µãƒ¼ãƒ“ã‚¹
pkill -f "cloud_run_data_collector.py"
nohup python3 cloud_run_data_collector.py &

# äºˆæ¸¬ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹
pkill -f "cloud_run_prediction_generator.py"
nohup python3 cloud_run_prediction_generator.py &

echo "âœ… ç½å®³å¾©æ—§å®Œäº†"
"""

        with open(script_path, 'w') as f:
            f.write(script_content)

        script_path.chmod(0o755)
        logger.info(f"âœ… å¾©æ—§ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ: {script_path}")
        return script_path


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    system = BackupDisasterRecoverySystem()

    if len(sys.argv) > 1:
        command = sys.argv[1]

        if command == "backup":
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
            logger.info("ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‡¦ç†é–‹å§‹")

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            db_backup = system.create_database_backup('critical')
            if db_backup:
                system.verify_backup(db_backup)

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            file_backup = system.create_file_backup()
            if file_backup:
                system.verify_backup(file_backup)

            # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            system.cleanup_old_backups()

        elif command == "test":
            # ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆ
            system.test_recovery_procedure()

        elif command == "script":
            # å¾©æ—§ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
            system.create_recovery_script()

    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
        logger.info("ğŸ“Š ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
        latest = system.find_latest_backup()
        if latest:
            age = datetime.now() - datetime.fromtimestamp(latest.stat().st_mtime)
            logger.info(f"æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {latest.name}")
            logger.info(f"çµŒéæ™‚é–“: {age}")
        else:
            logger.warning("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

        # ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        system.test_recovery_procedure()


if __name__ == "__main__":
    main()