"""
WebSocket Integration with FastAPI
Phase 3.1 - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ APIçµ±åˆ

FastAPI WebSocket endpoint integration for real-time AI inference
"""

import asyncio
import logging
from contextlib import asynccontextmanager
from typing import Dict, Any

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Depends, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn

from services.realtime_inference import realtime_server, RealtimeWebSocketServer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†"""
    logger.info("ğŸš€ Starting Realtime AI WebSocket server...")

    # ã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–ãƒ»é–‹å§‹
    await realtime_server.initialize()
    await realtime_server.start()

    logger.info("âœ… Realtime AI WebSocket server started successfully")

    yield

    # ã‚µãƒ¼ãƒãƒ¼åœæ­¢
    logger.info("â¹ Stopping Realtime AI WebSocket server...")
    await realtime_server.stop()
    logger.info("âœ… Realtime AI WebSocket server stopped")

# FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
app = FastAPI(
    title="MiraiKakaku Realtime AI API",
    description="High-performance real-time AI stock prediction WebSocket API",
    version="3.1.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªç’°å¢ƒã§ã¯é©åˆ‡ãªã‚ªãƒªã‚¸ãƒ³ã‚’è¨­å®š
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        server_stats = realtime_server.get_server_stats()

        return JSONResponse(
            status_code=200,
            content={
                "status": "healthy",
                "service": "realtime-ai-websocket",
                "version": "3.1.0",
                "stats": server_stats,
                "capabilities": [
                    "websocket_realtime_predictions",
                    "market_data_streaming",
                    "alert_system",
                    "system_health_monitoring"
                ]
            }
        )
    except Exception as e:
        logger.error(f"âŒ Health check failed: {e}")
        return JSONResponse(
            status_code=503,
            content={
                "status": "unhealthy",
                "error": str(e)
            }
        )

@app.get("/stats")
async def get_server_stats():
    """ã‚µãƒ¼ãƒãƒ¼çµ±è¨ˆå–å¾—"""
    try:
        stats = realtime_server.get_server_stats()
        return JSONResponse(content=stats)
    except Exception as e:
        logger.error(f"âŒ Stats retrieval failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get server stats: {str(e)}"
        )

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    WebSocket ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ AIäºˆæ¸¬ã€å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã€ã‚¢ãƒ©ãƒ¼ãƒˆã®é…ä¿¡

    æ¥ç¶šä¾‹:
    ```javascript
    const ws = new WebSocket('ws://localhost:8080/ws');

    // æ¥ç¶šç¢ºç«‹å¾Œ
    ws.send(JSON.stringify({
        type: 'subscribe',
        channel: 'predictions',
        symbol: 'AAPL'
    }));

    // äºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    ws.send(JSON.stringify({
        type: 'request_prediction',
        symbol: 'AAPL',
        options: {
            horizon: '1d',
            confidence: 0.95,
            model: 'ensemble'
        }
    }));
    ```
    """
    await realtime_server.handle_websocket(websocket)

@app.get("/")
async def root():
    """API ãƒ«ãƒ¼ãƒˆ"""
    return {
        "message": "MiraiKakaku Realtime AI WebSocket API",
        "version": "3.1.0",
        "endpoints": {
            "websocket": "/ws",
            "health": "/health",
            "stats": "/stats",
            "docs": "/docs"
        },
        "features": [
            "Real-time stock predictions (< 100ms)",
            "Live market data streaming",
            "Instant alert notifications",
            "System health monitoring",
            "Auto-reconnection support",
            "High-concurrency (10,000+ connections)"
        ]
    }

# WebSocket æ¥ç¶šç®¡ç†ç”¨ã®RESTã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

@app.get("/connections")
async def get_connection_info():
    """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ¥ç¶šæƒ…å ±ã‚’å–å¾—"""
    try:
        connection_stats = realtime_server.connection_manager.get_stats()
        return JSONResponse(content=connection_stats)
    except Exception as e:
        logger.error(f"âŒ Connection info retrieval failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get connection info: {str(e)}"
        )

@app.get("/inference-stats")
async def get_inference_stats():
    """æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    try:
        inference_stats = realtime_server.inference_engine.get_stats()
        return JSONResponse(content=inference_stats)
    except Exception as e:
        logger.error(f"âŒ Inference stats retrieval failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get inference stats: {str(e)}"
        )

@app.post("/broadcast")
async def broadcast_message(message: Dict[str, Any]):
    """
    ç®¡ç†è€…ç”¨ï¼šå…¨æ¥ç¶šã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é…ä¿¡

    ä¾‹:
    ```json
    {
        "channel": "system_health",
        "message": {
            "type": "maintenance_notice",
            "message": "System maintenance in 5 minutes",
            "severity": "medium"
        }
    }
    ```
    """
    try:
        channel = message.get("channel", "system_health")
        msg_data = message.get("message", {})

        sent_count = await realtime_server.connection_manager.broadcast_to_channel(
            channel, msg_data
        )

        return JSONResponse(content={
            "status": "success",
            "sent_to_connections": sent_count,
            "channel": channel
        })
    except Exception as e:
        logger.error(f"âŒ Broadcast failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to broadcast message: {str(e)}"
        )

# é–‹ç™ºç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

@app.post("/dev/simulate-prediction")
async def simulate_prediction_broadcast(symbol: str, count: int = 1):
    """
    é–‹ç™ºç”¨ï¼šæ¨¡æ“¬äºˆæ¸¬ã‚’é…ä¿¡
    """
    try:
        sent_total = 0

        for i in range(count):
            prediction = await realtime_server.inference_engine.generate_prediction(
                symbol=symbol,
                model="ensemble"
            )

            sent_count = await realtime_server.connection_manager.broadcast_to_channel(
                "predictions",
                {
                    "type": "prediction",
                    "data": prediction.__dict__
                },
                symbol=symbol
            )

            sent_total += sent_count

            if count > 1:
                await asyncio.sleep(0.1)  # 100msé–“éš”

        return JSONResponse(content={
            "status": "success",
            "predictions_sent": count,
            "total_deliveries": sent_total,
            "symbol": symbol
        })
    except Exception as e:
        logger.error(f"âŒ Prediction simulation failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to simulate prediction: {str(e)}"
        )

if __name__ == "__main__":
    # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    uvicorn.run(
        "websocket_main:app",
        host="0.0.0.0",
        port=8080,
        reload=True,
        log_level="info",
        ws_max_size=16777216,  # 16MB
        ws_ping_interval=20,   # 20ç§’é–“éš”ã§ping
        ws_ping_timeout=10,    # 10ç§’ã§pingã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    )