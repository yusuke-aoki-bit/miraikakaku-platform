#!/usr/bin/env python3
"""
Unified Prediction Orchestrator
LSTM & VertexAI äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
"""

import os
import sys
import time
import logging
import subprocess
import schedule
import threading
from datetime import datetime, timedelta
from typing import Dict, Any

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PredictionOrchestrator:
    """äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼"""

    def __init__(self):
        self.lstm_script = "separated_lstm_prediction_system.py"
        self.vertexai_script = "separated_vertexai_prediction_system.py"
        self.execution_log = []

    def log_execution(self, system_name: str, success: bool, duration: float, details: str = ""):
        """å®Ÿè¡Œãƒ­ã‚°è¨˜éŒ²"""
        log_entry = {
            'timestamp': datetime.now(),
            'system': system_name,
            'success': success,
            'duration_seconds': duration,
            'details': details
        }
        self.execution_log.append(log_entry)

        # æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
        if len(self.execution_log) > 100:
            self.execution_log = self.execution_log[-100:]

    def run_lstm_predictions(self):
        """LSTMäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ"""
        logger.info("ðŸš€ Starting LSTM Prediction System")
        start_time = time.time()

        try:
            result = subprocess.run([
                sys.executable, self.lstm_script
            ], capture_output=True, text=True, timeout=600)

            duration = time.time() - start_time
            success = result.returncode == 0

            if success:
                logger.info(f"âœ… LSTM System completed successfully in {duration:.2f}s")
                self.log_execution("LSTM", True, duration, "Completed successfully")
            else:
                logger.error(f"âŒ LSTM System failed: {result.stderr}")
                self.log_execution("LSTM", False, duration, f"Failed: {result.stderr[:200]}")

            return success

        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            logger.error(f"âŒ LSTM System timed out after {duration:.2f}s")
            self.log_execution("LSTM", False, duration, "Timeout after 600s")
            return False
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"âŒ LSTM System error: {e}")
            self.log_execution("LSTM", False, duration, f"Error: {str(e)}")
            return False

    def run_vertexai_predictions(self):
        """VertexAIäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ"""
        logger.info("ðŸ¤– Starting VertexAI Prediction System")
        start_time = time.time()

        try:
            result = subprocess.run([
                sys.executable, self.vertexai_script
            ], capture_output=True, text=True, timeout=600)

            duration = time.time() - start_time
            success = result.returncode == 0

            if success:
                logger.info(f"âœ… VertexAI System completed successfully in {duration:.2f}s")
                self.log_execution("VertexAI", True, duration, "Completed successfully")
            else:
                logger.error(f"âŒ VertexAI System failed: {result.stderr}")
                self.log_execution("VertexAI", False, duration, f"Failed: {result.stderr[:200]}")

            return success

        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            logger.error(f"âŒ VertexAI System timed out after {duration:.2f}s")
            self.log_execution("VertexAI", False, duration, "Timeout after 600s")
            return False
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"âŒ VertexAI System error: {e}")
            self.log_execution("VertexAI", False, duration, f"Error: {str(e)}")
            return False

    def run_both_systems_parallel(self):
        """ä¸¡ã‚·ã‚¹ãƒ†ãƒ ä¸¦åˆ—å®Ÿè¡Œ"""
        logger.info("ðŸš€ Starting Both Prediction Systems in Parallel")
        start_time = time.time()

        # ä¸¦åˆ—å®Ÿè¡Œç”¨ã‚¹ãƒ¬ãƒƒãƒ‰
        lstm_thread = threading.Thread(target=self.run_lstm_predictions)
        vertexai_thread = threading.Thread(target=self.run_vertexai_predictions)

        # çµæžœæ ¼ç´ç”¨
        results = {'lstm': False, 'vertexai': False}

        def lstm_wrapper():
            results['lstm'] = self.run_lstm_predictions()

        def vertexai_wrapper():
            results['vertexai'] = self.run_vertexai_predictions()

        # ã‚¹ãƒ¬ãƒƒãƒ‰å†å®šç¾©
        lstm_thread = threading.Thread(target=lstm_wrapper)
        vertexai_thread = threading.Thread(target=vertexai_wrapper)

        # ä¸¦åˆ—å®Ÿè¡Œ
        lstm_thread.start()
        vertexai_thread.start()

        # å®Œäº†å¾…æ©Ÿ
        lstm_thread.join()
        vertexai_thread.join()

        duration = time.time() - start_time
        success_count = sum(results.values())

        logger.info("=" * 60)
        logger.info("ðŸŽ‰ Parallel Prediction Execution Complete")
        logger.info(f"âœ… LSTM Success: {results['lstm']}")
        logger.info(f"âœ… VertexAI Success: {results['vertexai']}")
        logger.info(f"â±ï¸ Total Duration: {duration:.2f}s")
        logger.info(f"ðŸ“Š Success Rate: {success_count}/2 ({success_count/2*100:.1f}%)")
        logger.info("=" * 60)

        return results

    def run_both_systems_sequential(self):
        """ä¸¡ã‚·ã‚¹ãƒ†ãƒ é †æ¬¡å®Ÿè¡Œ"""
        logger.info("ðŸš€ Starting Both Prediction Systems Sequentially")
        start_time = time.time()

        # LSTMå®Ÿè¡Œ
        lstm_success = self.run_lstm_predictions()

        # çŸ­ã„é–“éš”ã‚’ã‚ã‘ã‚‹
        time.sleep(5)

        # VertexAIå®Ÿè¡Œ
        vertexai_success = self.run_vertexai_predictions()

        duration = time.time() - start_time
        success_count = sum([lstm_success, vertexai_success])

        logger.info("=" * 60)
        logger.info("ðŸŽ‰ Sequential Prediction Execution Complete")
        logger.info(f"âœ… LSTM Success: {lstm_success}")
        logger.info(f"âœ… VertexAI Success: {vertexai_success}")
        logger.info(f"â±ï¸ Total Duration: {duration:.2f}s")
        logger.info(f"ðŸ“Š Success Rate: {success_count}/2 ({success_count/2*100:.1f}%)")
        logger.info("=" * 60)

        return {'lstm': lstm_success, 'vertexai': vertexai_success}

    def get_execution_summary(self, hours=24):
        """å®Ÿè¡Œã‚µãƒžãƒªãƒ¼å–å¾—"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_logs = [log for log in self.execution_log if log['timestamp'] > cutoff_time]

        if not recent_logs:
            return "No recent executions found."

        lstm_logs = [log for log in recent_logs if log['system'] == 'LSTM']
        vertexai_logs = [log for log in recent_logs if log['system'] == 'VertexAI']

        lstm_success_rate = sum(1 for log in lstm_logs if log['success']) / max(1, len(lstm_logs)) * 100
        vertexai_success_rate = sum(1 for log in vertexai_logs if log['success']) / max(1, len(vertexai_logs)) * 100

        avg_lstm_duration = sum(log['duration_seconds'] for log in lstm_logs) / max(1, len(lstm_logs))
        avg_vertexai_duration = sum(log['duration_seconds'] for log in vertexai_logs) / max(1, len(vertexai_logs))

        summary = f"""
ðŸ“Š Execution Summary (Last {hours} hours)

ðŸ§  LSTM System:
  â€¢ Executions: {len(lstm_logs)}
  â€¢ Success Rate: {lstm_success_rate:.1f}%
  â€¢ Avg Duration: {avg_lstm_duration:.1f}s

ðŸ¤– VertexAI System:
  â€¢ Executions: {len(vertexai_logs)}
  â€¢ Success Rate: {vertexai_success_rate:.1f}%
  â€¢ Avg Duration: {avg_vertexai_duration:.1f}s

ðŸ“ˆ Overall:
  â€¢ Total Executions: {len(recent_logs)}
  â€¢ Combined Success Rate: {(lstm_success_rate + vertexai_success_rate) / 2:.1f}%
"""
        return summary

    def setup_schedule(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"""
        # æ¯Ž2æ™‚é–“å®Ÿè¡Œï¼ˆä¸¦åˆ—ï¼‰
        schedule.every(2).hours.do(self.run_both_systems_parallel)

        # æ¯Žæ—¥6æ™‚ã«é †æ¬¡å®Ÿè¡Œï¼ˆç¢ºå®Ÿæ€§é‡è¦–ï¼‰
        schedule.every().day.at("06:00").do(self.run_both_systems_sequential)

        logger.info("ðŸ“… Schedule configured:")
        logger.info("  ðŸ”„ Parallel execution: Every 2 hours")
        logger.info("  ðŸ“‹ Sequential execution: Daily at 6:00 AM")

    def run_scheduler(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼å®Ÿè¡Œ"""
        logger.info("ðŸ•’ Starting Prediction Scheduler")

        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # 1åˆ†é–“éš”ã§ãƒã‚§ãƒƒã‚¯
            except KeyboardInterrupt:
                logger.info("ðŸ›‘ Scheduler stopped by user")
                break
            except Exception as e:
                logger.error(f"âŒ Scheduler error: {e}")
                time.sleep(300)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯5åˆ†å¾…æ©Ÿ

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    orchestrator = PredictionOrchestrator()

    if len(sys.argv) > 1:
        command = sys.argv[1].lower()

        if command == "lstm":
            orchestrator.run_lstm_predictions()
        elif command == "vertexai":
            orchestrator.run_vertexai_predictions()
        elif command == "parallel":
            orchestrator.run_both_systems_parallel()
        elif command == "sequential":
            orchestrator.run_both_systems_sequential()
        elif command == "schedule":
            orchestrator.setup_schedule()
            orchestrator.run_scheduler()
        elif command == "summary":
            print(orchestrator.get_execution_summary())
        else:
            print("Usage: python unified_prediction_orchestrator.py [lstm|vertexai|parallel|sequential|schedule|summary]")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ä¸¦åˆ—å®Ÿè¡Œ
        orchestrator.run_both_systems_parallel()

if __name__ == "__main__":
    main()