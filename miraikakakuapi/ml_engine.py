#!/usr/bin/env python3
"""
Miraikakaku ML Engine - Production-Ready AI Integration System
æœ¬æ ¼çš„ãªMLãƒ¢ãƒ‡ãƒ«é‹ç”¨ã‚¨ãƒ³ã‚¸ãƒ³
"""

import asyncio
import logging
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
import pickle
import os
from pathlib import Path

# ML/AI Libraries
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

# Deep Learning Libraries
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    logger.warning("TensorFlow not available, LSTM models will be disabled")

# Google Cloud VertexAI
try:
    from google.cloud import aiplatform
    from google.cloud.aiplatform import CustomJob
    VERTEXAI_AVAILABLE = True
except ImportError:
    VERTEXAI_AVAILABLE = False
    logger.warning("VertexAI not available, cloud ML features will be disabled")

# Database
import psycopg2
import psycopg2.extras

# Configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelType(Enum):
    RANDOM_FOREST = "random_forest"
    GRADIENT_BOOSTING = "gradient_boosting"
    LINEAR_REGRESSION = "linear_regression"
    ENSEMBLE = "ensemble"
    LSTM = "lstm"
    TRANSFORMER = "transformer"

class PredictionTimeframe(Enum):
    ONE_DAY = 1
    ONE_WEEK = 7
    ONE_MONTH = 30
    THREE_MONTHS = 90

@dataclass
class ModelPerformance:
    mae: float
    mse: float
    rmse: float
    r2: float
    accuracy_percentage: float
    confidence_score: float
    last_updated: datetime

@dataclass
class PredictionResult:
    symbol: str
    predicted_price: float
    confidence_score: float
    model_type: str
    timeframe_days: int
    prediction_date: datetime
    features_used: List[str]
    market_conditions: Dict[str, Any]

class FeatureEngineering:
    """é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""

    @staticmethod
    def create_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
        """ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã®ç”Ÿæˆ"""
        df = df.copy()

        # ç§»å‹•å¹³å‡
        for window in [5, 10, 20, 50]:
            df[f'ma_{window}'] = df['close'].rolling(window=window).mean()
            df[f'price_to_ma_{window}'] = df['close'] / df[f'ma_{window}']

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # MACD
        exp1 = df['close'].ewm(span=12).mean()
        exp2 = df['close'].ewm(span=26).mean()
        df['macd'] = exp1 - exp2
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_histogram'] = df['macd'] - df['macd_signal']

        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        df['bb_middle'] = df['close'].rolling(window=20).mean()
        bb_std = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_width'] = df['bb_upper'] - df['bb_lower']
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        df['volatility'] = df['close'].rolling(window=20).std()
        df['price_change'] = df['close'].pct_change()
        df['volume_ma'] = df['volume'].rolling(window=20).mean()
        df['volume_ratio'] = df['volume'] / df['volume_ma']

        return df

    @staticmethod
    def create_market_features(df: pd.DataFrame) -> pd.DataFrame:
        """å¸‚å ´é–¢é€£ç‰¹å¾´é‡ã®ç”Ÿæˆ"""
        df = df.copy()

        # æ›œæ—¥ãƒ»æœˆåŠ¹æœ
        df['dayofweek'] = pd.to_datetime(df['date']).dt.dayofweek
        df['month'] = pd.to_datetime(df['date']).dt.month
        df['is_month_end'] = pd.to_datetime(df['date']).dt.is_month_end.astype(int)

        # ãƒ©ã‚°ç‰¹å¾´é‡
        for lag in [1, 2, 3, 5, 10]:
            df[f'close_lag_{lag}'] = df['close'].shift(lag)
            df[f'volume_lag_{lag}'] = df['volume'].shift(lag)
            df[f'return_lag_{lag}'] = df['price_change'].shift(lag)

        # ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆ
        for window in [5, 10, 20]:
            df[f'close_std_{window}'] = df['close'].rolling(window=window).std()
            df[f'close_min_{window}'] = df['close'].rolling(window=window).min()
            df[f'close_max_{window}'] = df['close'].rolling(window=window).max()
            df[f'volume_std_{window}'] = df['volume'].rolling(window=window).std()

        return df

class MLModelManager:
    """MLãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model_dir: str = "models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        self.models = {}
        self.scalers = {}
        self.performance_history = {}

    def create_model(self, model_type: ModelType, input_shape: Optional[Tuple] = None) -> Any:
        """ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
        if model_type == ModelType.RANDOM_FOREST:
            return RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
        elif model_type == ModelType.GRADIENT_BOOSTING:
            return GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42
            )
        elif model_type == ModelType.LINEAR_REGRESSION:
            return LinearRegression()
        elif model_type == ModelType.LSTM:
            if not TENSORFLOW_AVAILABLE:
                raise ValueError("TensorFlow not available for LSTM model")
            return self._create_lstm_model(input_shape)
        else:
            raise ValueError(f"Unsupported model type: {model_type}")

    def _create_lstm_model(self, input_shape: Tuple) -> Any:
        """LSTM ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
        from tensorflow.keras.layers import Input

        logger.info(f"Creating LSTM model with input shape: {input_shape}")

        model = Sequential()
        model.add(Input(shape=input_shape))
        model.add(LSTM(50, return_sequences=False))
        model.add(Dropout(0.2))
        model.add(Dense(25, activation='relu'))
        model.add(Dense(1))

        logger.info("LSTM model layers created successfully")

        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mean_squared_error',
            metrics=['mae']
        )

        return model

    def _prepare_lstm_data(self, X: np.ndarray, y: np.ndarray, time_steps: int = 60) -> Tuple[np.ndarray, np.ndarray]:
        """LSTMç”¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        X_seq, y_seq = [], []

        for i in range(time_steps, len(X)):
            X_seq.append(X[i-time_steps:i])
            y_seq.append(y[i])

        return np.array(X_seq), np.array(y_seq)

    def train_model(self, symbol: str, model_type: ModelType, X_train: np.ndarray,
                   y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray) -> Tuple[Any, ModelPerformance]:
        """ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""

        if model_type == ModelType.LSTM:
            # LSTMç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™
            time_steps = 60

            if len(X_train) < time_steps + 10:
                logger.warning(f"Insufficient data for LSTM training: {len(X_train)} samples")
                raise ValueError("Insufficient data for LSTM training")

            X_train_seq, y_train_seq = self._prepare_lstm_data(X_train, y_train, time_steps)
            X_val_seq, y_val_seq = self._prepare_lstm_data(X_val, y_val, time_steps)

            # Debug: Log shapes
            logger.info(f"LSTM training data shapes:")
            logger.info(f"  X_train_seq: {X_train_seq.shape}")
            logger.info(f"  y_train_seq: {y_train_seq.shape}")
            logger.info(f"  X_val_seq: {X_val_seq.shape}")
            logger.info(f"  y_val_seq: {y_val_seq.shape}")

            input_shape = (time_steps, X_train.shape[1])
            logger.info(f"  Expected input_shape: {input_shape}")
            model = self.create_model(model_type, input_shape)

            # EarlyStopping
            early_stopping = EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            )

            # è¨“ç·´ - Handle empty validation set
            if len(X_val_seq) == 0:
                logger.warning("Empty validation set, training without validation")
                history = model.fit(
                    X_train_seq, y_train_seq,
                    validation_split=0.2,  # Use built-in validation split
                    epochs=50,  # Reduce epochs since no early stopping
                    batch_size=32,
                    verbose=0
                )
                # Use train split for validation metrics
                X_val_seq = X_train_seq[-20:]  # Last 20% for validation
                y_val_seq = y_train_seq[-20:]
            else:
                history = model.fit(
                    X_train_seq, y_train_seq,
                    validation_data=(X_val_seq, y_val_seq),
                    epochs=100,
                    batch_size=32,
                    callbacks=[early_stopping],
                    verbose=0
                )

            # äºˆæ¸¬
            y_pred = model.predict(X_val_seq).flatten()
            y_val = y_val_seq
        else:
            # å¾“æ¥ã®SKLearnãƒ¢ãƒ‡ãƒ«
            model = self.create_model(model_type)

            # è¨“ç·´
            model.fit(X_train, y_train)

            # äºˆæ¸¬
            y_pred = model.predict(X_val)

        # æ€§èƒ½è©•ä¾¡
        mae = mean_absolute_error(y_val, y_pred)
        mse = mean_squared_error(y_val, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_val, y_pred)

        # ç²¾åº¦ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼ˆMAEãƒ™ãƒ¼ã‚¹ï¼‰
        mean_price = np.mean(y_val)
        accuracy_percentage = max(0, 100 * (1 - mae / mean_price))

        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        confidence_score = min(100, accuracy_percentage * r2 if r2 > 0 else 0)

        performance = ModelPerformance(
            mae=mae,
            mse=mse,
            rmse=rmse,
            r2=r2,
            accuracy_percentage=accuracy_percentage,
            confidence_score=confidence_score,
            last_updated=datetime.now()
        )

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_key = f"{symbol}_{model_type.value}"
        self.models[model_key] = model
        self.performance_history[model_key] = performance

        self.save_model(model_key, model)

        logger.info(f"Model trained for {symbol} ({model_type.value}): "
                   f"MAE={mae:.4f}, R2={r2:.4f}, Accuracy={accuracy_percentage:.2f}%")

        return model, performance

    def save_model(self, model_key: str, model: Any):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        model_path = self.model_dir / f"{model_key}.pkl"
        joblib.dump(model, model_path)

    def load_model(self, model_key: str) -> Optional[Any]:
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        model_path = self.model_dir / f"{model_key}.pkl"
        if model_path.exists():
            return joblib.load(model_path)
        return None

    def get_best_model(self, symbol: str) -> Tuple[Optional[Any], Optional[str]]:
        """æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        best_model = None
        best_model_type = None
        best_score = -float('inf')

        for model_type in ModelType:
            model_key = f"{symbol}_{model_type.value}"
            if model_key in self.performance_history:
                performance = self.performance_history[model_key]
                score = performance.confidence_score
                if score > best_score:
                    best_score = score
                    best_model = self.models.get(model_key)
                    best_model_type = model_type.value

        return best_model, best_model_type

class MiraikakakuMLEngine:
    """ãƒ¡ã‚¤ãƒ³MLã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        # ç›´æ¥PostgreSQLæ¥ç¶šã‚’ä½¿ç”¨
        self.db_config = {
            'host': '34.173.9.214',
            'user': 'postgres',
            'password': 'os.getenv('DB_PASSWORD', '')',
            'database': 'miraikakaku'
        }

        self.model_manager = MLModelManager()
        self.feature_engineering = FeatureEngineering()
        self.trained_symbols = set()

    def get_connection(self):
        try:
            return psycopg2.connect(**self.db_config, cursor_factory=psycopg2.extras.RealDictCursor)
        except Exception as e:
            logger.error(f"Database connection error: {e}")
            raise Exception(f"Database connection failed: {e}")

    async def get_stock_data(self, symbol: str, days: int = 365) -> pd.DataFrame:
        """æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã®å–å¾—"""
        connection = self.get_connection()

        try:
            with connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT date, open_price as open, high_price as high, low_price as low, close_price as close, volume
                    FROM stock_prices
                    WHERE symbol = %s
                    AND date >= %s
                    ORDER BY date ASC
                """, (symbol, datetime.now() - timedelta(days=days)))

                rows = cursor.fetchall()
                if not rows:
                    return pd.DataFrame()

                df = pd.DataFrame(rows)
                df['date'] = pd.to_datetime(df['date'])

                # Convert Decimal to float for numeric columns
                numeric_columns = ['open', 'high', 'low', 'close', 'volume']
                for col in numeric_columns:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')

                return df

        finally:
            connection.close()

    async def prepare_features(self, symbol: str) -> Tuple[pd.DataFrame, List[str]]:
        """ç‰¹å¾´é‡ã®æº–å‚™"""
        df = await self.get_stock_data(symbol)

        if df.empty:
            return pd.DataFrame(), []

        # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¿½åŠ 
        df = self.feature_engineering.create_technical_indicators(df)

        # å¸‚å ´ç‰¹å¾´é‡ã‚’è¿½åŠ 
        df = self.feature_engineering.create_market_features(df)

        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã®å®šç¾©
        feature_columns = [
            'open', 'high', 'low', 'volume',
            'ma_5', 'ma_10', 'ma_20', 'ma_50',
            'price_to_ma_5', 'price_to_ma_10', 'price_to_ma_20', 'price_to_ma_50',
            'rsi', 'macd', 'macd_signal', 'macd_histogram',
            'bb_width', 'bb_position', 'volatility', 'volume_ratio',
            'dayofweek', 'month', 'is_month_end',
            'close_lag_1', 'close_lag_2', 'close_lag_3', 'close_lag_5',
            'volume_lag_1', 'volume_lag_2', 'volume_lag_3',
            'return_lag_1', 'return_lag_2', 'return_lag_3',
            'close_std_5', 'close_std_10', 'close_std_20',
            'close_min_5', 'close_max_5', 'close_min_10', 'close_max_10',
            'volume_std_5', 'volume_std_10'
        ]

        # å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿ã‚’ä½¿ç”¨
        available_features = [col for col in feature_columns if col in df.columns]

        # NaNã‚’é™¤å»
        df = df.dropna()

        return df, available_features

    async def train_models_for_symbol(self, symbol: str) -> Dict[str, ModelPerformance]:
        """ç‰¹å®šéŠ˜æŸ„ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        logger.info(f"Training models for {symbol}")

        df, feature_columns = await self.prepare_features(symbol)

        if df.empty or len(df) < 100:
            logger.warning(f"Insufficient data for {symbol}")
            return {}

        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®æº–å‚™
        X = df[feature_columns].values
        y = df['close'].values

        # ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ï¼ˆæ™‚ç³»åˆ—ã‚’è€ƒæ…®ï¼‰
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]

        # æ­£è¦åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä¿å­˜
        scaler_key = f"{symbol}_scaler"
        self.model_manager.scalers[scaler_key] = scaler

        performances = {}

        # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ï¼ˆLSTMã‚’å«ã‚€ï¼‰
        model_types_to_train = [
            ModelType.RANDOM_FOREST,
            ModelType.GRADIENT_BOOSTING,
            ModelType.LINEAR_REGRESSION
        ]

        # TensorFlowãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯LSTMã‚‚è¨“ç·´
        if TENSORFLOW_AVAILABLE and len(X_train_scaled) >= 70:  # LSTMç”¨ã®æœ€å°ãƒ‡ãƒ¼ã‚¿æ•°
            model_types_to_train.append(ModelType.LSTM)

        for model_type in model_types_to_train:
            try:
                model, performance = self.model_manager.train_model(
                    symbol, model_type, X_train_scaled, y_train, X_val_scaled, y_val
                )
                performances[model_type.value] = performance
            except Exception as e:
                logger.error(f"Error training {model_type.value} for {symbol}: {e}")

        self.trained_symbols.add(symbol)
        return performances

    async def predict_price(self, symbol: str, timeframe: PredictionTimeframe = PredictionTimeframe.ONE_DAY) -> Optional[PredictionResult]:
        """ä¾¡æ ¼äºˆæ¸¬"""
        if symbol not in self.trained_symbols:
            await self.train_models_for_symbol(symbol)

        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        df, feature_columns = await self.prepare_features(symbol)

        if df.empty:
            return None

        # æœ€æ–°ã®ç‰¹å¾´é‡
        latest_features = df[feature_columns].iloc[-1:].values

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®å–å¾—
        scaler_key = f"{symbol}_scaler"
        scaler = self.model_manager.scalers.get(scaler_key)

        if scaler is None:
            logger.error(f"No scaler found for {symbol}")
            return None

        # æ­£è¦åŒ–
        latest_features_scaled = scaler.transform(latest_features)

        # æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
        best_model, best_model_type = self.model_manager.get_best_model(symbol)

        if best_model is None:
            logger.error(f"No trained model found for {symbol}")
            return None

        # äºˆæ¸¬å®Ÿè¡Œ
        predicted_price = best_model.predict(latest_features_scaled)[0]

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã®å–å¾—
        model_key = f"{symbol}_{best_model_type}"
        performance = self.model_manager.performance_history.get(model_key)
        confidence_score = performance.confidence_score if performance else 0.0

        # å¸‚å ´çŠ¶æ³ã®åˆ†æ
        market_conditions = self._analyze_market_conditions(df)

        return PredictionResult(
            symbol=symbol,
            predicted_price=float(predicted_price),
            confidence_score=confidence_score,
            model_type=best_model_type,
            timeframe_days=timeframe.value,
            prediction_date=datetime.now(),
            features_used=feature_columns,
            market_conditions=market_conditions
        )

    def _analyze_market_conditions(self, df: pd.DataFrame) -> Dict[str, Any]:
        """å¸‚å ´çŠ¶æ³ã®åˆ†æ"""
        if df.empty:
            return {}

        latest = df.iloc[-1]

        return {
            "volatility": float(latest.get('volatility', 0)),
            "rsi": float(latest.get('rsi', 50)),
            "macd_signal": "bullish" if latest.get('macd', 0) > latest.get('macd_signal', 0) else "bearish",
            "trend": "uptrend" if latest.get('close', 0) > latest.get('ma_20', 0) else "downtrend",
            "volume_analysis": "high" if latest.get('volume_ratio', 1) > 1.5 else "normal",
            "bollinger_position": float(latest.get('bb_position', 0.5))
        }

    async def save_prediction_to_db(self, prediction: PredictionResult):
        """äºˆæ¸¬çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
        connection = self.get_connection()

        try:
            with connection.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO stock_predictions
                    (symbol, predicted_price, confidence_score, model_type, timeframe_days,
                     prediction_date, features_used, market_conditions)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (symbol, prediction_date, timeframe_days)
                    DO UPDATE SET
                        predicted_price = EXCLUDED.predicted_price,
                        confidence_score = EXCLUDED.confidence_score,
                        model_type = EXCLUDED.model_type,
                        features_used = EXCLUDED.features_used,
                        market_conditions = EXCLUDED.market_conditions,
                        updated_at = CURRENT_TIMESTAMP
                """, (
                    prediction.symbol,
                    prediction.predicted_price,
                    prediction.confidence_score,
                    prediction.model_type,
                    prediction.timeframe_days,
                    prediction.prediction_date,
                    json.dumps(prediction.features_used),
                    json.dumps(prediction.market_conditions)
                ))

            connection.commit()
            logger.info(f"Prediction saved for {prediction.symbol}: {prediction.predicted_price}")

        except Exception as e:
            logger.error(f"Error saving prediction: {e}")
            connection.rollback()
        finally:
            connection.close()

    async def batch_train_popular_symbols(self, symbols: List[str]):
        """äººæ°—éŠ˜æŸ„ã®ä¸€æ‹¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        logger.info(f"Starting batch training for {len(symbols)} symbols")

        for symbol in symbols:
            try:
                await self.train_models_for_symbol(symbol)
                await asyncio.sleep(0.1)  # ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™
            except Exception as e:
                logger.error(f"Error training {symbol}: {e}")

        logger.info("Batch training completed")

    async def batch_predict_popular_symbols(self, symbols: List[str]) -> List[PredictionResult]:
        """äººæ°—éŠ˜æŸ„ã®ä¸€æ‹¬äºˆæ¸¬"""
        predictions = []

        for symbol in symbols:
            try:
                prediction = await self.predict_price(symbol)
                if prediction:
                    await self.save_prediction_to_db(prediction)
                    predictions.append(prediction)
                await asyncio.sleep(0.1)  # ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™
            except Exception as e:
                logger.error(f"Error predicting {symbol}: {e}")

        logger.info(f"Generated {len(predictions)} predictions")
        return predictions

    def get_model_performance_summary(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚µãƒãƒªãƒ¼"""
        summary = {
            "total_trained_symbols": len(self.trained_symbols),
            "models_by_symbol": {},
            "overall_performance": {
                "avg_accuracy": 0.0,
                "avg_confidence": 0.0,
                "best_performing_model": None,
                "worst_performing_model": None
            }
        }

        accuracies = []
        confidences = []

        for model_key, performance in self.model_manager.performance_history.items():
            symbol, model_type = model_key.rsplit('_', 1)

            if symbol not in summary["models_by_symbol"]:
                summary["models_by_symbol"][symbol] = {}

            summary["models_by_symbol"][symbol][model_type] = asdict(performance)
            accuracies.append(performance.accuracy_percentage)
            confidences.append(performance.confidence_score)

        if accuracies:
            summary["overall_performance"]["avg_accuracy"] = np.mean(accuracies)
            summary["overall_performance"]["avg_confidence"] = np.mean(confidences)

        return summary

class VertexAIIntegration:
    """VertexAIçµ±åˆã‚¯ãƒ©ã‚¹"""

    def __init__(self, project_id: str = "pricewise-huqkr", location: str = "us-central1"):
        self.project_id = project_id
        self.location = location
        self.initialized = False

        if VERTEXAI_AVAILABLE:
            try:
                aiplatform.init(project=project_id, location=location)
                self.initialized = True
                logger.info(f"VertexAI initialized for project {project_id}")
            except Exception as e:
                logger.error(f"Failed to initialize VertexAI: {e}")

    def is_available(self) -> bool:
        """VertexAIãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª"""
        return VERTEXAI_AVAILABLE and self.initialized

    async def deploy_lstm_model(self, symbol: str, model_path: str) -> Optional[str]:
        """LSTMãƒ¢ãƒ‡ãƒ«ã‚’VertexAIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ãƒ‡ãƒ—ãƒ­ã‚¤"""
        if not self.is_available():
            logger.warning("VertexAI not available for model deployment")
            return None

        try:
            # ãƒ¢ãƒ‡ãƒ«ã®ç™»éŒ²
            model = aiplatform.Model.upload(
                display_name=f"lstm-{symbol.lower()}-v1",
                artifact_uri=model_path,
                serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-8:latest"
            )

            # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ä½œæˆ
            endpoint = aiplatform.Endpoint.create(
                display_name=f"lstm-{symbol.lower()}-endpoint"
            )

            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤
            model.deploy(
                endpoint=endpoint,
                deployed_model_display_name=f"lstm-{symbol.lower()}-deployed",
                machine_type="n1-standard-2",
                min_replica_count=1,
                max_replica_count=1
            )

            logger.info(f"Model deployed to VertexAI: {endpoint.resource_name}")
            return endpoint.resource_name

        except Exception as e:
            logger.error(f"Failed to deploy model to VertexAI: {e}")
            return None

    async def predict_with_vertex_ai(self, endpoint_name: str, instances: List[Dict]) -> Optional[List[float]]:
        """VertexAIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§äºˆæ¸¬"""
        if not self.is_available():
            return None

        try:
            endpoint = aiplatform.Endpoint(endpoint_name)
            prediction = endpoint.predict(instances=instances)
            return prediction.predictions

        except Exception as e:
            logger.error(f"VertexAI prediction failed: {e}")
            return None

    async def batch_predict_vertex_ai(self, model_name: str, input_data: List[Dict],
                                     output_location: str) -> Optional[str]:
        """VertexAIãƒãƒƒãƒäºˆæ¸¬"""
        if not self.is_available():
            return None

        try:
            model = aiplatform.Model(model_name)

            batch_job = model.batch_predict(
                job_display_name=f"batch-prediction-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
                instances_format="jsonl",
                gcs_source=[input_data],
                gcs_destination_prefix=output_location,
                machine_type="n1-standard-4"
            )

            logger.info(f"Batch prediction job created: {batch_job.resource_name}")
            return batch_job.resource_name

        except Exception as e:
            logger.error(f"VertexAI batch prediction failed: {e}")
            return None

class EnhancedMiraikakakuMLEngine(MiraikakakuMLEngine):
    """LSTM+VertexAIçµ±åˆç‰ˆMLã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        super().__init__()
        self.vertex_ai = VertexAIIntegration()
        self.lstm_models = {}  # LSTMå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ä¿å­˜

    async def predict_with_lstm(self, symbol: str, timeframe: PredictionTimeframe = PredictionTimeframe.ONE_DAY) -> Optional[PredictionResult]:
        """LSTMå°‚ç”¨äºˆæ¸¬"""
        if not TENSORFLOW_AVAILABLE:
            logger.warning("TensorFlow not available for LSTM prediction")
            return await self.predict_price(symbol, timeframe)

        # LSTM ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¨“ç·´
        if symbol not in self.trained_symbols:
            await self.train_models_for_symbol(symbol)

        # LSTM ãƒ¢ãƒ‡ãƒ«ã®å–å¾—
        lstm_model_key = f"{symbol}_lstm"
        lstm_model = self.model_manager.models.get(lstm_model_key)

        if lstm_model is None:
            logger.warning(f"No LSTM model found for {symbol}, falling back to best available model")
            return await self.predict_price(symbol, timeframe)

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df, feature_columns = await self.prepare_features(symbol)
        if df.empty:
            return None

        # LSTMç”¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ãƒ‡ãƒ¼ã‚¿æº–å‚™
        time_steps = 60
        X = df[feature_columns].values
        scaler_key = f"{symbol}_scaler"
        scaler = self.model_manager.scalers.get(scaler_key)

        if scaler is None:
            logger.error(f"No scaler found for {symbol}")
            return None

        X_scaled = scaler.transform(X)

        # æœ€æ–°ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å–å¾—
        if len(X_scaled) < time_steps:
            logger.warning(f"Insufficient data for LSTM prediction: {len(X_scaled)} < {time_steps}")
            return await self.predict_price(symbol, timeframe)

        latest_sequence = X_scaled[-time_steps:].reshape(1, time_steps, X_scaled.shape[1])

        try:
            # LSTMäºˆæ¸¬å®Ÿè¡Œ
            logger.info(f"LSTM prediction input shape: {latest_sequence.shape}")
            predicted_price = lstm_model.predict(latest_sequence, verbose=0)[0][0]
            logger.info(f"LSTM prediction successful for {symbol}: {predicted_price}")

        except Exception as e:
            logger.error(f"Error during LSTM prediction for {symbol}: {e}")
            logger.warning(f"Falling back to best available model for {symbol}")
            return await self.predict_price(symbol, timeframe)

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã®å–å¾—
        model_key = f"{symbol}_lstm"
        performance = self.model_manager.performance_history.get(model_key)
        confidence_score = performance.confidence_score if performance else 85.0

        # å¸‚å ´çŠ¶æ³ã®åˆ†æ
        market_conditions = self._analyze_market_conditions(df)

        return PredictionResult(
            symbol=symbol,
            predicted_price=float(predicted_price),
            confidence_score=confidence_score,
            model_type="LSTM",
            timeframe_days=timeframe.value,
            prediction_date=datetime.now(),
            features_used=feature_columns,
            market_conditions=market_conditions
        )

    async def predict_with_vertex_ai(self, symbol: str, timeframe: PredictionTimeframe = PredictionTimeframe.ONE_DAY) -> Optional[PredictionResult]:
        """VertexAIç‹¬ç«‹äºˆæ¸¬ï¼ˆå°†æ¥çš„ã«GCPãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰"""
        if not self.vertex_ai.is_available():
            logger.warning("VertexAI not available")
            return None

        # ç¾åœ¨ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã§VertexAIé¢¨ã®ç‹¬è‡ªäºˆæ¸¬ã‚’ç”Ÿæˆ
        # å°†æ¥çš„ã«ã¯ã“ã“ã§VertexAIã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«æ¥ç¶š

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        df, feature_columns = await self.prepare_features(symbol)
        if df.empty:
            return None

        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‹¬è‡ªã®VertexAIé¢¨äºˆæ¸¬ã‚’ç”Ÿæˆ
        latest = df.iloc[-1]
        current_price = float(latest.get('close', 0))

        # VertexAIé¢¨ã®é«˜åº¦ãªAIäºˆæ¸¬ï¼ˆå°†æ¥çš„ã«ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰
        # ç¾åœ¨ã¯ç‹¬è‡ªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§äºˆæ¸¬
        volatility = float(latest.get('volatility', 0))
        rsi = float(latest.get('rsi', 50))
        macd = float(latest.get('macd', 0))

        # VertexAIç‹¬è‡ªã®äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        price_change = 0.0
        if rsi > 70:  # éè²·ã„
            price_change = -0.02
        elif rsi < 30:  # éå£²ã‚Š
            price_change = 0.03
        else:
            price_change = macd * 0.001

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´
        price_change = price_change * (1 + volatility * 0.1)

        # æ™‚é–“æ ã«ã‚ˆã‚‹äºˆæ¸¬èª¿æ•´
        price_change = price_change * timeframe.value

        predicted_price = current_price * (1 + price_change)

        # VertexAIäºˆæ¸¬ã®ä¿¡é ¼åº¦ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¢ãƒ‡ãƒ«æƒ³å®šï¼‰
        confidence_score = 85.0 + (15.0 if volatility < 10 else 0)

        # å¸‚å ´çŠ¶æ³ã®åˆ†æ
        market_conditions = self._analyze_market_conditions(df)

        return PredictionResult(
            symbol=symbol,
            predicted_price=float(predicted_price),
            confidence_score=confidence_score,
            model_type="VertexAI",
            timeframe_days=timeframe.value,
            prediction_date=datetime.now(),
            features_used=["Advanced Cloud AI Features"],
            market_conditions=market_conditions
        )

    async def get_dual_predictions(self, symbol: str, timeframe: PredictionTimeframe = PredictionTimeframe.ONE_DAY) -> Dict[str, Optional[PredictionResult]]:
        """LSTMãƒ»VertexAIä¸¡æ–¹ã®ç‹¬ç«‹äºˆæ¸¬ã‚’å–å¾—"""
        lstm_prediction = await self.predict_with_lstm(symbol, timeframe)
        vertex_ai_prediction = await self.predict_with_vertex_ai(symbol, timeframe)

        return {
            "lstm": lstm_prediction,
            "vertex_ai": vertex_ai_prediction
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«MLã‚¨ãƒ³ã‚¸ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆæ‹¡å¼µç‰ˆï¼‰
ml_engine = EnhancedMiraikakakuMLEngine()

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    async def test_ml_engine():
        # äººæ°—éŠ˜æŸ„ã§ãƒ†ã‚¹ãƒˆ
        test_symbols = ["AAPL", "MSFT", "GOOGL", "TSLA", "NVDA"]

        print("ğŸ¤– Starting ML Engine test...")

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        await ml_engine.batch_train_popular_symbols(test_symbols[:2])  # åˆ¶é™ãƒ†ã‚¹ãƒˆ

        # äºˆæ¸¬å®Ÿè¡Œ
        predictions = await ml_engine.batch_predict_popular_symbols(test_symbols[:2])

        # çµæœè¡¨ç¤º
        for prediction in predictions:
            print(f"ğŸ“ˆ {prediction.symbol}: ${prediction.predicted_price:.2f} "
                  f"(Confidence: {prediction.confidence_score:.1f}%)")

        # æ€§èƒ½ã‚µãƒãƒªãƒ¼
        summary = ml_engine.get_model_performance_summary()
        print(f"ğŸ“Š Performance Summary: {summary['overall_performance']}")

    asyncio.run(test_ml_engine())