#!/usr/bin/env python3
"""
Cloud Run ãƒãƒƒãƒå‡¦ç†ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ - ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªå¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
"""

import subprocess
import logging
import sys
import os
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def deploy_batch_jobs():
    """ãƒãƒƒãƒå‡¦ç†ã‚’Cloud Runã«ãƒ‡ãƒ—ãƒ­ã‚¤"""

    logger.info("ğŸš€ Cloud Run ãƒãƒƒãƒå‡¦ç†ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé–‹å§‹")

    # ä¸»è¦ãƒãƒƒãƒå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    batch_jobs = [
        {
            "name": "comprehensive-batch",
            "script": "comprehensive_batch.py",
            "description": "8380éŠ˜æŸ„ 2å¹´é–“ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ + 30æ—¥äºˆæ¸¬",
            "memory": "2Gi",
            "cpu": "2",
            "timeout": "3600s",
            "concurrency": "1",
        },
        {
            "name": "ultimate-100-point",
            "script": "ultimate_100_point_system.py",
            "description": "2000éŠ˜æŸ„ 10å¹´é–“ + 365æ—¥äºˆæ¸¬",
            "memory": "4Gi",
            "cpu": "4",
            "timeout": "7200s",
            "concurrency": "1",
        },
        {
            "name": "turbo-expansion",
            "script": "turbo_expansion.py",
            "description": "89éŠ˜æŸ„ é«˜é€Ÿãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ",
            "memory": "1Gi",
            "cpu": "1",
            "timeout": "1800s",
            "concurrency": "2",
        },
        {
            "name": "continuous-pipeline",
            "script": "continuous_247_pipeline.py",
            "description": "24/7 ç¶™ç¶šãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
            "memory": "1Gi",
            "cpu": "1",
            "timeout": "86400s",
            "concurrency": "1",
        },
        {
            "name": "foreign-key-fix",
            "script": "fix_foreign_key_constraints.py",
            "description": "å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ä¿®æ­£",
            "memory": "512Mi",
            "cpu": "0.5",
            "timeout": "1800s",
            "concurrency": "1",
        },
    ]

    project_id = "pricewise-huqkr"
    region = "us-central1"

    deployed_services = []

    for job in batch_jobs:
        try:
            logger.info(f"ğŸ“¦ ãƒ‡ãƒ—ãƒ­ã‚¤ä¸­: {job['name']} ({job['description']})")

            # Cloud Run ã‚µãƒ¼ãƒ“ã‚¹ä½œæˆã‚³ãƒãƒ³ãƒ‰
            cmd = [
                "gcloud",
                "run",
                "deploy",
                job["name"],
                "--source",
                ".",
                "--platform",
                "managed",
                "--region",
                region,
                "--project",
                project_id,
                "--memory",
                job["memory"],
                "--cpu",
                job["cpu"],
                "--timeout",
                job["timeout"],
                "--concurrency",
                job["concurrency"],
                "--max-instances",
                "5",
                "--min-instances",
                "0",
                "--port",
                "8080",
                "--set-env-vars",
                f'BATCH_SCRIPT={job["script"]}',
                "--allow-unauthenticated",
                "--quiet",
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)

            if result.returncode == 0:
                logger.info(f"  âœ… {job['name']} ãƒ‡ãƒ—ãƒ­ã‚¤æˆåŠŸ")

                # ã‚µãƒ¼ãƒ“ã‚¹URLã‚’å–å¾—
                url_cmd = [
                    "gcloud",
                    "run",
                    "services",
                    "describe",
                    job["name"],
                    "--region",
                    region,
                    "--format",
                    "value(status.url)",
                ]
                url_result = subprocess.run(url_cmd, capture_output=True, text=True)

                if url_result.returncode == 0:
                    service_url = url_result.stdout.strip()
                    deployed_services.append(
                        {
                            "name": job["name"],
                            "url": service_url,
                            "script": job["script"],
                            "description": job["description"],
                        }
                    )
                    logger.info(f"  ğŸŒ URL: {service_url}")
                else:
                    logger.warning(f"  âš ï¸  URLå–å¾—å¤±æ•—: {job['name']}")
            else:
                logger.error(f"  âŒ {job['name']} ãƒ‡ãƒ—ãƒ­ã‚¤å¤±æ•—:")
                logger.error(f"     STDOUT: {result.stdout}")
                logger.error(f"     STDERR: {result.stderr}")

        except subprocess.TimeoutExpired:
            logger.error(f"  â±ï¸  {job['name']} ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
        except Exception as e:
            logger.error(f"  ğŸ’¥ {job['name']} ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¨ãƒ©ãƒ¼: {e}")

    logger.info(f"ğŸ¯ ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†: {len(deployed_services)}/{len(batch_jobs)} ã‚µãƒ¼ãƒ“ã‚¹")

    # ãƒ‡ãƒ—ãƒ­ã‚¤æ¸ˆã¿ã‚µãƒ¼ãƒ“ã‚¹ä¸€è¦§
    if deployed_services:
        logger.info("ğŸ“‹ ãƒ‡ãƒ—ãƒ­ã‚¤æ¸ˆã¿ã‚µãƒ¼ãƒ“ã‚¹:")
        for service in deployed_services:
            logger.info(f"  ğŸŸ¢ {service['name']}: {service['description']}")
            logger.info(f"     URL: {service['url']}")

    return deployed_services


def setup_cloud_scheduler():
    """Cloud Scheduler ã§è‡ªå‹•å®Ÿè¡Œã‚’è¨­å®š"""

    logger.info("â° Cloud Scheduler è¨­å®šé–‹å§‹")

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚¸ãƒ§ãƒ–
    scheduler_jobs = [
        {
            "name": "comprehensive-batch-daily",
            "schedule": "0 2 * * *",  # æ¯æ—¥2æ™‚
            "target": "comprehensive-batch",
            "description": "æ—¥æ¬¡åŒ…æ‹¬ãƒãƒƒãƒå‡¦ç†",
        },
        {
            "name": "turbo-expansion-hourly",
            "schedule": "0 */6 * * *",  # 6æ™‚é–“æ¯
            "target": "turbo-expansion",
            "description": "6æ™‚é–“æ¯ã‚¿ãƒ¼ãƒœæ‹¡å¼µ",
        },
        {
            "name": "foreign-key-maintenance",
            "schedule": "30 1 * * *",  # æ¯æ—¥1:30
            "target": "foreign-key-fix",
            "description": "å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹",
        },
    ]

    region = "us-central1"
    created_jobs = []

    for job in scheduler_jobs:
        try:
            logger.info(f"ğŸ“… ä½œæˆä¸­: {job['name']}")

            cmd = [
                "gcloud",
                "scheduler",
                "jobs",
                "create",
                "http",
                job["name"],
                "--schedule",
                job["schedule"],
                "--uri",
                f"https://{job['target']}-123456789-uc.a.run.app",  # å®Ÿéš›ã®URLã«ç½®æ›
                "--http-method",
                "POST",
                "--location",
                region,
                "--description",
                job["description"],
            ]

            result = subprocess.run(cmd, capture_output=True, text=True)

            if result.returncode == 0:
                logger.info(f"  âœ… {job['name']} ä½œæˆæˆåŠŸ")
                created_jobs.append(job["name"])
            else:
                logger.error(f"  âŒ {job['name']} ä½œæˆå¤±æ•—: {result.stderr}")

        except Exception as e:
            logger.error(f"  ğŸ’¥ {job['name']} ã‚¨ãƒ©ãƒ¼: {e}")

    logger.info(f"â° ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šå®Œäº†: {len(created_jobs)} ã‚¸ãƒ§ãƒ–")
    return created_jobs


def create_batch_dockerfile():
    """ãƒãƒƒãƒå‡¦ç†ç”¨ã®Dockerfileã‚’æœ€é©åŒ–"""

    dockerfile_content = """FROM python:3.11-slim

WORKDIR /app

# å¿…è¦ãªã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN apt-get update && apt-get install -y \\
    gcc \\
    && rm -rf /var/lib/apt/lists/*

# è¦ä»¶ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
COPY . .

# ç’°å¢ƒå¤‰æ•°
ENV PYTHONPATH=/app
ENV PORT=8080

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ã‚µãƒ¼ãƒãƒ¼ã‚’å«ã‚€ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
COPY docker-entrypoint.py .
EXPOSE 8080

CMD ["python", "docker-entrypoint.py"]"""

    with open(
        "/mnt/c/Users/yuuku/cursor/miraikakaku/miraikakakuapi/functions/Dockerfile", "w"
    ) as f:
        f.write(dockerfile_content)

    logger.info("ğŸ“„ æœ€é©åŒ–ã•ã‚ŒãŸDockerfileä½œæˆå®Œäº†")


def create_entrypoint():
    """Cloud Runç”¨ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆä½œæˆ"""

    entrypoint_content = '''#!/usr/bin/env python3
"""
Cloud Run ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ - HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ
"""

import os
import sys
import subprocess
import logging
from http.server import HTTPServer, BaseHTTPRequestHandler
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BatchHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        """POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ"""

        batch_script = os.environ.get('BATCH_SCRIPT', 'comprehensive_batch.py')

        try:
            logger.info(f"ğŸš€ ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {batch_script}")

            # ãƒãƒƒãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
            result = subprocess.run(
                [sys.executable, batch_script],
                capture_output=True,
                text=True,
                timeout=3600  # 1æ™‚é–“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )

            if result.returncode == 0:
                response = {
                    'status': 'success',
                    'script': batch_script,
                    'stdout': result.stdout[-1000:],  # æœ€å¾Œã®1000æ–‡å­—ã®ã¿
                    'message': f'Batch {batch_script} completed successfully'
                }
                self.send_response(200)
            else:
                response = {
                    'status': 'error',
                    'script': batch_script,
                    'stderr': result.stderr[-1000:],
                    'message': f'Batch {batch_script} failed'
                }
                self.send_response(500)

        except subprocess.TimeoutExpired:
            response = {
                'status': 'timeout',
                'script': batch_script,
                'message': f'Batch {batch_script} timed out'
            }
            self.send_response(408)
        except Exception as e:
            response = {
                'status': 'exception',
                'script': batch_script,
                'error': str(e),
                'message': f'Batch {batch_script} encountered an exception'
            }
            self.send_response(500)

        self.send_header('Content-type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(response).encode())

    def do_GET(self):
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        self.send_response(200)
        self.send_header('Content-type', 'text/plain')
        self.end_headers()
        self.wfile.write(b'Batch service is healthy')

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    server = HTTPServer(('', port), BatchHandler)

    logger.info(f"ğŸŒ ãƒãƒƒãƒã‚µãƒ¼ãƒãƒ¼é–‹å§‹ ãƒãƒ¼ãƒˆ:{port}")
    logger.info(f"ğŸ“ ãƒãƒƒãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {os.environ.get('BATCH_SCRIPT', 'comprehensive_batch.py')}")

    server.serve_forever()
'''

    with open(
        "/mnt/c/Users/yuuku/cursor/miraikakaku/miraikakakuapi/functions/docker-entrypoint.py",
        "w",
    ) as f:
        f.write(entrypoint_content)

    logger.info("ğŸ³ Docker ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆä½œæˆå®Œäº†")


if __name__ == "__main__":
    logger.info("=" * 80)
    logger.info("ğŸš€ Google Cloud ãƒãƒƒãƒå‡¦ç†ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ")
    logger.info("=" * 80)

    # 1. å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    create_batch_dockerfile()
    create_entrypoint()

    # 2. Cloud Run ãƒ‡ãƒ—ãƒ­ã‚¤
    deployed = deploy_batch_jobs()

    # 3. Cloud Scheduler è¨­å®š
    scheduled = setup_cloud_scheduler()

    logger.info("=" * 80)
    logger.info(
        f"âœ… ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†: {len(deployed)} ã‚µãƒ¼ãƒ“ã‚¹, {len(scheduled)} ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼"
    )
    logger.info("ğŸ¯ 24/7 è‡ªå‹•ãƒãƒƒãƒå‡¦ç†ãŒé–‹å§‹ã•ã‚Œã¾ã—ãŸ")
    logger.info("=" * 80)
