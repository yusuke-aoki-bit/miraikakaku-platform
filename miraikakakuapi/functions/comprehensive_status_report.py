#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆ - ãƒãƒƒãƒå‡¦ç†ã€DBã€APIå…¨ä½“ã®è©³ç´°çŠ¶æ³
"""

import logging
import sys
import os
from datetime import datetime, timedelta
from sqlalchemy import text
import requests
import json
import subprocess

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from database.database import get_db

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_database_status():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ³è©³ç´°ãƒã‚§ãƒƒã‚¯"""
    logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è©³ç´°çŠ¶æ³")
    logger.info("="*60)
    
    db = next(get_db())
    try:
        # åŸºæœ¬çµ±è¨ˆ
        result = db.execute(text("""
            SELECT 
                (SELECT COUNT(*) FROM stock_master WHERE is_active = 1) as active_symbols,
                (SELECT COUNT(DISTINCT symbol) FROM stock_prices) as price_symbols,
                (SELECT COUNT(*) FROM stock_prices) as price_records,
                (SELECT COUNT(DISTINCT symbol) FROM stock_predictions) as pred_symbols,
                (SELECT COUNT(*) FROM stock_predictions) as pred_records
        """))
        basic_stats = result.fetchone()
        
        logger.info(f"ã€åŸºæœ¬çµ±è¨ˆã€‘")
        logger.info(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–éŠ˜æŸ„: {basic_stats[0]:,}å€‹")
        logger.info(f"  ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿éŠ˜æŸ„: {basic_stats[1]}å€‹ ({basic_stats[1]/basic_stats[0]*100:.2f}%)")
        logger.info(f"  ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {basic_stats[2]:,}ä»¶")
        logger.info(f"  äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿éŠ˜æŸ„: {basic_stats[3]}å€‹")
        logger.info(f"  äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {basic_stats[4]:,}ä»¶")
        
        # ä»Šæ—¥ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£
        result = db.execute(text("""
            SELECT 
                DATE(created_at) as date,
                COUNT(*) as records,
                COUNT(DISTINCT symbol) as symbols
            FROM stock_prices 
            WHERE created_at >= CURDATE() - INTERVAL 3 DAY
            GROUP BY DATE(created_at)
            ORDER BY date DESC
        """))
        daily_activity = result.fetchall()
        
        logger.info(f"\nã€éå»3æ—¥é–“ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã€‘")
        for date, records, symbols in daily_activity:
            logger.info(f"  {date}: {records:,}ä»¶ ({symbols}éŠ˜æŸ„)")
        
        result = db.execute(text("""
            SELECT 
                DATE(created_at) as date,
                COUNT(*) as records,
                COUNT(DISTINCT symbol) as symbols
            FROM stock_predictions 
            WHERE created_at >= CURDATE() - INTERVAL 3 DAY
            GROUP BY DATE(created_at)
            ORDER BY date DESC
        """))
        daily_pred_activity = result.fetchall()
        
        logger.info(f"\nã€éå»3æ—¥é–“ã®äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã€‘")
        for date, records, symbols in daily_pred_activity:
            logger.info(f"  {date}: {records:,}ä»¶ ({symbols}éŠ˜æŸ„)")
        
        # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã®å•é¡Œåˆ†æ
        logger.info(f"\nã€å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„åˆ†æã€‘")
        
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã§å¤–éƒ¨ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„éŠ˜æŸ„
        result = db.execute(text("""
            SELECT DISTINCT sp.symbol 
            FROM stock_prices sp 
            LEFT JOIN stock_master sm ON sp.symbol = sm.symbol 
            WHERE sm.symbol IS NULL
        """))
        orphan_price_symbols = [row[0] for row in result]
        
        # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã§å¤–éƒ¨ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„éŠ˜æŸ„
        result = db.execute(text("""
            SELECT DISTINCT spr.symbol 
            FROM stock_predictions spr 
            LEFT JOIN stock_master sm ON spr.symbol = sm.symbol 
            WHERE sm.symbol IS NULL
        """))
        orphan_pred_symbols = [row[0] for row in result]
        
        logger.info(f"  ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å­¤ç«‹éŠ˜æŸ„: {len(orphan_price_symbols)}å€‹")
        if orphan_price_symbols:
            logger.info(f"    ä¾‹: {', '.join(orphan_price_symbols[:5])}")
        
        logger.info(f"  äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®å­¤ç«‹éŠ˜æŸ„: {len(orphan_pred_symbols)}å€‹")
        if orphan_pred_symbols:
            logger.info(f"    ä¾‹: {', '.join(orphan_pred_symbols[:5])}")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        result = db.execute(text("""
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN open_price IS NULL THEN 1 ELSE 0 END) as null_open,
                SUM(CASE WHEN high_price IS NULL THEN 1 ELSE 0 END) as null_high,
                SUM(CASE WHEN low_price IS NULL THEN 1 ELSE 0 END) as null_low,
                SUM(CASE WHEN close_price IS NULL THEN 1 ELSE 0 END) as null_close,
                SUM(CASE WHEN volume = 0 THEN 1 ELSE 0 END) as zero_volume
            FROM stock_prices
        """))
        quality_stats = result.fetchone()
        
        logger.info(f"\nã€ãƒ‡ãƒ¼ã‚¿å“è³ªã€‘")
        logger.info(f"  ç·ãƒ¬ã‚³ãƒ¼ãƒ‰: {quality_stats[0]:,}ä»¶")
        logger.info(f"  NULLé–‹å§‹ä¾¡æ ¼: {quality_stats[1]}ä»¶ ({quality_stats[1]/quality_stats[0]*100:.1f}%)")
        logger.info(f"  NULLé«˜å€¤: {quality_stats[2]}ä»¶ ({quality_stats[2]/quality_stats[0]*100:.1f}%)")
        logger.info(f"  NULLå®‰å€¤: {quality_stats[3]}ä»¶ ({quality_stats[3]/quality_stats[0]*100:.1f}%)")
        logger.info(f"  NULLçµ‚å€¤: {quality_stats[4]}ä»¶ ({quality_stats[4]/quality_stats[0]*100:.1f}%)")
        logger.info(f"  ã‚¼ãƒ­å‡ºæ¥é«˜: {quality_stats[5]}ä»¶ ({quality_stats[5]/quality_stats[0]*100:.1f}%)")
        
        return {
            'active_symbols': basic_stats[0],
            'price_symbols': basic_stats[1],
            'price_records': basic_stats[2],
            'pred_symbols': basic_stats[3],
            'pred_records': basic_stats[4],
            'orphan_price_symbols': len(orphan_price_symbols),
            'orphan_pred_symbols': len(orphan_pred_symbols)
        }
        
    finally:
        db.close()

def check_batch_processes():
    """ãƒãƒƒãƒå‡¦ç†çŠ¶æ³ãƒã‚§ãƒƒã‚¯"""
    logger.info("\nğŸ”„ ãƒãƒƒãƒå‡¦ç†çŠ¶æ³")
    logger.info("="*60)
    
    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
    log_files = [
        'batch_progress.log',
        'massive_expansion.log', 
        'turbo_expansion.log',
        'instant_mega_boost.log',
        'ultimate_system.log',
        'continuous_pipeline.log',
        'synthetic_boost.log',
        'quick_boost.log'
    ]
    
    batch_status = {}
    
    for log_file in log_files:
        if os.path.exists(log_file):
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
                size = os.path.getsize(log_file)
                
                # æœ€æ–°ã®æ›´æ–°æ™‚é–“
                mtime = os.path.getmtime(log_file)
                last_update = datetime.fromtimestamp(mtime)
                
                # æœ€å¾Œã®æ•°è¡Œã‚’èª­ã‚€
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                    last_lines = lines[-5:] if len(lines) >= 5 else lines
                
                # ã‚¨ãƒ©ãƒ¼æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                error_count = sum(1 for line in lines if 'ERROR' in line or 'ã‚¨ãƒ©ãƒ¼' in line)
                
                batch_status[log_file] = {
                    'size': size,
                    'last_update': last_update,
                    'total_lines': len(lines),
                    'error_count': error_count,
                    'last_lines': [line.strip() for line in last_lines],
                    'active': (datetime.now() - last_update).seconds < 300  # 5åˆ†ä»¥å†…ã«æ›´æ–°
                }
                
                logger.info(f"ğŸ“„ {log_file}")
                logger.info(f"  ã‚µã‚¤ã‚º: {size:,} bytes")
                logger.info(f"  æœ€çµ‚æ›´æ–°: {last_update.strftime('%Y-%m-%d %H:%M:%S')}")
                logger.info(f"  ç·è¡Œæ•°: {len(lines):,}")
                logger.info(f"  ã‚¨ãƒ©ãƒ¼æ•°: {error_count}")
                logger.info(f"  çŠ¶æ…‹: {'ğŸŸ¢ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–' if batch_status[log_file]['active'] else 'ğŸ”´ åœæ­¢ä¸­'}")
                
                if last_lines:
                    logger.info(f"  æœ€æ–°ãƒ­ã‚°:")
                    for line in last_lines[-2:]:  # æœ€å¾Œã®2è¡Œã®ã¿è¡¨ç¤º
                        if line.strip():
                            logger.info(f"    {line}")
                
            except Exception as e:
                logger.error(f"  âŒ ãƒ­ã‚°èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
                batch_status[log_file] = {'error': str(e)}
        else:
            logger.info(f"ğŸ“„ {log_file}: âŒ ãƒ•ã‚¡ã‚¤ãƒ«æœªå­˜åœ¨")
            batch_status[log_file] = {'missing': True}
        
        logger.info("")
    
    return batch_status

def check_api_status():
    """APIçŠ¶æ³ãƒã‚§ãƒƒã‚¯"""
    logger.info("\nğŸŒ APIçŠ¶æ³")
    logger.info("="*60)
    
    api_status = {}
    
    # APIãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    try:
        response = requests.get('http://localhost:8001/health', timeout=5)
        api_status['health'] = {
            'status_code': response.status_code,
            'response': response.json() if response.status_code == 200 else None,
            'accessible': True
        }
        logger.info("ğŸŸ¢ API ã‚µãƒ¼ãƒãƒ¼: ç¨¼åƒä¸­")
        logger.info(f"  ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯: {response.status_code}")
        
        if response.status_code == 200:
            logger.info(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.json()}")
        
    except requests.exceptions.RequestException as e:
        api_status['health'] = {
            'accessible': False,
            'error': str(e)
        }
        logger.info("ğŸ”´ API ã‚µãƒ¼ãƒãƒ¼: åœæ­¢ä¸­ã¾ãŸã¯æ¥ç¶šä¸å¯")
        logger.info(f"  ã‚¨ãƒ©ãƒ¼: {e}")
    
    # å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ†ã‚¹ãƒˆ
    endpoints = [
        '/api/finance/stock/AAPL/price',
        '/api/finance/stock/AAPL/predictions',
        '/api/finance/stocks/list'
    ]
    
    for endpoint in endpoints:
        try:
            response = requests.get(f'http://localhost:8001{endpoint}', timeout=5)
            api_status[endpoint] = {
                'status_code': response.status_code,
                'accessible': True,
                'response_size': len(response.content) if response.content else 0
            }
            
            logger.info(f"ğŸ“ {endpoint}")
            logger.info(f"  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
            logger.info(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚º: {len(response.content)} bytes")
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    if isinstance(data, dict):
                        logger.info(f"  ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)} keys")
                    elif isinstance(data, list):
                        logger.info(f"  ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(data)} items")
                except:
                    logger.info("  ãƒ¬ã‚¹ãƒãƒ³ã‚¹: JSONä»¥å¤–")
            
        except requests.exceptions.RequestException as e:
            api_status[endpoint] = {
                'accessible': False,
                'error': str(e)
            }
            logger.info(f"ğŸ“ {endpoint}: âŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯ - {e}")
    
    return api_status

def check_system_resources():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ³"""
    logger.info("\nğŸ’» ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹")
    logger.info("="*60)
    
    try:
        # ãƒ—ãƒ­ã‚»ã‚¹ä¸€è¦§ï¼ˆPythoné–¢é€£ï¼‰
        result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
        if result.returncode == 0:
            python_processes = [line for line in result.stdout.split('\n') if 'python' in line.lower()]
            logger.info(f"ğŸ Python ãƒ—ãƒ­ã‚»ã‚¹: {len(python_processes)}å€‹å®Ÿè¡Œä¸­")
            
            # ãƒãƒƒãƒå‡¦ç†é–¢é€£ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            batch_processes = [p for p in python_processes if any(batch in p for batch in ['batch', 'expansion', 'boost', 'pipeline'])]
            logger.info(f"ğŸ”„ ãƒãƒƒãƒé–¢é€£ãƒ—ãƒ­ã‚»ã‚¹: {len(batch_processes)}å€‹")
            
            if batch_processes:
                logger.info("  å®Ÿè¡Œä¸­ã®ãƒãƒƒãƒãƒ—ãƒ­ã‚»ã‚¹:")
                for proc in batch_processes[:5]:  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º
                    logger.info(f"    {proc.split()[-1] if proc.split() else 'Unknown'}")
        
        # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡
        result = subprocess.run(['df', '-h', '.'], capture_output=True, text=True)
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            if len(lines) > 1:
                disk_info = lines[1].split()
                logger.info(f"ğŸ’¾ ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡: {disk_info[2]}/{disk_info[1]} ({disk_info[4]})")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆç°¡æ˜“ï¼‰
        try:
            with open('/proc/meminfo', 'r') as f:
                meminfo = f.read()
                for line in meminfo.split('\n'):
                    if 'MemTotal:' in line:
                        total_mem = int(line.split()[1]) // 1024
                        logger.info(f"ğŸ§  ç·ãƒ¡ãƒ¢ãƒª: {total_mem:,} MB")
                        break
        except:
            logger.info("ğŸ§  ãƒ¡ãƒ¢ãƒªæƒ…å ±: å–å¾—ä¸å¯")
    
    except Exception as e:
        logger.info(f"âŒ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

def generate_comprehensive_status_report():
    """åŒ…æ‹¬çš„çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    logger.info("="*80)
    logger.info("ğŸ“‹ Miraikakaku ã‚·ã‚¹ãƒ†ãƒ åŒ…æ‹¬çš„çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆ")
    logger.info(f"ğŸ• ç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*80)
    
    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ³
    db_status = check_database_status()
    
    # 2. ãƒãƒƒãƒå‡¦ç†çŠ¶æ³
    batch_status = check_batch_processes()
    
    # 3. APIçŠ¶æ³
    api_status = check_api_status()
    
    # 4. ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹
    check_system_resources()
    
    # 5. ç·åˆè©•ä¾¡
    logger.info("\nğŸ¯ ç·åˆè©•ä¾¡")
    logger.info("="*60)
    
    # ãƒ‡ãƒ¼ã‚¿å……å®Ÿåº¦
    data_completeness = (db_status['price_records'] / 100000) * 100
    symbol_coverage = (db_status['price_symbols'] / db_status['active_symbols']) * 100
    
    logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å……å®Ÿåº¦:")
    logger.info(f"  ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿: {data_completeness:.2f}% (ç›®æ¨™100,000ä»¶ã«å¯¾ã—{db_status['price_records']:,}ä»¶)")
    logger.info(f"  éŠ˜æŸ„ã‚«ãƒãƒ¼ç‡: {symbol_coverage:.2f}% ({db_status['price_symbols']}/{db_status['active_symbols']}éŠ˜æŸ„)")
    logger.info(f"  äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿: {db_status['pred_records']:,}ä»¶")
    
    # ãƒãƒƒãƒå‡¦ç†çŠ¶æ³
    active_batches = sum(1 for status in batch_status.values() 
                        if isinstance(status, dict) and status.get('active', False))
    total_batches = len([f for f in batch_status.keys() if not batch_status[f].get('missing', False)])
    
    logger.info(f"\nğŸ”„ ãƒãƒƒãƒå‡¦ç†çŠ¶æ³:")
    logger.info(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–: {active_batches}/{total_batches}")
    logger.info(f"  ç·ã‚¨ãƒ©ãƒ¼æ•°: {sum(status.get('error_count', 0) for status in batch_status.values() if isinstance(status, dict))}")
    
    # APIçŠ¶æ³
    api_accessible = api_status.get('health', {}).get('accessible', False)
    logger.info(f"\nğŸŒ APIçŠ¶æ³:")
    logger.info(f"  ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½: {'âœ…' if api_accessible else 'âŒ'}")
    
    # å•é¡Œç‚¹ã¨æ¨å¥¨äº‹é …
    logger.info(f"\nğŸš¨ æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:")
    issues = []
    
    if db_status['orphan_price_symbols'] > 0:
        issues.append(f"ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã«{db_status['orphan_price_symbols']}å€‹ã®å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„é•å")
    if db_status['orphan_pred_symbols'] > 0:
        issues.append(f"äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã«{db_status['orphan_pred_symbols']}å€‹ã®å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„é•å")
    if not api_accessible:
        issues.append("APIã‚µãƒ¼ãƒãƒ¼ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“")
    if active_batches == 0:
        issues.append("ãƒãƒƒãƒå‡¦ç†ãŒå…¨ã¦åœæ­¢ä¸­ã§ã™")
    
    if issues:
        for i, issue in enumerate(issues, 1):
            logger.info(f"  {i}. {issue}")
    else:
        logger.info("  âœ… é‡å¤§ãªå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    
    logger.info(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
    recommendations = []
    
    if symbol_coverage < 1:
        recommendations.append("ã‚ˆã‚Šå¤šãã®éŠ˜æŸ„ã§ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å®Ÿè¡Œ")
    if data_completeness < 10:
        recommendations.append("ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å¤§å¹…ãªå¢—åŠ ãŒå¿…è¦")
    if db_status['orphan_price_symbols'] > 0 or db_status['orphan_pred_symbols'] > 0:
        recommendations.append("å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã®ä¿®æ­£ã¾ãŸã¯stock_masterãƒ†ãƒ¼ãƒ–ãƒ«ã®æ›´æ–°")
    if active_batches < 3:
        recommendations.append("åœæ­¢ä¸­ã®ãƒãƒƒãƒå‡¦ç†ã®å†èµ·å‹•")
    
    if recommendations:
        for i, rec in enumerate(recommendations, 1):
            logger.info(f"  {i}. {rec}")
    else:
        logger.info("  âœ… ç¾åœ¨ã®çŠ¶æ³ã¯è‰¯å¥½ã§ã™")
    
    logger.info("="*80)
    
    return {
        'database': db_status,
        'batches': batch_status,
        'api': api_status,
        'data_completeness': data_completeness,
        'symbol_coverage': symbol_coverage,
        'active_batches': active_batches,
        'issues': issues,
        'recommendations': recommendations
    }

if __name__ == "__main__":
    report = generate_comprehensive_status_report()
    logger.info("âœ… åŒ…æ‹¬çš„çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆå®Œäº†")