#!/usr/bin/env python3
"""
SQLiteã‹ã‚‰Cloud SQLã¸ã®ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Miraikakakuæœ¬æ ¼é‹ç”¨ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç§»è¡Œ
"""

import os
import sqlite3
import logging
from datetime import datetime
from sqlalchemy import create_engine, text, MetaData, Table
from sqlalchemy.orm import sessionmaker
import pandas as pd

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DatabaseMigrator:
    def __init__(self):
        # SQLiteæ¥ç¶š
        self.sqlite_path = "./miraikakaku.db"
        self.sqlite_engine = create_engine(f"sqlite:///{self.sqlite_path}")

        # Cloud SQLæ¥ç¶š
        self.cloud_sql_url = "mysql+pymysql://miraikakaku-user:miraikakaku-secure-pass-2024@34.58.103.36:3306/miraikakaku"
        self.cloud_sql_engine = create_engine(self.cloud_sql_url)

    def check_connections(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª"""
        logger.info("=== ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª ===")

        # SQLiteç¢ºèª
        try:
            with self.sqlite_engine.connect() as conn:
                result = conn.execute(
                    text("SELECT name FROM sqlite_master WHERE type='table'")
                )
                sqlite_tables = [row[0] for row in result.fetchall()]
                logger.info(f"âœ… SQLiteæ¥ç¶šæˆåŠŸ: {len(sqlite_tables)}ãƒ†ãƒ¼ãƒ–ãƒ«")
        except Exception as e:
            logger.error(f"âŒ SQLiteæ¥ç¶šå¤±æ•—: {e}")
            return False

        # Cloud SQLç¢ºèª
        try:
            with self.cloud_sql_engine.connect() as conn:
                result = conn.execute(text("SELECT VERSION()"))
                version = result.fetchone()[0]
                logger.info(f"âœ… Cloud SQLæ¥ç¶šæˆåŠŸ: {version}")
        except Exception as e:
            logger.error(f"âŒ Cloud SQLæ¥ç¶šå¤±æ•—: {e}")
            return False

        return True

    def analyze_data_differences(self):
        """SQLiteã¨Cloud SQLã®ãƒ‡ãƒ¼ã‚¿å·®åˆ†åˆ†æ"""
        logger.info("=== ãƒ‡ãƒ¼ã‚¿å·®åˆ†åˆ†æ ===")

        tables_to_check = [
            "stock_master",
            "stock_predictions",
            "stock_price_history",
            "ai_inference_log",
        ]

        for table in tables_to_check:
            try:
                # SQLiteã‹ã‚‰ã‚«ã‚¦ãƒ³ãƒˆ
                with self.sqlite_engine.connect() as conn:
                    sqlite_result = conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
                    sqlite_count = sqlite_result.fetchone()[0]

                # Cloud SQLã‹ã‚‰ã‚«ã‚¦ãƒ³ãƒˆ
                with self.cloud_sql_engine.connect() as conn:
                    cloud_result = conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
                    cloud_count = cloud_result.fetchone()[0]

                logger.info(
                    f"{table}: SQLite={
                        sqlite_count:,    }, Cloud SQL={
                        cloud_count:,        }"
                )

                if sqlite_count > cloud_count:
                    logger.warning(
                        f"âš ï¸  {table}: SQLiteã®æ–¹ãŒ{
                            sqlite_count - cloud_count:,}ãƒ¬ã‚³ãƒ¼ãƒ‰å¤šã„"
                    )
                elif cloud_count > sqlite_count:
                    logger.info(
                        f"â„¹ï¸  {table}: Cloud SQLã®æ–¹ãŒ{
                            cloud_count - sqlite_count:,}ãƒ¬ã‚³ãƒ¼ãƒ‰å¤šã„"
                    )
                else:
                    logger.info(f"âœ… {table}: ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ä¸€è‡´")

            except Exception as e:
                logger.error(f"âŒ {table}ã®ç¢ºèªå¤±æ•—: {e}")

    def migrate_table_data(self, table_name, batch_size=1000):
        """ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ"""
        logger.info(f"=== {table_name}ãƒ‡ãƒ¼ã‚¿ç§»è¡Œé–‹å§‹ ===")

        try:
            # SQLiteã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            with self.sqlite_engine.connect() as conn:
                df = pd.read_sql(f"SELECT * FROM {table_name}", conn)

            if df.empty:
                logger.info(f"{table_name}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                return True

            logger.info(f"{table_name}: {len(df):,}ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç§»è¡Œé–‹å§‹")

            # Cloud SQLã«æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®å¯¾å¿œç¢ºèª
            with self.cloud_sql_engine.connect() as conn:
                existing_result = conn.execute(
                    text(f"SELECT COUNT(*) FROM {table_name}")
                )
                existing_count = existing_result.fetchone()[0]

                if existing_count > 0:
                    logger.warning(
                        f"{table_name}: Cloud SQLã«æ—¢å­˜ãƒ‡ãƒ¼ã‚¿{
                            existing_count:,}ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚ã‚Š"
                    )
                    response = input(
                        f"{table_name}ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ç§»è¡Œã—ã¾ã™ã‹? (y/N): "
                    )
                    if response.lower() == "y":
                        logger.info(f"{table_name}: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ä¸­...")
                        conn.execute(text(f"DELETE FROM {table_name}"))
                        conn.commit()
                        logger.info(f"{table_name}: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤å®Œäº†")
                    else:
                        logger.info(f"{table_name}: ç§»è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—")
                        return True

            # ãƒãƒƒãƒã”ã¨ã«æŒ¿å…¥
            total_batches = (len(df) - 1) // batch_size + 1

            for i in range(0, len(df), batch_size):
                batch_df = df.iloc[i : i + batch_size]
                batch_num = (i // batch_size) + 1

                # Cloud SQLã«æŒ¿å…¥
                batch_df.to_sql(
                    table_name,
                    self.cloud_sql_engine,
                    if_exists="append",
                    index=False,
                    method="multi",
                )

                logger.info(
                    f"{table_name}: ãƒãƒƒãƒ{batch_num}/{total_batches}å®Œäº† ({len(batch_df)}ãƒ¬ã‚³ãƒ¼ãƒ‰)"
                )

            # ç§»è¡Œå¾Œã®ç¢ºèª
            with self.cloud_sql_engine.connect() as conn:
                final_result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                final_count = final_result.fetchone()[0]

            logger.info(f"âœ… {table_name}ç§»è¡Œå®Œäº†: {final_count:,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
            return True

        except Exception as e:
            logger.error(f"âŒ {table_name}ç§»è¡Œå¤±æ•—: {e}")
            return False

    def update_application_config(self):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã‚’Cloud SQLç”¨ã«æ›´æ–°"""
        logger.info("=== ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šæ›´æ–° ===")

        env_file = "./functions/.env"

        try:
            # ç¾åœ¨ã®è¨­å®šã‚’èª­ã¿è¾¼ã¿
            with open(env_file, "r", encoding="utf-8") as f:
                content = f.read()

            # SQLiteã‹ã‚‰MySQLæ¥ç¶šã«å¤‰æ›´
            cloud_sql_database_url = "mysql+pymysql://miraikakaku-user:miraikakaku-secure-pass-2024@34.58.103.36:3306/miraikakaku"

            # DATABASE_URLã‚’æ›´æ–°
            updated_content = content.replace(
                "DATABASE_URL=sqlite:///./miraikakaku.db",
                f"DATABASE_URL={cloud_sql_database_url}",
            )

            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
            backup_file = f"{env_file}.backup.{
                datetime.now().strftime('%Y%m%d_%H%M%S')}"
            with open(backup_file, "w", encoding="utf-8") as f:
                f.write(content)
            logger.info(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")

            # æ–°ã—ã„è¨­å®šã‚’æ›¸ãè¾¼ã¿
            with open(env_file, "w", encoding="utf-8") as f:
                f.write(updated_content)

            logger.info(f"âœ… {env_file}æ›´æ–°å®Œäº†")
            logger.info(f"Database URL: {cloud_sql_database_url}")
            return True

        except Exception as e:
            logger.error(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°å¤±æ•—: {e}")
            return False

    def test_cloud_sql_connection(self):
        """Cloud SQLæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        logger.info("=== Cloud SQLæ¥ç¶šãƒ†ã‚¹ãƒˆ ===")

        try:
            with self.cloud_sql_engine.connect() as conn:
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
                result = conn.execute(text("SELECT VERSION()"))
                version = result.fetchone()[0]
                logger.info(f"MySQL Version: {version}")

                # ãƒ†ãƒ¼ãƒ–ãƒ«ç¢ºèª
                result = conn.execute(text("SHOW TABLES"))
                tables = [row[0] for row in result.fetchall()]
                logger.info(f"ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")

                # ä¸»è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ç¢ºèª
                for table in [
                    "stock_master",
                    "stock_predictions",
                    "stock_price_history",
                ]:
                    if table in tables:
                        result = conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
                        count = result.fetchone()[0]
                        logger.info(f"{table}: {count:,} ãƒ¬ã‚³ãƒ¼ãƒ‰")

                logger.info("âœ… Cloud SQLæ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ")
                return True

        except Exception as e:
            logger.error(f"âŒ Cloud SQLæ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
            return False

    def run_full_migration(self):
        """å®Œå…¨ç§»è¡Œã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ Miraikakaku Cloud SQLç§»è¡Œé–‹å§‹")

        # 1. æ¥ç¶šç¢ºèª
        if not self.check_connections():
            logger.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False

        # 2. ãƒ‡ãƒ¼ã‚¿å·®åˆ†åˆ†æ
        self.analyze_data_differences()

        # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèª
        print("\n" + "=" * 60)
        print("ğŸ”„ SQLite â†’ Cloud SQL ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ")
        print("ç¾åœ¨ã®ãƒ­ãƒ¼ã‚«ãƒ«SQLiteãƒ‡ãƒ¼ã‚¿ã‚’Cloud SQLã«ç§»è¡Œã—ã¾ã™")
        print("â€»æ—¢å­˜ã®Cloud SQLãƒ‡ãƒ¼ã‚¿ã¯ä¸Šæ›¸ãã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        print("=" * 60)

        proceed = input("ç§»è¡Œã‚’å®Ÿè¡Œã—ã¾ã™ã‹? (y/N): ")
        if proceed.lower() != "y":
            logger.info("ç§»è¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            return False

        # 4. ãƒ†ãƒ¼ãƒ–ãƒ«ç§»è¡Œ
        tables_to_migrate = [
            "stock_master",
            "stock_predictions",
            "stock_price_history",
            "ai_inference_log",
        ]

        success_count = 0
        for table in tables_to_migrate:
            if self.migrate_table_data(table):
                success_count += 1

        if success_count == len(tables_to_migrate):
            logger.info("âœ… å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ç§»è¡Œå®Œäº†")
        else:
            logger.warning(
                f"âš ï¸  {success_count}/{len(tables_to_migrate)}ãƒ†ãƒ¼ãƒ–ãƒ«ç§»è¡Œå®Œäº†"
            )

        # 5. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šæ›´æ–°
        if self.update_application_config():
            logger.info("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°å®Œäº†")

        # 6. æœ€çµ‚ãƒ†ã‚¹ãƒˆ
        if self.test_cloud_sql_connection():
            logger.info("âœ… Cloud SQLç§»è¡Œãƒ†ã‚¹ãƒˆæˆåŠŸ")

        logger.info("ğŸ‰ Miraikakaku Cloud SQLç§»è¡Œå®Œäº†!")
        logger.info("æœ¬æ ¼é‹ç”¨ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸ")

        return True


def main():
    migrator = DatabaseMigrator()

    print("ğŸ”„ Miraikakaku Database Migration Tool")
    print("SQLiteã‹ã‚‰Cloud SQLã¸ã®æœ¬æ ¼é‹ç”¨ç§»è¡Œ")
    print("-" * 50)

    migrator.run_full_migration()


if __name__ == "__main__":
    main()
