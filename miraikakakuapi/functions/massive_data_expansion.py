#!/usr/bin/env python3
"""
è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ - MLé©åˆåº¦ã‚’50ç‚¹ä»¥ä¸Šã«å¼•ãä¸Šã’
1000+éŠ˜æŸ„ã€5å¹´å±¥æ­´ã€å¤§é‡äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã§æ©Ÿæ¢°å­¦ç¿’ã«æœ€é©åŒ–
"""

import logging
import yfinance as yf
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import time
import sys
import os
from sqlalchemy import text
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue
import random

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from database.database import get_db

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MassiveDataExpander:
    def __init__(self, max_workers=15):
        self.max_workers = max_workers
        self.progress_lock = threading.Lock()
        self.stats = {
            'total_symbols': 0,
            'processed': 0,
            'price_records': 0,
            'predictions': 0,
            'errors': 0,
            'skipped': 0
        }
        self.error_queue = Queue()
        
    def get_massive_symbol_list(self, target_count=1500):
        """å¤§è¦æ¨¡éŠ˜æŸ„ãƒªã‚¹ãƒˆå–å¾—ï¼ˆ1500éŠ˜æŸ„ç›®æ¨™ï¼‰"""
        db = next(get_db())
        try:
            all_symbols = []
            
            # Tier 1: ä¸»è¦æŒ‡æ•°ï¼ˆå¿…é ˆï¼‰
            indices = [
                '^GSPC', '^DJI', '^IXIC', '^RUT', '^N225', '^FTSE', '^HSI',
                '^TNX', '^VIX', '^GDAXI', '^CAC', '^NIKKEI', '^KS11'
            ]
            all_symbols.extend(indices)
            logger.info(f"Tier 1 - ä¸»è¦æŒ‡æ•°: {len(indices)}éŠ˜æŸ„")
            
            # Tier 2: ç±³å›½å¤§å‹æ ªTOP500
            result = db.execute(text("""
                SELECT symbol FROM stock_master 
                WHERE market IN ('NASDAQ', 'NYSE')
                AND is_active = 1
                AND symbol IS NOT NULL
                AND LENGTH(symbol) BETWEEN 1 AND 5
                ORDER BY RAND()
                LIMIT 600
            """))
            us_large_cap = [row[0] for row in result]
            all_symbols.extend(us_large_cap)
            logger.info(f"Tier 2 - ç±³å›½å¤§å‹æ ª: {len(us_large_cap)}éŠ˜æŸ„")
            
            # Tier 3: æ—¥æœ¬æ ªï¼ˆæ±è¨¼å…¨ã‚»ã‚¯ã‚¿ãƒ¼ï¼‰
            result = db.execute(text("""
                SELECT symbol FROM stock_master 
                WHERE country = 'Japan'
                AND symbol REGEXP '^[0-9]{4}$'
                AND is_active = 1
                ORDER BY RAND()
                LIMIT 400
            """))
            jp_stocks = [row[0] + '.T' for row in result]
            all_symbols.extend(jp_stocks)
            logger.info(f"Tier 3 - æ—¥æœ¬æ ª: {len(jp_stocks)}éŠ˜æŸ„")
            
            # Tier 4: æ¬§å·ãƒ»ã‚¢ã‚¸ã‚¢ä¸»è¦æ ª
            result = db.execute(text("""
                SELECT symbol FROM stock_master 
                WHERE market IN ('LSE', 'HKEX', 'SSE', 'TSE', 'XETRA')
                AND is_active = 1
                AND symbol IS NOT NULL
                LIMIT 300
            """))
            intl_stocks = [row[0] for row in result]
            all_symbols.extend(intl_stocks)
            logger.info(f"Tier 4 - å›½éš›æ ª: {len(intl_stocks)}éŠ˜æŸ„")
            
            # Tier 5: ã‚»ã‚¯ã‚¿ãƒ¼ETFã¨å•†å“
            etf_commodities = [
                'SPY', 'QQQ', 'IWM', 'VTI', 'VEA', 'VWO', 'BND', 'GLD', 'SLV',
                'XLK', 'XLF', 'XLE', 'XLV', 'XLI', 'XLY', 'XLP', 'XLU', 'XLRE',
                'USO', 'UNG', 'DBA', 'VNQ', 'EEM', 'FXI', 'EWJ', 'EWZ'
            ]
            all_symbols.extend(etf_commodities)
            logger.info(f"Tier 5 - ETF/å•†å“: {len(etf_commodities)}éŠ˜æŸ„")
            
            # é‡è¤‡é™¤å»ã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            unique_symbols = list(set(all_symbols))
            random.shuffle(unique_symbols)
            
            final_list = unique_symbols[:target_count]
            logger.info(f"æœ€çµ‚å¯¾è±¡: {len(final_list)}éŠ˜æŸ„ (ç›®æ¨™: {target_count})")
            
            return final_list
            
        finally:
            db.close()
    
    def fetch_massive_historical_data(self, symbol):
        """5å¹´é–“ã®å¤§é‡å±¥æ­´ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        result = {'symbol': symbol, 'prices': 0, 'predictions': 0, 'error': None}
        
        try:
            # 5å¹´é–“ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆMLå­¦ç¿’ã«ååˆ†ï¼‰
            ticker = yf.Ticker(symbol)
            end_date = datetime.now()
            start_date = end_date - timedelta(days=1825)  # 5å¹´
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã§ãƒ‡ãƒ¼ã‚¿å–å¾—
            hist = ticker.history(start=start_date, end=end_date, timeout=45)
            
            if hist.empty:
                result['error'] = 'No historical data'
                return result
            
            # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(hist) < 50:
                result['error'] = 'Insufficient data points'
                return result
            
            db = next(get_db())
            try:
                db_symbol = symbol.replace('.T', '').replace('^', '')
                
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèªï¼ˆé‡è¤‡å‡¦ç†ã‚’é¿ã‘ã‚‹ï¼‰
                existing_count = db.execute(text(
                    "SELECT COUNT(*) FROM stock_prices WHERE symbol = :sym"
                ), {"sym": db_symbol}).scalar()
                
                # æ—¢ã«å¤§é‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if existing_count >= len(hist) * 0.9:
                    result['error'] = 'Already has sufficient data'
                    return result
                
                # ãƒãƒ«ã‚¯ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
                price_data = []
                for date, row in hist.iterrows():
                    # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                    exists = db.execute(text(
                        "SELECT COUNT(*) FROM stock_prices WHERE symbol = :sym AND date = :dt"
                    ), {"sym": db_symbol, "dt": date.date()}).scalar()
                    
                    if exists == 0:
                        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
                        if (row['Open'] > 0 and row['High'] > 0 and 
                            row['Low'] > 0 and row['Close'] > 0):
                            price_data.append({
                                "sym": db_symbol,
                                "dt": date.date(),
                                "op": float(row['Open']),
                                "hi": float(row['High']),
                                "lo": float(row['Low']),
                                "cl": float(row['Close']),
                                "vol": int(row['Volume']) if row['Volume'] > 0 else 0,
                                "adj": float(row['Close'])
                            })
                
                # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥å®Ÿè¡Œ
                inserted = 0
                for data in price_data:
                    try:
                        db.execute(text("""
                            INSERT INTO stock_prices 
                            (symbol, date, open_price, high_price, low_price, close_price, 
                             volume, adjusted_close, created_at)
                            VALUES (:sym, :dt, :op, :hi, :lo, :cl, :vol, :adj, NOW())
                        """), data)
                        inserted += 1
                    except Exception:
                        continue
                
                if inserted > 0:
                    db.commit()
                    result['prices'] = inserted
                
                # å¤§é‡äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ90æ—¥åˆ†ï¼‰
                if inserted > 100:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿
                    pred_count = self.generate_extensive_predictions(db, db_symbol, hist)
                    result['predictions'] = pred_count
                
                return result
                
            finally:
                db.close()
                
        except Exception as e:
            result['error'] = str(e)
            return result
    
    def generate_extensive_predictions(self, db, db_symbol, price_data):
        """90æ—¥é–“ã®è©³ç´°äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        try:
            if len(price_data) < 100:
                return 0
            
            prices = price_data['Close'].values
            returns = np.diff(np.log(prices))
            latest_price = float(prices[-1])
            
            # é«˜åº¦ãªæŠ€è¡“åˆ†ææŒ‡æ¨™
            # 1. è¤‡æ•°æœŸé–“ç§»å‹•å¹³å‡
            ma_periods = [5, 10, 20, 50, 100]
            mas = {}
            for period in ma_periods:
                if len(prices) >= period:
                    mas[period] = np.mean(prices[-period:])
            
            # 2. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£åˆ†æï¼ˆè¤‡æ•°æœŸé–“ï¼‰
            vol_short = np.std(returns[-30:]) if len(returns) >= 30 else 0.02
            vol_long = np.std(returns[-100:]) if len(returns) >= 100 else 0.02
            vol_regime = vol_short / vol_long if vol_long > 0 else 1.0
            
            # 3. ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
            trend_signals = {}
            if 20 in mas and 50 in mas:
                trend_signals['medium'] = (mas[20] - mas[50]) / mas[50]
            if 50 in mas and 100 in mas:
                trend_signals['long'] = (mas[50] - mas[100]) / mas[100]
            
            # 4. ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™ç¾¤
            momentum_periods = [5, 10, 20]
            momentum = {}
            for period in momentum_periods:
                if len(prices) > period:
                    momentum[period] = (prices[-1] - prices[-period-1]) / prices[-period-1]
            
            prediction_count = 0
            
            # 90æ—¥é–“ã®äºˆæ¸¬ç”Ÿæˆï¼ˆMLè¨“ç·´ã«ååˆ†ãªé‡ï¼‰
            for days_ahead in range(1, 91):
                prediction_date = datetime.now().date() + timedelta(days=days_ahead)
                
                # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
                exists = db.execute(text(
                    "SELECT COUNT(*) FROM stock_predictions WHERE symbol = :sym AND prediction_date = :dt"
                ), {"sym": db_symbol, "dt": prediction_date}).scalar()
                
                if exists > 0:
                    continue
                
                # è¤‡åˆäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
                prediction_components = []
                
                # A. ãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šæˆåˆ†
                if trend_signals:
                    trend_component = np.mean(list(trend_signals.values())) * min(days_ahead * 0.05, 0.3)
                    prediction_components.append(trend_component)
                
                # B. å¹³å‡å›å¸°æˆåˆ†
                if momentum:
                    mean_revert = -np.mean(list(momentum.values())) * 0.1 * np.sqrt(days_ahead)
                    prediction_components.append(mean_revert)
                
                # C. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æˆåˆ†
                vol_cluster = vol_regime * np.random.normal(0, vol_short) * np.sqrt(days_ahead)
                prediction_components.append(vol_cluster)
                
                # D. é•·æœŸå›å¸°æˆåˆ†
                if len(prices) >= 252:
                    annual_return = (prices[-1] - prices[-253]) / prices[-253]
                    long_term_drift = annual_return / 252 * days_ahead * 0.3
                    prediction_components.append(long_term_drift)
                
                # E. ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ§ãƒƒã‚¯
                shock_component = np.random.normal(0, 0.01) * np.sqrt(days_ahead / 30)
                prediction_components.append(shock_component)
                
                # ç·åˆäºˆæ¸¬
                total_change = sum(prediction_components)
                predicted_price = latest_price * (1 + total_change)
                
                # å‹•çš„ä¿¡é ¼åº¦è¨ˆç®—
                data_quality = min(1.0, len(prices) / 1000)
                volatility_penalty = max(0.2, 1 - vol_short * 10)
                time_penalty = max(0.1, 1 - days_ahead * 0.008)
                model_complexity_bonus = min(0.2, len(prediction_components) * 0.04)
                
                confidence = (data_quality * volatility_penalty * time_penalty + 
                            model_complexity_bonus) * 0.9
                
                # ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ï¼ˆå±¥æ­´ãƒ‡ãƒ¼ã‚¿é‡ã¨ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ï¼‰
                base_accuracy = 0.65
                data_bonus = min(0.2, len(prices) / 2000)
                volatility_adjustment = max(-0.15, -vol_short * 2)
                model_accuracy = base_accuracy + data_bonus + volatility_adjustment
                
                # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
                db.execute(text("""
                    INSERT INTO stock_predictions 
                    (symbol, prediction_date, current_price, predicted_price,
                     confidence_score, prediction_days, model_version, 
                     model_accuracy, created_at)
                    VALUES (:sym, :dt, :cur, :pred, :conf, :days, :model, :acc, NOW())
                """), {
                    "sym": db_symbol,
                    "dt": prediction_date,
                    "cur": latest_price,
                    "pred": round(predicted_price, 4),
                    "conf": round(confidence, 3),
                    "days": days_ahead,
                    "model": 'MASSIVE_EXPANSION_V1',
                    "acc": round(model_accuracy, 3)
                })
                prediction_count += 1
            
            if prediction_count > 0:
                db.commit()
                
            return prediction_count
            
        except Exception as e:
            logger.debug(f"äºˆæ¸¬ç”Ÿæˆã‚¨ãƒ©ãƒ¼ {db_symbol}: {e}")
            return 0
    
    def update_progress_thread_safe(self, result):
        """ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªé€²æ—æ›´æ–°"""
        with self.progress_lock:
            self.stats['processed'] += 1
            
            if result['error']:
                if 'sufficient data' in result['error']:
                    self.stats['skipped'] += 1
                else:
                    self.stats['errors'] += 1
            else:
                self.stats['price_records'] += result['prices']
                self.stats['predictions'] += result['predictions']
            
            # é€²æ—ãƒ­ã‚°ï¼ˆ25å€‹ã”ã¨ï¼‰
            if self.stats['processed'] % 25 == 0:
                progress = (self.stats['processed'] / self.stats['total_symbols']) * 100
                logger.info(f"é€²æ— {progress:.1f}%: å‡¦ç†{self.stats['processed']}/{self.stats['total_symbols']} | "
                          f"ä¾¡æ ¼+{self.stats['price_records']} äºˆæ¸¬+{self.stats['predictions']} "
                          f"ã‚¨ãƒ©ãƒ¼{self.stats['errors']} ã‚¹ã‚­ãƒƒãƒ—{self.stats['skipped']}")
    
    def execute_massive_expansion(self, target_symbols=1200):
        """è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Ÿè¡Œ"""
        logger.info("="*100)
        logger.info("ğŸš€ è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µé–‹å§‹ - MLé©åˆåº¦50ç‚¹çªç ´ç›®æ¨™")
        logger.info("="*100)
        
        start_time = time.time()
        
        # å¤§è¦æ¨¡éŠ˜æŸ„ãƒªã‚¹ãƒˆå–å¾—
        symbols = self.get_massive_symbol_list(target_symbols)
        self.stats['total_symbols'] = len(symbols)
        
        logger.info(f"å¯¾è±¡éŠ˜æŸ„: {len(symbols)}")
        logger.info(f"ä¸¦è¡Œå‡¦ç†æ•°: {self.max_workers}")
        logger.info(f"äºˆæƒ³å‡¦ç†æ™‚é–“: {len(symbols) * 3 / self.max_workers / 60:.1f}åˆ†")
        
        # å¤§è¦æ¨¡ä¸¦è¡Œå‡¦ç†å®Ÿè¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # å…¨ã‚¿ã‚¹ã‚¯ã‚’ä¸¦è¡ŒæŠ•å…¥
            futures = {
                executor.submit(self.fetch_massive_historical_data, symbol): symbol 
                for symbol in symbols
            }
            
            # çµæœã‚’é †æ¬¡å‡¦ç†
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=90)
                    self.update_progress_thread_safe(result)
                    
                except Exception as e:
                    symbol = futures[future]
                    error_result = {
                        'symbol': symbol, 'prices': 0, 'predictions': 0, 
                        'error': f'Processing error: {e}'
                    }
                    self.update_progress_thread_safe(error_result)
        
        # æœ€çµ‚çµæœ
        elapsed = time.time() - start_time
        success_count = self.stats['processed'] - self.stats['errors'] - self.stats['skipped']
        
        logger.info("="*100)
        logger.info("ğŸ‰ è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Œäº†")
        logger.info(f"â±ï¸  ç·å‡¦ç†æ™‚é–“: {elapsed/60:.1f}åˆ†")
        logger.info(f"ğŸ“ˆ æˆåŠŸå‡¦ç†: {success_count}/{self.stats['total_symbols']} ({success_count/self.stats['total_symbols']*100:.1f}%)")
        logger.info(f"ğŸ’¾ è¿½åŠ ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿: {self.stats['price_records']:,}ä»¶")
        logger.info(f"ğŸ”® è¿½åŠ äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿: {self.stats['predictions']:,}ä»¶")
        logger.info(f"âš ï¸  ã‚¨ãƒ©ãƒ¼æ•°: {self.stats['errors']}")
        logger.info(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—æ•°: {self.stats['skipped']}")
        logger.info("="*100)
        
        # æ‹¡å¼µå¾Œãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        self.verify_massive_expansion()
        
        return self.stats
    
    def verify_massive_expansion(self):
        """æ‹¡å¼µå¾Œã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼"""
        db = next(get_db())
        try:
            # ç·åˆçµ±è¨ˆ
            result = db.execute(text("""
                SELECT 
                    COUNT(DISTINCT symbol) as unique_symbols,
                    COUNT(*) as total_records,
                    MIN(date) as oldest_date,
                    MAX(date) as newest_date,
                    AVG(close_price) as avg_price
                FROM stock_prices
            """))
            price_stats = result.fetchone()
            
            result = db.execute(text("""
                SELECT 
                    COUNT(DISTINCT symbol) as unique_symbols,
                    COUNT(*) as total_records,
                    MIN(prediction_date) as oldest_pred,
                    MAX(prediction_date) as newest_pred
                FROM stock_predictions
            """))
            pred_stats = result.fetchone()
            
            # MLé©åˆåº¦å†è¨ˆç®—
            # 1. ãƒ‡ãƒ¼ã‚¿é‡ã‚¹ã‚³ã‚¢ (ç›®æ¨™: 10,000+ records = 30ç‚¹)
            data_score = min(30, price_stats[1] / 10000 * 30)
            
            # 2. éŠ˜æŸ„å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢ (ç›®æ¨™: 100+ symbols = 25ç‚¹)
            diversity_score = min(25, price_stats[0] / 100 * 25)
            
            # 3. é•·æœŸãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢
            result = db.execute(text("""
                SELECT symbol, COUNT(*) as cnt, DATEDIFF(MAX(date), MIN(date)) as days
                FROM stock_prices 
                GROUP BY symbol 
                HAVING COUNT(*) >= 100
                ORDER BY cnt DESC
                LIMIT 10
            """))
            long_term_data = result.fetchall()
            
            if long_term_data:
                avg_span = np.mean([row[2] for row in long_term_data])
                time_score = min(25, avg_span / 1000 * 25)
            else:
                time_score = 0
            
            # 4. äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢ (ç›®æ¨™: 5,000+ predictions = 20ç‚¹)
            pred_score = min(20, pred_stats[1] / 5000 * 20)
            
            total_ml_score = data_score + diversity_score + time_score + pred_score
            
            logger.info("\n" + "="*80)
            logger.info("ğŸ“Š å¤§è¦æ¨¡æ‹¡å¼µå¾Œãƒ‡ãƒ¼ã‚¿çŠ¶æ³")
            logger.info("="*80)
            logger.info(f"ã€ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã€‘")
            logger.info(f"  éŠ˜æŸ„æ•°: {price_stats[0]:,}å€‹")
            logger.info(f"  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {price_stats[1]:,}ä»¶")
            logger.info(f"  æœŸé–“: {price_stats[2]} ï½ {price_stats[3]}")
            logger.info(f"  å¹³å‡ä¾¡æ ¼: ${price_stats[4]:.2f}" if price_stats[4] else "  å¹³å‡ä¾¡æ ¼: N/A")
            
            logger.info(f"\nã€äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã€‘")
            logger.info(f"  éŠ˜æŸ„æ•°: {pred_stats[0]:,}å€‹")
            logger.info(f"  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {pred_stats[1]:,}ä»¶")
            logger.info(f"  äºˆæ¸¬æœŸé–“: {pred_stats[2]} ï½ {pred_stats[3]}")
            
            logger.info(f"\nã€MLé©åˆåº¦ã‚¹ã‚³ã‚¢ã€‘")
            logger.info(f"  ç·åˆã‚¹ã‚³ã‚¢: {total_ml_score:.1f}/100ç‚¹")
            logger.info(f"    ãƒ‡ãƒ¼ã‚¿é‡: {data_score:.1f}/30")
            logger.info(f"    éŠ˜æŸ„å¤šæ§˜æ€§: {diversity_score:.1f}/25")
            logger.info(f"    æ™‚ç³»åˆ—é•·: {time_score:.1f}/25")
            logger.info(f"    äºˆæ¸¬å……å®Ÿåº¦: {pred_score:.1f}/20")
            
            if total_ml_score >= 50:
                logger.info(f"  ğŸ¯ ç›®æ¨™é”æˆï¼MLè¨“ç·´ã«é©ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Œæˆ")
            elif total_ml_score >= 30:
                logger.info(f"  ğŸŸ¡ åŸºæœ¬ãƒ¬ãƒ™ãƒ«é”æˆ - ã•ã‚‰ãªã‚‹æ”¹å–„ä½™åœ°ã‚ã‚Š")
            else:
                logger.info(f"  ğŸ”´ ã¾ã ä¸è¶³ - ç¶™ç¶šçš„ãƒ‡ãƒ¼ã‚¿åé›†ãŒå¿…è¦")
            
            logger.info("="*80)
            
        finally:
            db.close()

if __name__ == "__main__":
    # è¶…å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Ÿè¡Œ
    expander = MassiveDataExpander(max_workers=12)
    result = expander.execute_massive_expansion(target_symbols=1000)
    
    logger.info(f"âœ… å¤§è¦æ¨¡æ‹¡å¼µå®Œäº† - ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿+{result['price_records']:,}ä»¶, äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿+{result['predictions']:,}ä»¶")