# Cloud Run Deployment Configuration
# Replacing GCP Batch with reliable Cloud Run services

# Data Collector Service Configuration
data_collector_service:
  name: miraikakaku-data-collector
  description: "Reliable stock price data collection service - replacement for GCP Batch"
  docker_image: "gcr.io/pricewise-huqkr/data-collector:stable"

  resources:
    cpu: "1000m"          # 1 CPU core
    memory: "2Gi"         # 2GB RAM
    timeout: "600s"       # 10 minute timeout

  scaling:
    min_instances: 0      # Scale to zero when idle
    max_instances: 10     # Scale up to 10 instances
    max_concurrent: 100   # Handle 100 concurrent requests

  environment_variables:
    DB_HOST: "34.173.9.214"
    DB_USER: "postgres"
    DB_NAME: "miraikakaku"
    DB_PORT: "5432"
    ENVIRONMENT: "production"

  endpoints:
    - path: "/health"
      method: "GET"
      description: "Service health check"
    - path: "/collect"
      method: "POST"
      description: "Data collection endpoint"
      body: |
        {
          "mode": "standard|priority|full",
          "symbols": ["AAPL", "GOOGL"],
          "days_back": 30
        }
    - path: "/tasks"
      method: "GET"
      description: "Task status monitoring"

# Prediction Generator Service Configuration
prediction_generator_service:
  name: miraikakaku-prediction-generator
  description: "AI-powered prediction generation service - replacement for GCP Batch"
  docker_image: "gcr.io/pricewise-huqkr/prediction-generator:stable"

  resources:
    cpu: "2000m"          # 2 CPU cores
    memory: "4Gi"         # 4GB RAM
    timeout: "900s"       # 15 minute timeout

  scaling:
    min_instances: 1      # Keep 1 instance warm
    max_instances: 5      # Scale up to 5 instances
    max_concurrent: 50    # Handle 50 concurrent requests

  environment_variables:
    DB_HOST: "34.173.9.214"
    DB_USER: "postgres"
    DB_NAME: "miraikakaku"
    DB_PORT: "5432"
    ENVIRONMENT: "production"

  endpoints:
    - path: "/health"
      method: "GET"
      description: "Service health check"
    - path: "/predict"
      method: "POST"
      description: "Prediction generation endpoint"
      body: |
        {
          "mode": "standard|priority|full|historical",
          "symbols": ["AAPL", "GOOGL"],
          "model_types": ["cloudrun_lstm_v1", "cloudrun_transformer_v1"],
          "prediction_days": [1, 3, 7, 14, 30]
        }
    - path: "/tasks"
      method: "GET"
      description: "Task status monitoring"
    - path: "/models"
      method: "GET"
      description: "Available AI models"

# Cloud Scheduler Jobs - Replace all GCP Batch jobs
scheduler_jobs:

  # Hourly data collection (replacing hourly batch jobs)
  hourly_data_collection:
    name: "miraikakaku-hourly-data-collection"
    description: "Hourly stock price data collection via Cloud Run"
    schedule: "0 * * * *"                    # Every hour
    timezone: "Asia/Tokyo"
    target_service: "miraikakaku-data-collector"
    http_config:
      uri: "https://miraikakaku-data-collector-xxxx-xx.a.run.app/collect"
      http_method: "POST"
      headers:
        Content-Type: "application/json"
      body: |
        {
          "mode": "standard",
          "days_back": 7
        }
    retry_config:
      retry_count: 2
      max_retry_duration: "300s"
      max_backoff_duration: "60s"
      max_doublings: 3

  # Daily prediction generation (replacing daily batch jobs)
  daily_prediction_generation:
    name: "miraikakaku-daily-predictions"
    description: "Daily AI prediction generation via Cloud Run"
    schedule: "0 6 * * *"                    # 6 AM JST daily
    timezone: "Asia/Tokyo"
    target_service: "miraikakaku-prediction-generator"
    http_config:
      uri: "https://miraikakaku-prediction-generator-xxxx-xx.a.run.app/predict"
      http_method: "POST"
      headers:
        Content-Type: "application/json"
      body: |
        {
          "mode": "standard",
          "model_types": ["cloudrun_lstm_v1", "cloudrun_transformer_v1"],
          "prediction_days": [1, 3, 7, 14, 30]
        }
    retry_config:
      retry_count: 1
      max_retry_duration: "600s"

  # Priority data collection (replacing priority batch jobs)
  priority_data_collection:
    name: "miraikakaku-priority-data-collection"
    description: "Priority symbols data collection via Cloud Run"
    schedule: "0 9,15,21 * * 1-5"            # 9AM, 3PM, 9PM on weekdays
    timezone: "Asia/Tokyo"
    target_service: "miraikakaku-data-collector"
    http_config:
      uri: "https://miraikakaku-data-collector-xxxx-xx.a.run.app/collect"
      http_method: "POST"
      headers:
        Content-Type: "application/json"
      body: |
        {
          "mode": "priority",
          "days_back": 5
        }
    retry_config:
      retry_count: 2
      max_retry_duration: "300s"

  # Weekly full collection (replacing weekly batch jobs)
  weekly_full_collection:
    name: "miraikakaku-weekly-full-collection"
    description: "Weekly comprehensive data collection via Cloud Run"
    schedule: "0 2 * * 0"                    # 2 AM on Sundays
    timezone: "Asia/Tokyo"
    target_service: "miraikakaku-data-collector"
    http_config:
      uri: "https://miraikakaku-data-collector-xxxx-xx.a.run.app/collect"
      http_method: "POST"
      headers:
        Content-Type: "application/json"
      body: |
        {
          "mode": "full",
          "days_back": 30
        }
    retry_config:
      retry_count: 1
      max_retry_duration: "900s"

  # System health monitoring
  system_health_monitoring:
    name: "miraikakaku-system-health-check"
    description: "System health monitoring via Cloud Run"
    schedule: "*/15 * * * *"                 # Every 15 minutes
    timezone: "Asia/Tokyo"
    target_service: "miraikakaku-data-collector"
    http_config:
      uri: "https://miraikakaku-data-collector-xxxx-xx.a.run.app/health"
      http_method: "GET"
      headers: {}
    retry_config:
      retry_count: 1
      max_retry_duration: "60s"

# Deployment Commands
deployment_commands:

  # Build and deploy data collector
  deploy_data_collector: |
    # Build Docker image
    docker build -t gcr.io/pricewise-huqkr/data-collector:stable -f Dockerfile.data-collector .
    docker push gcr.io/pricewise-huqkr/data-collector:stable

    # Deploy to Cloud Run
    gcloud run deploy miraikakaku-data-collector \
      --image gcr.io/pricewise-huqkr/data-collector:stable \
      --platform managed \
      --region us-central1 \
      --allow-unauthenticated \
      --memory 2Gi \
      --cpu 1 \
      --timeout 600 \
      --max-instances 10 \
      --min-instances 0 \
      --concurrency 100 \
      --set-env-vars "DB_HOST=34.173.9.214,DB_USER=postgres,DB_NAME=miraikakaku,DB_PORT=5432,ENVIRONMENT=production"

  # Build and deploy prediction generator
  deploy_prediction_generator: |
    # Build Docker image
    docker build -t gcr.io/pricewise-huqkr/prediction-generator:stable -f Dockerfile.prediction-generator .
    docker push gcr.io/pricewise-huqkr/prediction-generator:stable

    # Deploy to Cloud Run
    gcloud run deploy miraikakaku-prediction-generator \
      --image gcr.io/pricewise-huqkr/prediction-generator:stable \
      --platform managed \
      --region us-central1 \
      --allow-unauthenticated \
      --memory 4Gi \
      --cpu 2 \
      --timeout 900 \
      --max-instances 5 \
      --min-instances 1 \
      --concurrency 50 \
      --set-env-vars "DB_HOST=34.173.9.214,DB_USER=postgres,DB_NAME=miraikakaku,DB_PORT=5432,ENVIRONMENT=production"

  # Create Cloud Scheduler jobs
  create_schedulers: |
    # Hourly data collection
    gcloud scheduler jobs create http miraikakaku-hourly-data-collection \
      --schedule="0 * * * *" \
      --time-zone="Asia/Tokyo" \
      --uri="https://miraikakaku-data-collector-xxxx-xx.a.run.app/collect" \
      --http-method=POST \
      --headers="Content-Type=application/json" \
      --message-body='{"mode":"standard","days_back":7}' \
      --max-retry-attempts=2 \
      --max-retry-duration=300s

    # Daily predictions
    gcloud scheduler jobs create http miraikakaku-daily-predictions \
      --schedule="0 6 * * *" \
      --time-zone="Asia/Tokyo" \
      --uri="https://miraikakaku-prediction-generator-xxxx-xx.a.run.app/predict" \
      --http-method=POST \
      --headers="Content-Type=application/json" \
      --message-body='{"mode":"standard","model_types":["cloudrun_lstm_v1","cloudrun_transformer_v1"],"prediction_days":[1,3,7,14,30]}' \
      --max-retry-attempts=1 \
      --max-retry-duration=600s

    # Priority data collection
    gcloud scheduler jobs create http miraikakaku-priority-data-collection \
      --schedule="0 9,15,21 * * 1-5" \
      --time-zone="Asia/Tokyo" \
      --uri="https://miraikakaku-data-collector-xxxx-xx.a.run.app/collect" \
      --http-method=POST \
      --headers="Content-Type=application/json" \
      --message-body='{"mode":"priority","days_back":5}' \
      --max-retry-attempts=2 \
      --max-retry-duration=300s

    # Weekly full collection
    gcloud scheduler jobs create http miraikakaku-weekly-full-collection \
      --schedule="0 2 * * 0" \
      --time-zone="Asia/Tokyo" \
      --uri="https://miraikakaku-data-collector-xxxx-xx.a.run.app/collect" \
      --http-method=POST \
      --headers="Content-Type=application/json" \
      --message-body='{"mode":"full","days_back":30}' \
      --max-retry-attempts=1 \
      --max-retry-duration=900s

    # Health monitoring
    gcloud scheduler jobs create http miraikakaku-system-health-check \
      --schedule="*/15 * * * *" \
      --time-zone="Asia/Tokyo" \
      --uri="https://miraikakaku-data-collector-xxxx-xx.a.run.app/health" \
      --http-method=GET \
      --max-retry-attempts=1 \
      --max-retry-duration=60s

# Monitoring and Alerting
monitoring:

  # Service Level Objectives (SLOs)
  data_collector_slo:
    availability: 99.5%
    latency_p95: 30s
    success_rate: 95%

  prediction_generator_slo:
    availability: 99.0%
    latency_p95: 60s
    success_rate: 90%

  # Alerting policies
  alerts:
    - name: "Data Collection Service Down"
      condition: "Service unavailable for > 5 minutes"
      severity: "CRITICAL"

    - name: "Prediction Service High Error Rate"
      condition: "Error rate > 10% for 15 minutes"
      severity: "WARNING"

    - name: "Scheduler Job Failures"
      condition: "Scheduler job failed 3 times consecutively"
      severity: "ERROR"

# Migration Plan
migration_steps:
  1. "Deploy Cloud Run services to production"
  2. "Create Cloud Scheduler jobs with new schedules"
  3. "Test Cloud Run services with small workloads"
  4. "Gradually migrate workloads from Batch to Cloud Run"
  5. "Monitor performance and adjust scaling parameters"
  6. "Disable old GCP Batch jobs after successful migration"
  7. "Clean up unused Batch resources"

# Expected Benefits
benefits:
  reliability_improvement: "80% failure rate → 5% failure rate (95% success rate)"
  cost_optimization: "Pay-per-use with scale-to-zero capability"
  operational_simplicity: "Managed service with automatic scaling"
  faster_debugging: "Real-time logs and monitoring"
  better_performance: "HTTP-based triggering instead of batch job creation overhead"